<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Natural Language Processing on Davidham的博客</title><link>https://davidham3.github.io/blog/tags/natural-language-processing/</link><description>Recent content in Natural Language Processing on Davidham的博客</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 15 Jun 2018 19:45:48 +0000</lastBuildDate><atom:link href="https://davidham3.github.io/blog/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml"/><item><title>汉语分词最大匹配算法</title><link>https://davidham3.github.io/blog/p/%E6%B1%89%E8%AF%AD%E5%88%86%E8%AF%8D%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95/</link><pubDate>Fri, 15 Jun 2018 19:45:48 +0000</pubDate><guid>https://davidham3.github.io/blog/p/%E6%B1%89%E8%AF%AD%E5%88%86%E8%AF%8D%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95/</guid><description>&lt;p&gt;正向最大匹配，逆向最大匹配&lt;/p&gt;
&lt;h2 id="汉语正向逆向最大分词算法"&gt;汉语正向、逆向最大分词算法
&lt;/h2&gt;&lt;p&gt;汉语分词最大匹配法(Maximum Matching)：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;正向最大匹配算法(Forward MM)&lt;/li&gt;
&lt;li&gt;逆向最大匹配算法(Backward MM)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="算法"&gt;算法
&lt;/h3&gt;&lt;p&gt;假设句子：$S = c_1c_2···c_n$，某一词：$w_i = c_1c_2···c_m$，$m$为词典中最长词的字数。
FMM 算法描述&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;令$i=0$，当前指针$p_i$指向输入字串的初始位置，执行下面的操作：&lt;/li&gt;
&lt;li&gt;计算当前指针$p_i$到字串末端的字数（即未被切分字串的长度）$n$，如果$n=1$，转(4)，结束算法。否则，令$m=$词典中最长单词的字数，如果$n&amp;lt;m$，令$m=n$。&lt;/li&gt;
&lt;li&gt;从当前$p_i$起取$m$个汉字作为词$w_i$，判断：
3.1. 如果$w_i$确实是词典中的词，则在$w_i$后添加一个切分标志，转(3.3);
3.2. 如果$w_i$不是词典中的词且$w_i$的长度大于1，将$w_i$从右端去掉一个字，转(3.1)步；否则（$w_i$的长度等于1），则在$w_i$后添加一个切分标志，将$w_i$作为单字词添加到词典中，执行(3.3)步；
3.3. 根据$w_i$的长度修改指针$p_i$的位置，如果$p_i$指向字串末端，转(4)，否则，$i=i+1$，返回(2)；&lt;/li&gt;
&lt;li&gt;输出切分结果，结束分词程序。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;逆向最大匹配算法同理。&lt;/p&gt;
&lt;h3 id="数据"&gt;数据
&lt;/h3&gt;&lt;p&gt;人民日报语料，总共100344条样本。
样例：﻿’/w ９９/m 昆明/ns 世博会/n 组委会/j 秘书长/n 、/w 云南省/ns 副/b 省长/n 刘/nr 京/nr 介绍/v 说/v ，/w ’/w ９９/m 世博会/j&lt;/p&gt;
&lt;h3 id="代码"&gt;代码
&lt;/h3&gt;&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt; 1
&lt;/span&gt;&lt;span class="lnt"&gt; 2
&lt;/span&gt;&lt;span class="lnt"&gt; 3
&lt;/span&gt;&lt;span class="lnt"&gt; 4
&lt;/span&gt;&lt;span class="lnt"&gt; 5
&lt;/span&gt;&lt;span class="lnt"&gt; 6
&lt;/span&gt;&lt;span class="lnt"&gt; 7
&lt;/span&gt;&lt;span class="lnt"&gt; 8
&lt;/span&gt;&lt;span class="lnt"&gt; 9
&lt;/span&gt;&lt;span class="lnt"&gt;10
&lt;/span&gt;&lt;span class="lnt"&gt;11
&lt;/span&gt;&lt;span class="lnt"&gt;12
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# -*- coding:utf-8 -*-&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;use&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fivethirtyeight&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;readFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; read file return a generator, each element is one line
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; Parameters
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>Lattice LSTM 中文NER</title><link>https://davidham3.github.io/blog/p/lattice-lstm-%E4%B8%AD%E6%96%87ner/</link><pubDate>Wed, 23 May 2018 16:54:12 +0000</pubDate><guid>https://davidham3.github.io/blog/p/lattice-lstm-%E4%B8%AD%E6%96%87ner/</guid><description>&lt;p&gt;ACL 2018，基于LSTM+CRF，用word2vec对字符进行表示，然后用大规模自动分词的预料，将词进行表示，扔进LSTM获得细胞状态，与基于字符的LSTM的细胞状态相结合，得到序列的隐藏状态，然后套一个CRF。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1805.02023" target="_blank" rel="noopener"
&gt;Chinese NER Using Lattice LSTM&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="摘要"&gt;摘要
&lt;/h2&gt;&lt;p&gt;我们调查了lattice-structured LSTM模型在中文分词上的表现，这个模型将输入的字符序列和所有可能匹配到词典中的词进行编码。对比基于字符的方法，我们的模型明显的利用了词与词序列的信息。对于基于词的方法，lattice LSTM不会受到错误分词的影响。门控循环细胞可以使模型从序列中选取最相关的字符和单词获得更好的NER结果。实验在各种数据集上都显示出lattice LSTM比基于词和基于字的LSTM要好，获得了最好的效果。&lt;/p&gt;
&lt;h2 id="引言"&gt;引言
&lt;/h2&gt;&lt;p&gt;信息抽取中最基础的任务，NER近些年受到了广泛的关注。NER以往被当作一个序列标注问题来解决，实体的边界和类别标签是同时进行预测的。当前最先进的英文命名实体识别的方法是使用集成进单词表示的字符信息的LSTM-CRF模型（Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018）。
中文NER与分词联系的很紧密。尤其是命名实体的边界也是词的边界。一个直观的想法是先分词，再标注词。然而这个pipeline会受到错误分词的影响，因为命名实体是分词中OOV中的很重要的一部分，而且不正确的实体边界划分会导致错误的NER。这个问题在open domain中很严重，因为跨领域的分词还是为解决的问题（Liu and Zhang, 2012; Jiang et al., 2013; Liu et al., 2014; Qiu and Zhang, 2015; Chen et al., 2017; Huang et al., 2017）。基于字符的方法比基于词的方法在中文NER中表现的好（He and Wang, 2008; Liu et al., 2010; Li et al., 2014）。
然而，基于字符的NER的一个缺点是，词与词的序列信息不能被完全利用到，然而这部分信息可能很有用。为了解决这个问题，我们通过使用一个lattice LSTM表示句子中的lexicon words，在基于字符的LSTM-CRF模型中集成了latent word information。如图1所示，我们通过使用一个大型的自动获取的词典来匹配一个句子，构建了一个词-字lattice。结果是，词序列，像“长江大桥”，“长江”，“大桥”可以用来在上下文中区分潜在的相关的命名实体，比如人名“江大桥”。
&lt;img src="https://davidham3.github.io/blog/images/lattice-lstm-%e4%b8%ad%e6%96%87ner/Fig1.PNG"
loading="lazy"
alt="Fig1"
&gt;
&lt;img src="https://davidham3.github.io/blog/images/lattice-lstm-%e4%b8%ad%e6%96%87ner/Fig2.PNG"
loading="lazy"
alt="Fig2"
&gt;
因为在lattice中有很多潜在的词-字路径，我们利用了一个lattice-LSTM结构来自动地控制句子的开始到结尾的信息流。如图2所示，门控细胞被用于动态规划信息从不同的路径到每个字符上。在NER数据上训练的lattice LSTM可以学习到如何从上下文中找到有用的单词，自动地提高NER的精度。对比基于字符的和基于单词的NER方法，我们的模型的优势在于利用在字符序列标签上的单词信息，且不会受到错误分词的影响。
结果显示我们的模型比字符序列标注模型和使用LSTM-CRF的单词序列标注模型都要好很多，在很多中文跨领域的NER数据集上都获得了很好的结果。我们的模型和数据在https://github.com/jiesutd/LatticeLSTM。&lt;/p&gt;
&lt;h2 id="相关工作"&gt;相关工作
&lt;/h2&gt;&lt;p&gt;我们的工作与当前处理NER的神经网络一致。Hammerton(2003)尝试解决使用一个单向的LSTM解决这个问题，这个第一个处理NER的神经网络。Collobert et al. (2011)使用了一个CNN-CRF的结构，获得了和最好的统计模型相当的结果。dos Santos et al. (2015)使用了字符CNN来增强CNN-CRF模型。大部分最近的工作利用了LSTM-CRF架构。Huang et al. (2015)使用手工的拼写特征；Ma和Hovy（2016）以及Chiu and Nichols（2016）使用了一个字符CNN来表示拼写的字符；Lample et al.（2016）使用一个字符LSTM，没有使用CNN。我们的baseline基于词的系统使用了与这些相似的架构。
字符序列标注是处理中文NER的主要方法（Chen et al., 2006b; Lu et al., 2016; Dong et al., 2016）。已经有讨论基于词的和基于字符的方法的统计的方法对比，表明了后者一般有更好的表现（He and Wang, 2008; Liu et al., 2010; Li et al., 2014）。我们发现有着恰当的表示设定，结论同样适用于神经NER。另一方面，lattice LSTM相比于词LSTM和字符LSTM是更好的一个选择。
如何更好的利用词的信息在中文NER任务中受到了持续的关注（Gao et al., 2015），分词信息在NER任务中作为soft features（Zhao and Kit, 2008; Peng and Dredze, 2015; He and Sun, 2017a），使用对偶分解的分词与NER联合学习也被人研究了（Xu et al., 2014），多任务学习（Peng and Dredze, 2016）等等。我们的工作也是，聚焦于神经表示学习。尽管上述的方法可能会被分词训练数据和分词的错误影响，我们的方法不需要一个分词器。这个模型不需要考虑多任务设定，因此从概念上来看就更简单。
NER可以利用外部信息。特别地，词典特征已经被广泛地使用了（Collobert et al., 2011; Passos et al., 2014; Huang et al., 2015; Luo et al., 2015）。Rei(2017)使用了一个词级别的语言模型目的是增强NER的训练，在大量原始语料上实现多任务学习。Peters et al.(2017)预训练了一个字符语言模型来增强词的表示。Yang et al.(2017b)通过多任务学习探索了跨领域和跨语言的知识。我们通过在大量自动分词的文本上预训练文本嵌入词典利用了外部信息，尽管半监督技术如语言模型are orthogonal to而且也可以在我们的lattice LSTM模型中使用。
Lattice结构的RNN可以被看作是一个树状结构的RNN（Tai et al., 2015）对DAG的自然扩展。他们已经有被用来建模运动力学（Sun et al., 2017），dependency-discourse DAGs(Peng et al., 2017)，还有speech tokenization lattice（Sperber et al., 2017）以及对NMT（neural machine translation）编码器的多粒度分词输出。对比现在的工作，我们的lattice LSTM在动机和结构上都是不同的。比如，对于以字符为中心的lattice-LSTM-CRF序列标注设计的模型，它有循环细胞但是没有针对词的隐藏向量。据我们所知，我们第一个设计了一个新型的lattice LSTM对字母和词进行混合的表示，也是第一个使用一个基于词的lattice处理不分词的中文NER任务的。&lt;/p&gt;
&lt;h2 id="模型"&gt;模型
&lt;/h2&gt;&lt;p&gt;我们跟从最好的英语NER模型（Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016），使用LSTM-CRF作为主要的网络结构。使用$s=c_1, c_2, &amp;hellip;, c_m$表示输入的句子，其中$c_j$表示第$j$个字符。$s$可以被看作一个单词序列$s=w_1, w_2, &amp;hellip;, w_n$，其中$w_i$表示序列中的第$i$个单词，由一个中文分词器获得。我们使用$t(i, k)$表示句子中第$i$个单词的第$k$个字符表示下标$j$。取图1的句子作为例子。如果分词结果是“南京市 长江大桥”，下标从1开始，那么$t(2, 1)=4$（长），$t(1, 3)=3$（市）。我们使用BIOES标记（Ratinov and Roth, 2009）对基于词和基于字的NER进行标记。
&lt;img src="https://davidham3.github.io/blog/images/lattice-lstm-%e4%b8%ad%e6%96%87ner/Fig3.PNG"
loading="lazy"
alt="Fig3"
&gt;&lt;/p&gt;
&lt;h3 id="基于字符的模型"&gt;基于字符的模型
&lt;/h3&gt;&lt;p&gt;基于字符的模型如图3(a)所示。它在$c_1, c_2, &amp;hellip;, c_m$上使用了LSTM-CRF模型。每个字符$c_j$表示为
&lt;/p&gt;
$$x^c\_j = e^c(c\_j)$$&lt;p&gt;
其中$e^c$表示一个字符嵌入到了lookup table中。
一个双向LSTM（与式11同结构）被使用在$x_1, x_2, &amp;hellip;, x_m$来获取从左到右的$\overrightarrow{h}^c_1, \overrightarrow{h}^c_2, &amp;hellip;, \overrightarrow{h}^c_m$和从右到左的$\overleftarrow{h}^c_1, \overleftarrow{h}^c_2, &amp;hellip;, \overleftarrow{h}^c_m$隐藏状态，这两个隐藏状态有两组不同的参数。每个字符的隐藏向量表示为
&lt;/p&gt;
$$h^c\_j = [\overrightarrow{h}^c\_j, \overleftarrow{h}^c\_j]$$&lt;p&gt;
一个标准的CRF模型被用在$h^c_1, h^c_2, &amp;hellip;, h^c_m$上来进行序列标注。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;字符+双字符
Character bigrams在分词中用来表示字符已经很有用了（Chen et al., 2015; Yang et al., 2017a）。我们提出了通过拼接双元字符嵌入和字符嵌入的基于字符的模型：
$$x^c\_j = [e^c(c\_j); e^b(c\_j, c\_{j+1})]$$
其中$e^b$表示一个character bigram lookup table。&lt;/li&gt;
&lt;li&gt;字符+softword
已经有实验表明使用分词作为soft features对于基于字符的NER模型可以提升性能（Zhao and Kit, 2008; Peng and Dredze, 2016）。我们提出的通过拼接分词标记嵌入和字符嵌入的带有分词信息的字符表示：
$$x^c\_j = [e^c(c\_j); e^s(seg(c\_j))]$$
其中$e^s$表示一个分词标签嵌入查询表。$seg(c_j)$表示一个分词器在字符$c_j$上给出的分词标签。我们使用了BMES策略来表示分词（Xue, 2003）
$$h^w\_i = [\overrightarrow{h^w\_i}, \overleftarrow{h^w\_i}]$$
与基于字符的情况类似，一个标准的CRF模型在序列标记中被用在了$h^w_1, h^w_2, &amp;hellip;, h^w_m$上。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="基于词的模型"&gt;基于词的模型
&lt;/h3&gt;&lt;p&gt;基于词的模型如图3（b）所示，它将word embedding $e^w(w_i)$作为每个词$w_i$的表示：
&lt;/p&gt;
$$x^w\_i = e^w(w\_i)$$&lt;p&gt;
其中$e^w$表示一个词嵌入查找表。一个双向LSTM被用来获取词序列$w_1, w_2, &amp;hellip;, w_n$上一个从左到右的隐藏状态$\overrightarrow{h}^w_1, \overrightarrow{h}^w_2, &amp;hellip;, \overrightarrow{h}^w_n$和一个从右到左的隐藏状态序列$\overleftarrow{h}^w_1, \overleftarrow{h}^w_2, &amp;hellip;, \overleftarrow{h}^w_n$。最后，对于每个词$w_i$，$\overrightarrow{h^w_i}$和$\overleftarrow{h^w_i}$会被拼在一起成为它的表示：
&lt;strong&gt;集成字符表示&lt;/strong&gt;
字符CNN（Ma and Hovy, 2016）和LSTM（Lample et al., 2016）两种方法都被用于过表示一个单词中的字符序列。我们在中文NER中对两个方法都进行了实验。我们使用$x^c_i$表示$w_i$中的字符，通过拼接$e^w(w_i)$和$x^c_i$可以获得一个新词的表示：
&lt;/p&gt;
$$x^w\_i = [e^w(w\_i; x^c\_i)]$$&lt;ol&gt;
&lt;li&gt;词+字符LSTM
将每个输入字符的嵌入记作$e^c(c_j)$，我们使用一个双向LSTM来学习词$w_i$的字符$c_{t(i, 1)}, &amp;hellip;, c_{t(i, len(i))}$的隐藏状态$\overrightarrow{h}^c_{t(i, 1)}, &amp;hellip;, \overrightarrow{h}^c_{t(i, len(i))}$和$\overleftarrow{h}^c_{t(i, 1)}, &amp;hellip;, \overleftarrow{h}^c_{t(i, len(i))}$，其中$len(i)$表示词$w_i$的字符个数。最后$w_i$的字符表示为：
$$x^c\_i = [\overrightarrow{h}^c\_{t(i, len(i))};\overleftarrow{h}^c\_{t(i, 1)}]$$&lt;/li&gt;
&lt;li&gt;词+字符LSTM'
我们调查了一种词+字符LSTM的变形，这个模型使用单向的LSTM对每个字符获取$\overrightarrow{h}^c_j$和$\overleftarrow{h}^c_j$。与Liu et al. (2018)的结构相似但是没有使用highway layer。使用了相同的LSTM结构和相同的方法集成字符隐藏状态进词嵌入中。&lt;/li&gt;
&lt;li&gt;词+字符CNN
我们使用标准的CNN（LeCun et al., 1989）应用在词的字符序列上获得字符表示$x^c_i$。将字符$c_j$的嵌入记为$e^c(c_j)$，向量$x^c_i$通过以下式子得到：
$$x^c\_i = \max\_{t(i,1) \leq j \leq t(i, len(i))}(W^T\_{CNN} \begin{bmatrix}
e^c(c\_{j-\frac{ke-1}{2}}) \\
... \\
e^c(c\_{j+\frac{ke-1}{2}})
\end{bmatrix}+ b\_{CNN})$$
其中，$W_{CNN}$和$b_{CNN}$和参数，$ke=3$是核的大小，$max$表示最大池化。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="lattice模型"&gt;Lattice模型
&lt;/h3&gt;&lt;p&gt;图2中展示了词-字lattice模型的整个结构，可以看作是基于字的模型的扩展，集成了基于词的细胞和用来控制信息流的额外的门。
图3（c）展示了模型的输入是一个字符序列$c_1, c_2, &amp;hellip;, c_m$，与之一起的还有所有字符序列，字符都能在词典$\mathbb{D}$中匹配到。如部分2中指示的，我们使用自动分词的大型原始语料来构建$\mathbb{D}$。使用$w^d_{b,e}$来表示一个起始字符下标为$b$，结尾字符下标为$e$，图1中的$w^d_{1,2}$是“南京（Nanjing）”，$w^d_{7,8}$是“大桥（Bridge）”。
模型涉及到了四种类型的向量，分别是输入向量、输出隐藏向量、细胞向量、门向量。作为基本的组成部分，一个字符输入向量被用来表示每个字符$c_j$，就像在基于字符的模型中：
$x^c_j = e^c(c_j)$
基本的循环结构是通过一个在每个字符$c_j$上的字符细胞向量$\mathbf{c}^c_j$和一个隐藏向量$\mathbf{h}^c_j$构造的，其中$\mathbf{c}^c_j$提供句子的开始到$c_j$的信息流，$\mathbf{h}^c_j$用于CRF序列标注。
基础的循环LSTM函数如下：
&lt;/p&gt;
$$
\begin{bmatrix}
i^c\_j \\
o^c\_j \\
f^c\_j \\
\widetilde{c}^c\_j
\end{bmatrix} =
\begin{bmatrix}
\sigma \\
\sigma \\
\sigma \\
tanh
\end{bmatrix}({W^c}^T
\begin{bmatrix}
x^c\_j \\
h^c\_{j-1}
\end{bmatrix}+b^c)
$$&lt;p&gt;
&lt;/p&gt;
$$c^c\_j = f^c\_j \odot c^c\_{j-1} + i^c\_j \odot \hat{c}^c\_j$$&lt;p&gt;
&lt;/p&gt;
$$h^c\_j = o^c\_j \odot tanh(c^c\_j)$$&lt;p&gt;
其中，$i^c_j$，$f^c_j$和$o^c_j$表示一组输入、遗忘和输出门。${w^c}^T$和$b^c$是模型参数。$\sigma()$表示sigmoid function。
不同于基于字符的模型，现在计算$c^c_j$的时候需要考虑句子中词典序列$w^d_{b,e}$。特别地，每个序列$w^d_{b,e}$被表示为：
&lt;/p&gt;
$$x^w\_{b,e} = e^w(w^d\_{b,e})$$&lt;p&gt;
其中$e^w$表示3.2节相同的词嵌入查询表。
此外，一个词细胞$c^w_{b,e}$用来表示$x^w_{b,e}$从句子开始的循环状态。$c^w_{b,e}$通过以下式子计算得到：
&lt;/p&gt;
$$
\begin{bmatrix}
i^w\_{b,e} \\
f^w\_{b,e} \\
\widetilde{c}^w\_{b,e}
\end{bmatrix} = \begin{bmatrix}
\sigma \\
\sigma \\
tanh
\end{bmatrix}({w^w}^T \begin{bmatrix}
x^w\_{b,e} \\
h^c\_b
\end{bmatrix} + b^w)
$$&lt;p&gt;
&lt;/p&gt;
$$c^w\_{b,e} = f^w\_{b,e} \odot c^c\_b + i^w\_{b,e} \odot \widetilde{c}^w\_{b,e}$$&lt;p&gt;
其中$i^w_{b,e}$和$f^w_{b,e}$是一组输入和遗忘门。对于词细v胞来说没有输出门因为标记只在字符层面上做。
有了$c^w_{b,e}$，会有很多路径可以使信息流向每个$c^c_j$。比如，在图2中，对于$c^c_7$的输入包含$x^c_7$（桥Bridge），$c^w_{6,7}$（大桥Bridge）和$c^w_{4,7}$（长江大桥Yangtze River Bridge）。我们将$c^w_{b,e}$和$b \in \lbrace b&amp;rsquo; \mid w^d_{b&amp;rsquo;,e} \in \mathbb{D}\rbrace$连接到细胞$c^c_e$。我们使用额外的门$i^c_{b,e}$对每个序列细胞$c^w_{b,e}$来控制它对$c^c_{b,e}$的贡献：
&lt;/p&gt;
$$i^c\_{b,e} = \sigma({w^l}^T \begin{bmatrix}
x^c\_e \\
c^w\_{b,e}
\end{bmatrix} + b^l)$$&lt;p&gt;
因此，$c^c_j$的计算变为：
&lt;/p&gt;
$$c^c\_j = \sum\_{b \in \lbrace b' \mid w^d\_{b',j} \in \mathbb{D}\rbrace } \alpha^c\_{b,j} \odot c^w\_{b,j} + \alpha^c\_j \odot \widetilde{c}^c\_j$$&lt;p&gt;
在上式中，门$i^c_{b,j}$和$i^c_{j}$的值被归一化到$\alpha^c_{b,j}$和$\alpha^c_j$，和为1。
&lt;/p&gt;
$$
\alpha^c\_{b,j} = \frac{exp(i^c\_{b,j})}{exp(i^c\_j)+\sum\_{b' \in \lbrace b'' \mid w^d\_{b'',j} \in \mathbb{D}\rbrace}exp(i^c\_{b',j})}
$$&lt;p&gt;
&lt;/p&gt;
$$
\alpha^c\_{j} = \frac{exp(i^c\_{j})}{exp(i^c\_j)+\sum\_{b' \in \lbrace b'' \mid w^d\_{b'',j} \in \mathbb{D}\rbrace}exp(i^c\_{b',j})}
$$&lt;p&gt;
最后的隐藏向量$h^c_j$仍然由之前的LSTM计算公式得到。在NER训练过程中，损失值反向传播到参数$w^c, b^c, w^w, b^w, w^l$和$b^l$使得模型可以动态地在NER标注过程中关注更相关的词。&lt;/p&gt;
&lt;h3 id="解码和训练"&gt;解码和训练
&lt;/h3&gt;&lt;p&gt;一个标准的CRF层被用在$h_1, h_2, &amp;hellip;, h_{\tau}$上面，其中$\tau$对于基于字符的模型来说是$n$，对于基于词的模型来说是$m$。一个标签序列$y = l_1, l_2, &amp;hellip;, l_{\tau}$的概率是
&lt;/p&gt;
$$
p(y \mid s) = \frac{exp(\sum\_i(w^{l\_i}\_{CRF} h\_i + b^{(l\_{i-1}, l\_i)}\_{CRF}))}{\sum\_{y'}exp(\sum\_i(w^{l'\_i}\_{CRF} h\_i + b^{(l'\_{i-1}, l'\_i)}\_{CRF}))}
$$&lt;p&gt;
这里$y&amp;rsquo;$表示一个任意标签序列，$W^{l_i}_{CRF}$是针对于$l_i$的模型参数，$b^{(l_{i-1},l_i)}_{CRF}$是针对$l_{i-1}$和$l_i$的偏置。
我们使用一阶维特比算法来寻找一个基于词或基于字符的输入序列中得分最高的标签序列。给定一组手动标注的训练数据$\lbrace (s_i, y_i)\rbrace \mid^N_{i=1}$，带有L2正则项的句子层面的log-likelihood作为loss，训练模型：
&lt;/p&gt;
$$L = \sum^N\_{i=1} log(P(y\_i \mid s\_i)) + \frac{\lambda}{2}\Vert \Theta \Vert^2$$&lt;p&gt;
其中，$\lambda$是L2正则项系数，$\Theta$表示了参数集合。&lt;/p&gt;</description></item><item><title>门控卷积网络语言建模</title><link>https://davidham3.github.io/blog/p/%E9%97%A8%E6%8E%A7%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E5%BB%BA%E6%A8%A1/</link><pubDate>Wed, 23 May 2018 10:54:44 +0000</pubDate><guid>https://davidham3.github.io/blog/p/%E9%97%A8%E6%8E%A7%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E5%BB%BA%E6%A8%A1/</guid><description>&lt;p&gt;ICML 2017，大体思路：卷积+一个线性门控单元，替代了传统的RNN进行language modeling，后来的Facebook将这个用于机器翻译，提出了卷积版的seq2seq模型。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1612.08083" target="_blank" rel="noopener"
&gt;Language Modeling with Gated Convolutional Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="摘要"&gt;摘要
&lt;/h2&gt;&lt;p&gt;当前流行的语言建模模型是基于RNN的。在这类任务上的成功经常和他们捕捉unbound context有关。这篇文章中我们提出了一个通过堆叠convolutions的finite context方法，卷积可以变得更有效因为他们可以在序列上并行。我们提出了一个新型的简单的门控机制，这个门控机制表现的要比Oord et al.(2016b)要好，我们也探究了关键架构决策的影响。我们提出的方法在WikiText-103上达到了最好的效果，even though it features long-term dependencies，在Google Billion Words上也达到了最好的效果。Our model reduces the latency to score a sentece by an order of magnitude compared to a recurrent baseline. 据我们所知，这是在大规模语言任务上第一次一个非循环结构的方法超越了强有力的循环模型。&lt;/p&gt;
&lt;h2 id="引言"&gt;引言
&lt;/h2&gt;&lt;p&gt;统计语言模型估计一个单词序列的概率分布，通过给定当前的单词序列，对下一个单词的概率进行建模
&lt;/p&gt;
$$P(w\_0, ..., w\_N) = P(w\_0)\prod^N\_{i=1}P(w\_i \mid w\_0, ..., w\_{i-1})$$&lt;p&gt;
其中$w_i$是单词表中的单词下标。语言模型对语音识别(Yu &amp;amp; Deng, 2014)和机器翻译(Koehn, 2010)来说是很重要的一部分。
最近，神经网络(Bengio et al., 2014; Mikolov et al., 2010; Jozefowicz et al., 2016)已经展示出了比传统n-gram模型(Kneser &amp;amp; Ney, 1995; Chen &amp;amp; Goodman, 1996)更好的语言模型。这些传统模型不能解决数据稀疏的问题，这个问题导致这些方法很难对大量的上下文进行表示，因此也不能回长范围的依赖进行表示。神经语言模型通过在连续空间中对词的嵌入解决了这个问题。当前最好的语言模型是基于LSTM（Hochreiter et al., 1997）的模型，LSTM理论上可以对任意长度的依赖进行建模。
在这篇文章中，我们引入了新的门控卷积网络，并且用它进行语言建模。卷积网络可以被堆叠起来来表示大量的上下文并且在越来越长的有着抽象特征（LeCun &amp;amp; Bengio, 1995）的上下文中提取层次特征。这使得这些模型可以通过上下文为$N$，卷积核宽度为$k$，$O(\frac{N}{k})$的操作对长时间的依赖关系进行建模。相反，循环网络将输入看作是一个链式结构，因此需要一个线性时间$O(N)$的操作。
层次的分析输入与传统的语法分析相似，传统的语法分析建立粒度增加的语法树结构，比如，包含名词短语和动词短语的句子，短语中又包含了更内在的结构（Manning &amp;amp; Schutze, 1999; Steedman, 2002）。层次结构也会让学习变得更简单，因为对于一个给定的上下文大小，相比链式结构，非线性单元的数量会减少，因此减轻了梯度消失的问题（Glorot &amp;amp; Bengio, 2010）。
现代的硬件对高度并行的模型支持的很好。在循环神经网络中，下一个输出依赖于之前的隐藏状态，而之前的隐藏状态在序列中元素上是不能并行的。然而，卷积神经网络对这个计算流程支持的很好因为卷积是可以在输入元素上同时进行的。
对于RNN来说想要达到很好的效果（Jozefowicz et al., 2016），门的作用很重要。我们的门控线性单元为深层的结构对梯度提供了一条线性的通道，同时又保留了非线性的特性，减少了梯度消失的现象。
我们展示了门控卷积网络比其他的已经发表的语言模型都要好，比如在Google Billion Word Benchmark（Chelba et al., 2013）上的LSTM。我们也评估了我们的模型在处理长范围依赖关系WikiText-103上的能力，在这个数据集上，模型是以段落为条件进行输入的，而不是一个句子，我们在这个数据集（Merity et al., 2016）上获得了最好的效果。最后，我们展示了门控线性单元获得了更好的精度以及相比于Oord et al., 2016的LSTM门收敛的更快。&lt;/p&gt;
&lt;h2 id="方法"&gt;方法
&lt;/h2&gt;&lt;p&gt;在这篇文章中我们引入了一种新的神经语言模型，这种模型使用门控时间卷积替代了使用在循环神经网络中使用的循环链接。神经语言模型（Bengio et al., 2003）提供了一种对每个单词$w_0, &amp;hellip;, w_N$的上下文表示$H=[h_0, &amp;hellip;, h_N]$用来预测下一个词的概率$P(w_i \mid h_i)$。循环神经网络$f$通过一个循环函数$h_i = f(h_{i-1}, w_{i-1})$计算$H$，这个循环函数本质上是一种不能并行处理的序列操作。
我们提出的方法使用函数$f$对输入进行卷积来获得$H = f \ast w$并且因此没有时间上的依赖，所以它能更好的在句子中的单词上并行计算。这个过程将会把许多前面出现的单词作为一个函数进行计算。对比卷积神经网络，上下文的大小是有限的，但是我们展示出了有限的上下文大小不是必须的，而且我们的模型可以表示足够大的上下文并表现的很好。
&lt;img src="https://davidham3.github.io/blog/images/%e9%97%a8%e6%8e%a7%e5%8d%b7%e7%a7%af%e7%bd%91%e7%bb%9c%e8%af%ad%e8%a8%80%e5%bb%ba%e6%a8%a1/Fig1.PNG"
loading="lazy"
alt="Fig1"
&gt;
图1展示了模型的架构。词通过一个嵌入的向量进行表示，这些表示存储在lookup table$\mathbf{D}^{\vert \mathcal{V} \vert \times e}$中，其中$\vert \mathcal{V} \vert$是词库中单词的数量，$e$是嵌入的大小。我们模型的输入是一个词序列$w_0, &amp;hellip;, w_N$，这个序列被词向量表示为$E = [D_{w_0}, &amp;hellip;, D_{w_N}]$。我们将隐藏层$h_0, &amp;hellip;, h_L$计算为
&lt;/p&gt;
$$h\_l(X) = (X \ast W + b) \otimes \sigma(X \ast V + c)$$&lt;p&gt;
其中，$m$，$n$分别是输入和输出的feature map的数量，$k$是patch size，$X \in \mathbb{R}^{N \times m}$是层$h_l$的输入（要么是词嵌入，要么是前一层的输出）,$W \in \mathbb{R}^{k \times m \times n}$，$b \in \mathbb{R}^n$，$V \in \mathbb{R}^{k \times m \times n}$，$c \in \mathbb{R}^n$是学习到的参数，$\sigma$是sigmoid function，$\otimes$是矩阵间的element-wise product。
当卷积输入时，我们注意$h_i$不包含未来单词的信息。我们通过移动卷积输入来防止卷积核看到未来的上下文（Oord et al., 2016a）来解决这个问题。特别地，我们在序列的开始加入了$k-1$宽度的0作为padding补全，假设第一个输入的元素是序列的开始元素，起始的标记我们是不预测的，$k$是卷积核的宽度。
每层的输出是一个线性变换$X \ast W + b$通过门$\sigma(X \ast W + b)$调节。与LSTM相似的是，这些门乘以矩阵的每个元素$X \ast W + b$，并且以层次的形式控制信息的通过。我们称这种门控机制为Gated Linear Units(GLU)。通过在输入$E$上堆叠多个这样的层，可以得到每个词$H = h_L \circ &amp;hellip; \circ h_0(E)$的上下文表示。我们将卷积和门控线性单元放在了一个preactivation residual block，这个块将输入与输出相加（He et al., 2015a）。这个块有个bottleneck结构，可以使计算更高效并且每个块有5层。
获得模型预测结果最简单的是使用softmax层，但是这个选择对于语料库很大和近似来说一般计算起来很慢，像noise contrastive estimation(Gutmann &amp;amp; Hyvarinen)或层次softmax(Morin &amp;amp; Bengio, 2005)一般更常用。我们选了后者的改良版adaptive softmax，这个算法将higher capacity分配给出现频率更高的单词，lower capacity分配给频率低的单词（Grave et al., 2016a）。这使得在训练和测试的时候内存占用更少且计算速度更快。&lt;/p&gt;
&lt;h2 id="门控机制"&gt;门控机制
&lt;/h2&gt;&lt;p&gt;门控机制控制了网络中信息流通的路径，在循环神经网络中已经证明了是非常有效的手段（Hochreiter &amp;amp; Schumidhuber, 1997）。LSTM通过输入和遗忘门控制分离的细胞使得LSTM获得长时间的记忆。这使得信息可以不受阻碍的流通多个时间步。没有这些门，信息会在通过时间步的转移时轻易地消失。与之相比，卷积神经网络不会遇到这样的梯度消失现象，我们通过实验发现卷积神经网络不需要遗忘门。
因此，我们认为模型只需要输出门，这个门可以控制信息是否应该通过这些层。我们展示了这个模型对语言建模很有效，因为它可以使模型选择预测下一个单词的时候哪个单词是相关的。和我们同时进行研究的，Oord et al.(2016b)展示了LSTM风格的门控机制，$tanh(X \ast W + b) \otimes \sigma(X \ast V + c)$在对图像进行卷积建模的有效性。后来，Kalchbrenner et al. (2016)在翻译和字符级别的语言建模上使用额外的门扩展了这个机制。
门控线性单元是一种简化的门控机制，基于Dauphin &amp;amp; Grangier(2015)对non-deterministic gates的研究，这个门可以通过和门组合在一起的线性单元减少梯度消失的问题。这个门尽管允许梯度通过线性单元进行传播而不发生缩放的变化，但保持了层非线性的性质。我们称之为gated tanh unit(GTU)的LSTM风格的门的梯度是：
&lt;/p&gt;
$$\nabla[tanh(X) \otimes \sigma(X)]=tanh'(X) \nabla X \otimes \sigma(X) + \sigma'(X) \nabla X \otimes tanh(X)$$&lt;p&gt;
注意到随着我们堆叠的层数的增加，它会渐渐地消失，因为$tanh&amp;rsquo;(X)$和$\sigma&amp;rsquo;(X)$这两个因数的数值范围在减小。相对来说，门控线性单元的梯度：
&lt;/p&gt;
$$\nabla [X \otimes \sigma(X)] = \nabla X \otimes \sigma(X) + X \otimes \sigma'(X) \nabla X$$&lt;p&gt;
有一条路径$\nabla X \otimes \sigma(X)$对于在$\sigma(X)$中的激活的门控单元没有减小的因数。这可以被理解为一个跳过乘法的连接帮助梯度传播过这些层。我们通过实验比较了不同的门策略后发现门控线性单元可以收敛地更快且困惑度的值更好。&lt;/p&gt;</description></item></channel></rss>