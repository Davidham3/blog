<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on Davidham的博客</title><link>https://davidham3.github.io/blog/tags/deep-learning/</link><description>Recent content in Deep Learning on Davidham的博客</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 06 Jan 2025 12:41:49 +0000</lastBuildDate><atom:link href="https://davidham3.github.io/blog/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>TrajDL</title><link>https://davidham3.github.io/blog/p/trajdl/</link><pubDate>Mon, 06 Jan 2025 12:41:49 +0000</pubDate><guid>https://davidham3.github.io/blog/p/trajdl/</guid><description>&lt;img src="https://trajdl.readthedocs.io/en/latest/_static/wide-logo.svg" alt="Featured image of post TrajDL" /&gt;&lt;p&gt;记录一下关于TrajDL的信息。TrajDL是我编写的第一个体系完整的python工具包，面向轨迹深度学习，目的是支持很多轨迹深度学习算法，包括轨迹表示学习、轨迹分类、下一位置预测等方法。&lt;/p&gt;
&lt;p&gt;目前已经在GitHub上开源，项目主页是：&lt;a class="link" href="https://github.com/Spatial-Temporal-Data-Mining/TrajDL" target="_blank" rel="noopener"
&gt;https://github.com/Spatial-Temporal-Data-Mining/TrajDL&lt;/a&gt;，已经发布了0.1.0版本，可以在pypi上下载，官方文档是：&lt;a class="link" href="https://trajdl.readthedocs.io/en/latest/" target="_blank" rel="noopener"
&gt;https://trajdl.readthedocs.io/en/latest/&lt;/a&gt;，可以通过这个文档查阅它的使用方法。&lt;/p&gt;
&lt;p&gt;目前0.1.0版本只支持了TULER、t2vec、GMVSAE、ST-LSTM 4个算法，当然还开发了比如CTLE、HIER这样的算法，但是还没有开发完。&lt;/p&gt;
&lt;p&gt;TrajDL的核心优势在于帮用户管理了数据集，用户可以快速下载公开数据集，并且通过TrajDL设计的dataset高效地完成实验，一些关键算子通过C++实现，数据集基于Arrow构建，所以整个框架在性能、内存使用上都有比较好的效果。整个训练过程构建在lightning上，所以即支持API开发训练代码，又支持通过配置文件进行训练。&lt;/p&gt;
&lt;p&gt;0.2.0版本主要会在轨迹相似度计算上面实现一些算法，目前还在开发中。&lt;/p&gt;</description></item><item><title>DeepTEA: Effective and Efficient Online Time-dependent Trajectory Outlier Detection</title><link>https://davidham3.github.io/blog/p/deeptea-effective-and-efficient-online-time-dependent-trajectory-outlier-detection/</link><pubDate>Fri, 09 Sep 2022 14:24:04 +0000</pubDate><guid>https://davidham3.github.io/blog/p/deeptea-effective-and-efficient-online-time-dependent-trajectory-outlier-detection/</guid><description>&lt;p&gt;PVLDB 2022. &lt;a class="link" href="https://www.vldb.org/pvldb/vol15/p1493-han.pdf" target="_blank" rel="noopener"
&gt;DeepTEA: Effective and Efficient Online Time-dependent Trajectory Outlier&lt;br&gt;
Detection&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;研究异常轨迹检测，目的是提取道路中车辆的异常移动。可以帮助理解交通行为，识别出租车欺诈。由于交通状况在不同时间不同地点会发生变化，所以这个问题具有挑战。本文提出了Deep-probabilistic-based time-dependent anomaly detection algorithm (DeepTEA)。使用深度学习方法从大量轨迹里面获得time-dependent outliners，可以处理复杂的交通状况，并精确地检测异常现象。本文还提出了一个快速的近似方法，为了在实时环境下捕获到异常行为。相比SOTA方法，本文方法提升了17.52%，并且可以处理百万计的轨迹数据。&lt;/p&gt;
&lt;h1 id="2-problem-definition"&gt;2. Problem Definition
&lt;/h1&gt;&lt;p&gt;Definition 1(Trajectory). 轨迹点$p_{t_i}$是一个三元组$(t_i, x, y)$，分别是时间戳、纬度、经度。轨迹$T$是一个轨迹点的有序序列，其中$t_1 &amp;lt; \dots &amp;lt; t_i &amp;lt; \dots &amp;lt; t_n$。&lt;/p&gt;
&lt;p&gt;轨迹异常检测分为两类，一类是只考虑与正常路线不同的异常轨迹。另一种是考虑与time-dependent的正常路线不同的异常轨迹。&lt;/p&gt;
&lt;p&gt;Definition 2(Time-dependent Trajectory Outlier)。给一条轨迹$T$，起点$S_T$，终点是$D_T$，还有travel时间，一个time-dependent轨迹异常定义为：相同的$S_T$和$D_T$以及相同的出发、到达时间下的轨迹里面，一个很稀有的、不同于其他轨迹的轨迹。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deeptea-effective-and-efficient-online-time-dependent-trajectory-outlier-detection/Fig1.jpg"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;举例，如果一个轨迹在2016年10月1日的上午10点出发，11点到达，那么一条稀有的轨迹且和相同时间相同OD的轨迹不同的轨迹就是这个Time-dependent Trajectory Outlier.&lt;/p&gt;
&lt;p&gt;Problem 1(Online Time-dependent Trajectory Outlier Detection)。给定一条正在前进的轨迹$T$，实时计算并更新这条轨迹是时间依赖的异常轨迹的概率。&lt;/p&gt;
&lt;h1 id="3-the-deeptea-model"&gt;3. The DeepTEA Model
&lt;/h1&gt;&lt;h2 id="31-framework"&gt;3.1 Framework
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deeptea-effective-and-efficient-online-time-dependent-trajectory-outlier-detection/Fig2.jpg"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;p&gt;给定轨迹$T$，在旅行过程中推算latent traffic pattern $q(z \mid T)$。轨迹观测值 $\tau$ 反映时间依赖的轨迹转移，可以在 inference network 里面用来建模latent time-dependent route $r$。之后，time-dependent route $r$用来生成轨迹观测值$\tau$。&lt;/p&gt;
&lt;h2 id="32-latent-traffic-pattern-inference"&gt;3.2 Latent Traffic Pattern Inference
&lt;/h2&gt;&lt;p&gt;latent traffic pattern $z$，表示旅途过程中的动态交通状况，比如 ${\text{smooth} \rightarrow \text{congested} \rightarrow \text{smooth} }$，或者${\text{congested} \rightarrow \text{smooth} }$。&lt;/p&gt;
&lt;h3 id="321-challenges"&gt;3.2.1 Challenges
&lt;/h3&gt;&lt;p&gt;给定轨迹 $T$，我们想基于 $T$ 的空间转移，推算出 latent traffic pattern $q(z \mid T)$。举个例子，一个远距离的移动表明当时的交通状况是通畅的。但是一个轨迹 $T$ 可能表示一些随机行为，比如停车休息，这种行为不能表明当时的交通状况。这是在表示实际交通模式时的第一个挑战。第二个挑战是交通状态在不同的地方，不同的时间是不同的，动态的。同一时间，不同OD也是不一样的。而且在整个旅途中的交通状态变化也是剧烈的。可能开始的时候顺畅，结束的时候拥堵。捕获交通状态很重要，因为它对时间依赖的正常路径影响很大。&lt;/p&gt;
&lt;h3 id="322-design"&gt;3.2.2 Design
&lt;/h3&gt;&lt;p&gt;为了解决第一个挑战，我们从时间 $t_i$ 的一组轨迹 ${ T_{t_i} }$ 里面学习latent traffic pattern $z$，而不是单条轨迹 $T$。这里我们用 time point series 表示旅行时间。为了从 ${ T_{t_i} }$ 里面很好地组织交通信息，我们使用一个 map grid matrix $Z_{t_i}$，这里面每个单元表示平均速度，用这个对 $t_i$ 的交通状态建模。从图2可以看出，红色表示速度低，表示拥堵。绿色表示畅通。黄色表示即将拥堵，平均速度介于红绿之间。为了解决同一时间不同位置交通状态的多样性，我们用CNN建模。对于没有车辆的区域，CNN可以从有车辆的单元学到信息，而不是将他们表示为缺失值。因为实时状态下交通状况变得很频繁，我们用RNN捕获这种不断变化的动态性，解决第二个挑战。交通状态的变化可以通过RNN很好的建模。然后我们用一个高斯分布和RNN的隐藏状态，推算 latent traffic pattern $z$。CNN+RNN选用了 Convolutional LSTM。&lt;/p&gt;
&lt;p&gt;我们用上面的随时间变化的动态交通状态来推算latent traffic pattern $z$。这里的想法是交通状态会因为复杂的实时特征发生变化，比如信号灯、事故、早晚高峰。因此，我们在DeepTEA里面会随时间变化更新$Z$，表示为$Z_{t_i}$，表示轨迹点$p_{t_i}$在时间$t_i$的交通状态。我们会从实际交通状况$Z = { Z_{t_i}, Z_{t_{i+1}}, \dots, Z_{t_{i+n}} }$里面推算latent traffic pattern $z$。这里面的交通状况对应的时间分别是对应轨迹点${ p_t, p_{t+1}, \dots, p_{t_{i+n}} }$的时间。对于真实交通状况$Z$，我们可以得到轨迹$T$经过的时间的平均速度。换句话说就是，真实交通状况$Z_{t_i}$是一个平均速度矩阵，它包含了整个城市在$t_i$的移动状态。如果两个轨迹点之间的时间差很小，那么$Z_{t_i}$和$Z_{t_{i+1}}$可能会很相似。这样的话，我们需要把速度按时段提前聚合起来，就取平均速度，比如10分钟的时段，而不再使用时间点。为了减轻稀疏的问题，我们使用CNN将有车辆的位置的信息传播到没有车辆没数据的位置上。为了捕获不同时段的动态变化，我们用RNN建模时间维度的交通转移。这样，spatial traffic correlation和temporal transition通过$f_1(Z)$来捕获：&lt;/p&gt;
$$
\tag{1} f\_1(Z) = \text{RNN}(\text{CNN}(Z)),
$$&lt;p&gt;这里函数$f_1(\cdot)$是一个CNN+RNN，CNN对每个$Z_i$都使用，然后用RNN对他们建模，捕获traffic transition。&lt;/p&gt;
&lt;p&gt;为了让模型具有生成能力，并且对交通状态的不确定性建模，在给定实际交通状态$Z$的时候，我们用高斯分布对latent traffic pattern $z$建模，可以用来在给定轨迹$T$时近似latent traffic pattern $z$的分布，如公式2所示。我们将参数记为$\phi$，&lt;/p&gt;
$$
\tag{2} q\_\phi(z \mid T) \coloneqq q\_\phi(z \mid Z) = \mathcal{N}(\mu\_Z, \text{diag}(\sigma^2\_Z)),
$$&lt;p&gt;均值$\mu_Z$和标准差$\sigma_Z$通过MLP函数 $g_1(f_1(Z))$得到，参数是$\phi = { f_1(\cdot), g_1(\cdot) }$。&lt;/p&gt;
&lt;p&gt;这种方式在给定轨迹$T$的时候可以很好的推断出latent traffic pattern $z$。在训练阶段，参数$\phi$可以学到如何捕获latent traffic pattern $z$，而且能表示交通状态的多样性和动态性。&lt;/p&gt;
&lt;h2 id="33-latent-time-dependent-route-inference"&gt;3.3 Latent Time-dependent Route Inference
&lt;/h2&gt;&lt;h3 id="331-challenges"&gt;3.3.1 Challenges
&lt;/h3&gt;&lt;p&gt;轨迹$T$不仅可以表示位置信息，还可以表示两个轨迹点之间转移的latent traffic pattern $z$。相比只最大化位置信息的似然，对位置和latent traffic pattern $z$同时做更informative，因为它可以反映在时间依赖的交通状态下的轨迹转移。&lt;/p&gt;
&lt;h3 id="332-design"&gt;3.3.2 Design
&lt;/h3&gt;&lt;p&gt;一条轨迹 $T$ 不仅能反映位置 $p_{t_i}$，还能基于两个连续轨迹点 $p_{t_{i-1}}$ 和 $p_{t_i}$ 之间的转移，传递出 latent traffic pattern $z$。这里我们用 $o(p_{t_i}, z)$ 表示轨迹 $T$ 背后的观测值 $\tau_i$。这里希望用一个神经网络处理观测值 $p_{t_i}$ 和 $z$：&lt;/p&gt;
$$
\tag{3} \tau\_i = o(p\_{t\_i}, z) = f\_2(p\_{t\_i}, z) = \text{NN}(p\_{t\_i}, z),
$$&lt;p&gt;我们使用一个神经网络学习 latent traffic pattern $z$ 的观测值 $p_{t_i}$：&lt;/p&gt;
$$
\tag{4} \text{NN}(p\_{t\_i}, z) = W p\_{t\_i} + Q z,
$$&lt;p&gt;$\text{NN}$的参数是$W$和$Q$。&lt;/p&gt;
&lt;p&gt;然后，我们学习一条轨迹 $T$ 经过的 latent time-dependent route $r$。我们解释过，轨迹 $T$ 不仅能表示轨迹点 $p_{t_i}$ 的位置信息，还能指明两个轨迹点之间转移的 latent traffic pattern $z$。latent time-dependent route $r$ 的含义可以解释为：高峰时间段城市路段的交通状态是拥堵的，驾驶员通常会上高速，因为那里会畅通。&lt;/p&gt;
&lt;p&gt;轨迹 $T$ 经过的 latent time-dependent route $r$ 的表示为：&lt;/p&gt;
$$
\tag{5} r\_T \sim q\_\gamma (r \mid T),
$$&lt;p&gt;$\gamma$ 是推测 latent time-dependent route $r$ 时的参数。&lt;/p&gt;
&lt;p&gt;基于之前的轨迹点和 latent traffic pattern $z$，我们可以用RNN获得轨迹观测值之间的转移，RNN记为 $f_3$，这里使用GRU：&lt;/p&gt;
$$
\tag{6} h\_i = f\_3 (h\_{i-1}, \tau\_i),
$$&lt;p&gt;$h_{i-1}$ 是之前观测值 $\tau_{i-1}$ 的隐藏状态，也就是轨迹点 $p_{t_{i-1}}$ 带着 latent traffic pattern $z$。&lt;/p&gt;
&lt;p&gt;对于轨迹观测值的不确定性，我们通过高斯分布建模 $q_\gamma (r \mid T)$：&lt;/p&gt;
$$
\tag{7} q\_\gamma (r \mid T) = \mathcal{N} (\mu\_T, \text{diag}(\sigma^2\_T)),
$$&lt;p&gt;我们用一个神经网络 $g_3(h_n)$ 来学习均值和标准差。&lt;/p&gt;
&lt;p&gt;为了在 latent traffic pattern 里面区分正常的轨迹转移和异常的轨迹转移，需要设计一个模块对轨迹里 latent time-dependent normal route建模，这里 $z$ 会提供 time-dependent traffic 信息。使用高斯分布：&lt;/p&gt;
$$
\tag{8} p\_\gamma (r \mid k, z) = \mathcal{N}(\mu\_r, \text{diag}(\sigma^2\_r)),
$$&lt;p&gt;$k$ 表示 latent time-dependent route 的类型，服从多项式分布：&lt;/p&gt;
$$
\tag{9} p\_\gamma (k) = \text{Mult} (\pi),
$$&lt;p&gt;$\pi$ 是多项式分布的参数。然后，趋近 $q_\gamma (r \mid T)$ 的均值的 latent time-dependent route 是time-dependent normal route。&lt;/p&gt;
&lt;p&gt;然后，推断网络可以从给定的轨迹 $T$ 里面推算 latent time-dependent route $r$，latent time-dependent route type $k$，还有 latent traffic pattern $z$ 为 $q_{\gamma,\phi}(r, k, z \mid T)$。通过使用 mean-field approximation，可以分解为：&lt;/p&gt;
$$
\tag{10} q\_{\gamma,\phi}(r, k, z \mid T) = q\_\gamma(r \mid T) \ q\_\phi(z \mid T) \ q\_\gamma (k \mid T),
$$&lt;p&gt;$q_\gamma (k \mid T)$ 可以转换为在给定 轨迹 $T$ 经过 latent time-dependent route $r$ 的条件下，route type $k$ 的分布：&lt;/p&gt;
$$
\tag{11} q\_\gamma (k \mid T) \coloneqq p\_\gamma (k \mid r\_T) = \frac{p\_\gamma (k) p\_\gamma (r\_T \mid k)}{\sum^K\_{i=1} p\_\gamma (k\_i) p\_\gamma (r\_T \mid k\_i)},
$$&lt;p&gt;$K$ 是一个超参数，表示 route 类型的个数。&lt;/p&gt;
&lt;p&gt;因此，推断网络可以从轨迹 $T$ 的观测值 $o(p_{t_i}, z)$ 推断出 latent time-dependent route $r$。$\gamma = { f_2(\cdot), f_3(\cdot), g_3(\cdot), \pi, \mu_r, \sigma_r }$ 这些都是参数。&lt;/p&gt;
&lt;h2 id="34-trajectory-observation-generation"&gt;3.4 Trajectory Observation Generation
&lt;/h2&gt;&lt;p&gt;生成轨迹观测值的目标是给定 latent time-dependent route $r$，time-dependent route type $k$ 和 latent traffic pattern $z$ 时，最大化生成轨迹观测值 $\tau_i$ 的概率，也就是 $o(p_{t_i}, z)$。这个概率记为 $p_\theta (T \mid r, z, k)$，$\theta$ 表示用于生成过程的参数。从对称的角度来看，我们用RNN来生成轨迹观测值 $\tau_i$，也就是 $o(p_{t_i}, z)$：&lt;/p&gt;
$$
\tag{12} \begin{align} \eta\_i &amp;= f\_4 (\tau\_i, \eta\_{i-1}) \\ &amp;= f\_4(o(p\_{t\_i}, z), \eta\_{i-1}) \\ &amp;= \text{RNN} (o(p\_{t\_i}, z), \eta\_{i-1}), i = 1, 2, \dots, n, \ and \ \eta\_o = r, \end{align}
$$&lt;p&gt;RNN的起始输入是 $\eta_0$。从 $\eta_1$ 开始，输入变成上一个隐藏状态 $\eta_{i-1}$ 和轨迹观测值 $o(p_{t_i}, z)$。因此，观测值 $\tau_i$，也就是 $p_{t_i}$ 在 latent traffic pattern $z$ 的时候，可以通过下面的公式生成：&lt;/p&gt;
$$
\tag{13} \begin{align} \tau\_i &amp;= o(p\_{t\_i}, z) \sim p\_\theta (o(p\_{t\_i}, z) \mid o(p\_{1:i-1}, z), r) \\ &amp;= p\_\theta(\tau \mid \eta\_{i-1}) \\ &amp;= \text{Mult}(\text{softmax}(g\_4 (\eta\_{i-1}))), \end{align}
$$&lt;p&gt;$g_4(\cdot)$是一个函数，把输出映射到网格的个数。softmax 用来把概率的和变成1。然后轨迹观测值 $\tau_i$ 可以通过多项式分布生成。&lt;/p&gt;
&lt;p&gt;因此，轨迹观测值 $\tau_i$，也就是 $o(p_{t_i}, z)$，可以基于 latent time-dependent route $r$，route type $k$ 和 latent traffic pattern $z$ 生成。我们给生成用的参数记为 $\theta = { f_4(\cdot), g_4(\cdot) }$。这些参数会在训练的过程中学到。&lt;/p&gt;
&lt;h2 id="35-optimization"&gt;3.5 Optimization
&lt;/h2&gt;&lt;p&gt;我们上面讲了，轨迹观测值不仅能反映位置信息，还能基于两个连续的轨迹点的转移传递出 latent traffic pattern。因此，目标函数是最大化观测到的轨迹的边缘对数似然：&lt;/p&gt;
$$
\tag{14} \log p\_\theta(T^{(1)}, T^{(2)}, \dots, T^{(N)}) \coloneqq \log p\_\theta (\tau^{(1)}, \tau^{(2)}, \dots, \tau^{(N)}).
$$&lt;p&gt;我们通过最大化ELBO来优化上面的边缘对数似然函数：&lt;/p&gt;
$$
\tag{15} \log p\_\theta (T) \geq \text{ELBO} = \mathcal{L}(\phi, \gamma, \theta; T).
$$&lt;p&gt;轨迹 $T$ 的边缘对数似然函数的 ELBO 通过下面的公式计算：&lt;/p&gt;
$$
\tag{16} \begin{align} \mathcal{L}(\phi, \gamma, \theta; T) &amp;= \mathbb{E}\_{q\_{\gamma, \phi}(r, k, z \mid T)}[ \log \frac{p\_{\phi, \gamma, \theta}(r, k, z, T)}{q\_{\gamma, \theta}(r, k, z \mid T)}] \\ &amp;= - \mathbb{E}\_{q\_\gamma(r \mid T)} D\_{KL} (q\_\gamma (k \mid T) \Vert p\_\gamma (k)) \\ &amp; - \mathbb{E}\_{q\_\gamma (k \mid T)} D\_{KL} ( q\_\gamma (r \mid T) \Vert p\_\gamma (r \mid k, z)) \\ &amp; - D\_{KL} (q\_\phi (z \mid T) \Vert p\_\phi(z)) + \mathbb{E}\_{q\_{\gamma, \phi}(r, k, z \mid T)} \log p\_\theta (T \mid r, z, k), \end{align}
$$&lt;p&gt;其中，$p_\theta(z)$ 是 latent traffic pattern $z$ 的先验概率。生成网络 $\log p_\theta(T \mid r, z, k)$ 可以通过下面公式计算：&lt;/p&gt;
$$
\tag{17} \log p\_\theta (T \mid r, z, k) = \sum^n\_{i=1} \log p\_\theta (\tau\_i \mid \tau\_{1: i-1}, r, z, k)
$$&lt;p&gt;整个训练过程的算法如算法1所示。在训练过程中，模型参数通过优化轨迹 $T$ 的 ELBO 来学习。然后这些学到的参数会用于 online anomaly detection，后面会介绍。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deeptea-effective-and-efficient-online-time-dependent-trajectory-outlier-detection/Algo1.jpg"
loading="lazy"
alt="Algo1"
&gt;&lt;/p&gt;
&lt;h2 id="36-complexity-analysis"&gt;3.6 Complexity Analysis
&lt;/h2&gt;&lt;p&gt;训练 DeepTEA 的复杂度是 $O(N \cdot (d_{Z_1} d_{Z_2} \bar{V} + \bar{n}))$，$N$ 是轨迹数，$d_{Z_1}$ 和 $d_{Z_2}$ 是 $Z$ 的大小，$\bar{V}$ 是时间间隔的平均个数，$\bar{n}$ 是轨迹的平均长度。&lt;/p&gt;
&lt;h1 id="online-trajectory-outlier-detection-by-deeptea"&gt;Online Trajectory Outlier Detection by DeepTEA
&lt;/h1&gt;&lt;p&gt;基于算法1学习得到的参数，当下一个轨迹观测值 $\tau_{i+1}$ 实时过来的时候，异常分数会实时更新。这个过程要快。而且直到轨迹完成，这个异常分数都要更新。&lt;/p&gt;
&lt;h2 id="41-online-detection-by-generation"&gt;4.1 Online Detection by Generation
&lt;/h2&gt;&lt;p&gt;图3展示了在线异常轨迹检测的步骤。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deeptea-effective-and-efficient-online-time-dependent-trajectory-outlier-detection/Fig3.jpg"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;我们通过学到的网络生成观测到的轨迹来检测异常。latent time-dependent route 的分布 $q_\gamma (r \mid T)$ 可以通过参数 $\gamma$ 来计算。latent traffic pattern $z$ 可以通过参数 $\phi$ 和 $Z$ 获得。给定 $q_\gamma (r \mid T)$ 里面第 $k$ 个均值 $u_k$，我们用 RNN 生成轨迹观测值：&lt;/p&gt;
$$
\tag{18} \eta\_i = f\_4(\tau\_i, \eta\_{i-1}) = \text{RNN} (\tau\_i, \eta\_{i - 1}), i = 1, 2, \dots, n, \text{and} \ \eta\_0 = u\_k,
$$&lt;p&gt;RNN的起始输入是 $\eta_0$，这里设置为 $u_k$。从 $\eta_1$ 开始，输入标称隐藏状态 $\eta_{i-1}$ 和轨迹观测值 $\tau_i$，即 $o(p_{t_i}, z)$。因此 $\tau_{i+1}$ 可以用下面的公式生成：&lt;/p&gt;
$$
\tag{19} p\_\theta (\tau\_{i+1} \mid \tau\_{i:i}, u\_k) = \text{softmax}(g\_4 (\eta\_{i-1})),
$$&lt;p&gt;$g_4(\cdot)$ 是用来把输出映射到网格数的函数。&lt;/p&gt;
&lt;p&gt;给定学到的 $q_\gamma (r \mid T)$ 和 latent traffic pattern $z$，轨迹 $T$ 的实时异常分数 $s_a(\tau_{i:i})$ 可以计算为 1 - 生成轨迹观测值 $\tau_{i:i}$ 的似然，即 ${ \tau_1 \rightarrow \tau_2 \rightarrow \dots \tau_i }$：&lt;/p&gt;
$$
\tag{20} s\_a(\tau\_{i:i}) = 1 - \arg \max\_k \exp [\frac{\sum^n\_{i=1} \log p\_\theta (\tau\_i \mid \tau\_{1:i-1}, u\_k)}{n}]
$$&lt;p&gt;在线上场景下，给定之前的轨迹观测值 $\tau_{i:i}$，下一个轨迹观测值的异常分数可以通过之前这个数来计算：&lt;/p&gt;
$$
\tag{21} s\_a(\tau\_{i:i+1}) = 1 - \arg \max\_k \exp [\frac{\log p\_\theta (\tau\_{1:i} \mid u\_k) p\_\theta(\tau\_{i+1} \mid \tau\_{1:i}, u\_k)}{i + 1}]
$$&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deeptea-effective-and-efficient-online-time-dependent-trajectory-outlier-detection/Algo2.jpg"
loading="lazy"
alt="Algo2"
&gt;&lt;/p&gt;
&lt;p&gt;算法2是在线检测的过程。输入是轨迹 $T$，参数是从算法1学到的。对于新来的轨迹观测值 $\tau_{i+1}$，如果 latent traffic pattern $z$ 变了，那我们就更新 $z$。然后基于 $\tau_{1:i}$ 计算 $\tau_{1:i+1}$。最后返回异常分数。&lt;/p&gt;
&lt;h2 id="42-complexity-analysis"&gt;4.2 Complexity Analysis
&lt;/h2&gt;&lt;p&gt;整个检测的复杂度是 $\mathcal{O}(d_{Z_1} d_{Z_2})$，$d_{Z_1}$ 和 $d_{Z_2}$ 是 $Z$ 的大小。&lt;/p&gt;
&lt;h1 id="5-the-deeptea-a-model-approximate-online-detection"&gt;5 The DeepTEA-A Model: Approximate Online Detection
&lt;/h1&gt;&lt;h2 id="51-approximation-algorithm"&gt;5.1 Approximation Algorithm
&lt;/h2&gt;&lt;h3 id="511-challenge"&gt;5.1.1 Challenge
&lt;/h3&gt;&lt;p&gt;基于 $\tau_{1:i}$ 的异常分数更新依赖 $t_{i+1}$ 的交通状态矩阵 $Z$，如果路网很大，这里耗时会长。&lt;/p&gt;
&lt;h3 id="512-design"&gt;5.1.2 Design
&lt;/h3&gt;&lt;p&gt;受 GM-VSAE 的启发，本文提出了近似算法。使用 $\tau_1$ 的时段的交通状态矩阵 $Z$ 作为整个 trip 过程的交通状况的近似。这样，$Z$ 只要在第一个轨迹观测值 $\tau_1$ 的时候算一下就好了。online 更新的时候就不需要再算这个了。&lt;/p&gt;
&lt;p&gt;给定一个起点 $S_T$ 和终点 $D_T$，从 $q(k \mid S_T, D_T, z_{S_T})$ 这里面取出最优的 latent route type $k$ 来近似最有 latent route pattern $u_k$，这需要从 $q_\gamma (r \mid T)$ 的 $k$ 个均值里面找，$Z_{S_T}$ 是 trip 开始时的交通状况。这样，最优的 latent route type $k$ 在 trip 一开始的时候就能拿到。从第二个轨迹观测值开始，这个 $k$ 就不需要再算了。&lt;/p&gt;
&lt;p&gt;对于起点 $S_T$，交通状况 $Z_{S_T}$ 的隐藏状态可以通过下面公式计算：&lt;/p&gt;
$$
\tag{22} f\_1(Z\_{S\_T}) = \text{CNN} (Z\_{S\_T}),
$$&lt;p&gt;然后 $z_{S_T}$ 可以从 $q_\phi (z_{S_T} \mid Z_{S_T})$ 里面采样得到：&lt;/p&gt;
$$
\tag{23} q\_\phi (z\_{S\_T} \mid Z\_{S\_T}) = \mathcal{N}(\mu\_{Z\_{S\_T}}, diag(\sigma^2\_{Z\_{S\_T}})),
$$&lt;p&gt;然后，$\tau_{S_T}$ 可以通过 $f_2(\cdot)$ 得到：&lt;/p&gt;
$$
\tag{24} \tau\_{S\_T} = f\_2(S\_T, z\_{S\_T}) = \text{NN}(S\_T, z\_{S\_T}) = W S\_T + Q z\_{S\_T},
$$&lt;p&gt;同理，$\tau_{D_T}$ 按同样的方式计算。&lt;/p&gt;
&lt;p&gt;然后 $q(k \mid S_T, D_T, z_{S_T})$ 通过 MLP 计算：&lt;/p&gt;
$$
\tag{25} q(k \mid S\_T, D\_T, z\_{S\_T}) = \text{softmax}(f\_t(\tau\_{S\_T}, \tau\_{D\_T})),
$$&lt;p&gt;$f_5$ 就是 MLP。参数记为 $\delta = { f_5 (\cdot) }$。&lt;/p&gt;
&lt;p&gt;公式20里面，为了获得最优的 $k$，需要跑 $k$ 次。一个简单的方法是通过 $q_\gamma (k \mid T)$ 从轨迹 $T$ 里面找到最优的 $k$。因此 $q(k \mid S_T, D_T, z_{S_T})$ 和 $q_\gamma (k \mid T)$ 要尽可能的相近。我们用交叉熵最小化两个分布的差别：&lt;/p&gt;
$$
\tag{26} l\_k = - \sum^K\_{k=1} q\_\gamma (k \mid T) \log q(k \mid S\_T, D\_T, z\_{S\_T}),
$$&lt;p&gt;这个交叉熵 $l_k$ 会和公式 16 的ELBO同时训练。然后在线检测阶段的时候，最优的 $k$ 是 $q(k \mid S_T, D_T, z_{S_T})$ 里面最高概率的那个。然后直接就能拿到最优 latent time-dependent route $u_k$。&lt;/p&gt;
&lt;p&gt;需要注意的是，近似算法的训练过程和算法1不一样。首先，交通状况的使用不一样，近似算法只用了 $Z_{S_T}$。第二，公式26，是一个 co-training 过程，为了近似两个分布。训练后得到模型参数 $\phi, \gamma, \theta, \delta$。整个训练过程如算法3所示。先从 $Z_{S_T}$ 里面获得 $z_{S_T}$。然后获得最优的 latent time-dependent route $u_k$，这个数只要在轨迹开始的时候算一下就好了。然后基于 $p_\theta(\tau_{1:i} \mid u_k)$ 更新异常分数就好了。&lt;/p&gt;
&lt;h2 id="52-complexity-analysis"&gt;5.2 Complexity Analysis
&lt;/h2&gt;&lt;p&gt;在线检测的复杂度 $\mathcal{O}(d_{h_t}(d_{h_t} + d_{\tau_i}))$。这项是新的轨迹观测值 $\tau_{i+1}$ 到来的时候 RNN 的变换过程的复杂度。因为 $d_{h_t}, d_{\tau_i}$ 是常数，所以近似算法的复杂度是 $\mathcal{O}(1)$&lt;/p&gt;</description></item><item><title>Modeling The Intensity Function Of Point Process Via Recurrent Neural Networks</title><link>https://davidham3.github.io/blog/p/modeling-the-intensity-function-of-point-process-via-recurrent-neural-networks/</link><pubDate>Wed, 25 May 2022 15:01:51 +0000</pubDate><guid>https://davidham3.github.io/blog/p/modeling-the-intensity-function-of-point-process-via-recurrent-neural-networks/</guid><description>&lt;p&gt;AAAI 2017, Intensity RNN: &lt;a class="link" href="https://arxiv.org/pdf/1705.08982.pdf" target="_blank" rel="noopener"
&gt;Modeling The Intensity Function Of Point Process Via Recurrent Neural Networks&lt;/a&gt;。相比RMTPP，用LSTM。然后模型加了一个时间序列模块，主要是为了支持有时间序列信息的数据集。然后计算事件发生时间的损失时，用了一个高斯核函数。本质上还是MSE，没啥区别。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 Introduction
&lt;/h1&gt;&lt;p&gt;${z_i, t_i }^N_{i=1}$表示事件数据。事件序列的时间间隔不像时间序列那样相等。点过程是研究事件序列的重要方法。&lt;/p&gt;
&lt;p&gt;最近一些机器学习方法在数学公式和优化技术上做了一些巧妙的修改，还有一些新的条件强度函数建模方法，主要是用一些数据集上的先验知识来刻画数据的性质。点过程的主要缺点是表达能力受限，模型太弱了，难以捕获复杂的数据。而且如果模型选错了的话，效果会很差。&lt;/p&gt;
&lt;p&gt;本文将点过程的条件强度看作是模型的输入信息到事件发生强度之间的非线性映射。输入i西南西包含事件的类型、事件信息、系统历史。这样的非线性映射目标是足够复杂且足够灵活，可以对事件数据的特性建模。&lt;/p&gt;
&lt;p&gt;本文用RNN编码这种非线性关系，在不用先验知识的情况下以端到端的形式建模非线性强度映射。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/modeling-the-intensity-function-of-point-process-via-recurrent-neural-networks/Fig1.jpg"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;h1 id="3-network-structure-and-end-to-end-learning"&gt;3 Network Structure and End-to-End Learning
&lt;/h1&gt;&lt;p&gt;RNN的输入序列${\mathbf{x}}^T_{t=1}$，隐藏状态${\mathbf{h}}^T_{t=1}$。本文用LSTM。&lt;/p&gt;
$$
\begin{align} \mathbf{i}\_t &amp;= \sigma(\mathbf{W}\_i \mathbf{x}\_t + \mathbf{U}\_i \mathbf{h}\_{t-1} + \mathbf{V}\_i \mathbf{c}\_{t-1} + \mathbf{b}\_i),\\ \mathbf{f}\_t &amp;= \sigma(\mathbf{W}\_f \mathbf{x}\_t + \mathbf{U}\_f \mathbf{h}\_{t-1} + \mathbf{V}\_f \mathbf{c}\_{t-1} + \mathbf{b}\_f),\\ \mathbf{c}\_t &amp;= \mathbf{f}\_t \mathbf{c}\_{t-1} + \mathbf{i}\_t \odot \text{tanh}(\mathbf{W}\_c \mathbf{x}\_t + \mathbf{U}\_c \mathbf{h}\_{t-1} + \mathbf{b}\_c),\\ \mathbf{o}\_t &amp;= \sigma(\mathbf{W}\_o \mathbf{x}\_t + \mathbf{U}\_o \mathbf{h}\_{t-1} + \mathbf{V}\_o \mathbf{c}\_t + \mathbf{b}\_o),\\ \mathbf{h}\_t &amp;= \mathbf{o}\_t \odot \text{tanh}(\mathbf{c}\_t) \end{align}
$$&lt;p&gt;$\odot$表示element-wise multiplication，$\sigma$是sigmoid。&lt;/p&gt;
&lt;p&gt;上述LSTM可以写为：&lt;/p&gt;
$$
(\mathbf{h}\_t, \mathbf{c}\_t) = \text{LSTM}(\mathbf{x}\_t, \mathbf{h}\_{t-1} + \mathbf{c}\_{t-1})
$$&lt;p&gt;考虑两类输入：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;连续等间隔分布的时间序列&lt;/li&gt;
&lt;li&gt;间隔随机的事件数据&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;网络由两部分组成，一个RNN建模时间序列${y_t}^T_{t=1}$，捕获事件发生的background intensity，另一个建模事件序列${z_i, t_i}^N_{i=1}$，捕获long-range事件的依赖关系。因此：&lt;/p&gt;
$$
\begin{align} (\mathbf{h}^y\_t, \mathbf{c}^y\_t) &amp;= \text{LSTM}\_y(\mathbf{y}\_t, \mathbf{h}^y\_{t-1} + \mathbf{c}^y\_{t-1}),\\ (\mathbf{h}^z\_t, \mathbf{c}^z\_t) &amp;= \text{LSTM}\_z(\mathbf{z}\_t, \mathbf{h}^z\_{t-1} + \mathbf{c}^z\_{t-1}),\\ \mathbf{e}\_t &amp;= \text{tanh}(\mathbf{W}\_f [\mathbf{h}^y\_t, \mathbf{h}^z\_t] + \mathbf{b}\_f),\\ \mathbf{U}\_t &amp;= \text{softMax}(\mathbf{W}\_U \mathbf{e}\_t + \mathbf{b}\_U),\\ \mathbf{u}\_t &amp;= \text{softMax}(\mathbf{W}\_u[\mathbf{e}\_t, \mathbf{U}\_t] + \mathbf{b}\_u),\\ s\_t &amp;= \mathbf{W}\_s \mathbf{e}\_t + b\_s,\\ \end{align}
$$&lt;p&gt;$U$和$u$分别表示事件的主要类型和子类，$s$表示事件的时间戳，损失定义为：&lt;/p&gt;
$$
\sum^N\_{j=1}(- W^j\_U \log(U^j\_t) - w^j\_u \log(u^j\_t) - \log(f(s^j\_t \mid h^j\_{t-1})))
$$&lt;p&gt;$N$是样本数，$j$是样本的index，$s^j_t$是下一个事件的时间戳，$h^j_{t-1}$是历史信息。第三项的含义是，我们不仅要预测对下一个事件的类型，也要让下一个事件的预测发生时间尽可能准确。这里用一个高斯惩罚函数，$\sigma^2 = 10$：&lt;/p&gt;
$$
f(s^j\_t \mid h^j\_{t-1}) = \frac{1}{\sqrt{2 \pi \sigma}} \exp(\frac{-(s^j\_t - \tilde{s}^j\_t)^2}{2\sigma^2})
$$&lt;p&gt;时间戳预测层的输出$\tilde{s}^j_t$计算损失的时候，要计算上面的惩罚。&lt;/p&gt;
&lt;p&gt;$W, w$分别是主类和子类的权重，用来平衡样本。这里说，如果主类和子类的预测是相互独立的，那么就把这两个数设为0。这里明显不对吧，设为0了损失不久只剩下事件发生时间了吗。。。。。。&lt;/p&gt;
&lt;p&gt;用RMSprop优化。&lt;/p&gt;</description></item><item><title>MULTIBENCH: Multiscale Benchmarks for Multimodal Representation Learning</title><link>https://davidham3.github.io/blog/p/multibench-multiscale-benchmarks-for-multimodal-representation-learning/</link><pubDate>Tue, 24 May 2022 15:10:01 +0000</pubDate><guid>https://davidham3.github.io/blog/p/multibench-multiscale-benchmarks-for-multimodal-representation-learning/</guid><description>&lt;p&gt;NIPS 2021, datasets and benchmarks track, &lt;a class="link" href="https://openreview.net/pdf?id=izzQAL8BciY" target="_blank" rel="noopener"
&gt;MULTIBENCH: Multiscale Benchmarks for Multimodal Representation Learning&lt;/a&gt;。代码：&lt;a class="link" href="https://github.com/pliang279/MultiBench" target="_blank" rel="noopener"
&gt;MultiBench&lt;/a&gt;。这是个benchmark，涵盖15个数据集，10个模态，20个预测任务，6个研究领域。&lt;/p&gt;
&lt;p&gt;这是个benchmark，涵盖15个数据集，10个模态，20个预测任务，6个研究领域。&lt;/p&gt;
&lt;p&gt;评估三项内容：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;generalization&lt;/li&gt;
&lt;li&gt;time and space complexity&lt;/li&gt;
&lt;li&gt;modality robustness&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;提供了20个关于融合、优化目标、训练方法的核心方法的标准实现。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 Introduction
&lt;/h1&gt;&lt;p&gt;A modality refers to a way in which a signal exists or is experienced.模态是指数据的存在或表现形式。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/multibench-multiscale-benchmarks-for-multimodal-representation-learning/Fig1.jpg"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Limitations of current multimodal datasets&lt;/strong&gt;：现在的多模态主要研究图像和文本，其他模态太少了。此外，当前的benchmark过分关注效果，忽略了时间和空间复杂度，同时也忽略了一些不完美的模态带来的鲁棒性的下降。在实际应用中，以上三点都应该被考虑。&lt;/p&gt;</description></item><item><title>Semi-supervised Learning for Marked Temporal Point Processes</title><link>https://davidham3.github.io/blog/p/semi-supervised-learning-for-marked-temporal-point-processes/</link><pubDate>Mon, 23 May 2022 15:27:10 +0000</pubDate><guid>https://davidham3.github.io/blog/p/semi-supervised-learning-for-marked-temporal-point-processes/</guid><description>&lt;p&gt;&lt;a class="link" href="https://arxiv.org/pdf/2107.07729.pdf" target="_blank" rel="noopener"
&gt;Semi-supervised Learning for Marked Temporal Point Processes&lt;/a&gt;。MTPP的半监督学习，模型称为SSL-MTPP。有标签的地方就用RMTPP，没有标签的地方用RMTPP的编码器和解码器来重构。两边的损失加在一起优化网络。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/semi-supervised-learning-for-marked-temporal-point-processes/Fig1.jpg"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;h1 id="3-proposed-algorithm"&gt;3 Proposed Algorithm
&lt;/h1&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/semi-supervised-learning-for-marked-temporal-point-processes/Fig2.jpg"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;p&gt;架构如图2所示，损失函数：&lt;/p&gt;
$$
\tag{1} \mathcal{L}\_{SSL-MTPP} = \mathcal{L}\_{Time} + \mathcal{L}\_{Marker} + \mathcal{L}\_{Recon}
$$&lt;h2 id="31-ssl-mtpp-algorithm"&gt;3.1 SSL-MTPP Algorithm
&lt;/h2&gt;&lt;p&gt;有标签的数据，一组序列$(S)$，包含$n$个序列pair，$(x_i, y_i)$，$(x_i)$是事件的时间信息，$(y_i)$是marker信息。用RNN捕获marker和序列的时间信息。嵌入表示用于预测marker和时间。&lt;/p&gt;
&lt;p&gt;没有标签的数据，用RNN编解码器模型，只学习时间信息。学习到的时间表示用来增强marker-time embedding。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unsupervised Reconstruction Loss Component&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;重构损失，只重构时间，不考虑marker，因此有没有标签都可以用。给定$n$个序列的训练集$S = {x_1, x_2, \dots, x_n }$，每个序列$x_i$包含$k$个事件，重构损失定义为：&lt;/p&gt;
$$
\tag{2} \mathcal{L}\_{Recon} = \sum^n\_{i=1} \Vert x\_i - \mathcal{D}(\mathcal{E}(x\_i)) \Vert^2\_2
$$&lt;p&gt;$\mathcal{E}$和$\mathcal{D}$分别表示RNN编码器和RNN解码器。重构损失专注于在给定的时间序列上学习有意义的表示，用于后续marker的预测。重构损失在训练过程完全是无监督的。$(\mathcal{E}(x_i))$是时间序列的编码。如何用这个嵌入表示预测后续的marker后面会讲。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/semi-supervised-learning-for-marked-temporal-point-processes/Fig3.jpg"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Supervised Marker and Time Prediction Loss Components&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$(x_i, y_i)$包含事件的时间信息和marker信息，输入到RNN模块后可以获得marker和时间相互依赖的表示：&lt;/p&gt;
$$
\tag{3} f\_i = RNN(x\_i, y\_i)
$$&lt;p&gt;提取出的特征表示与无监督的时间表示$(\mathcal{E}(x_i))$一起生成融合嵌入表示：&lt;/p&gt;
$$
\tag{4} f^{fused}\_i = f\_i + \lambda \ast \mathcal{E}(x\_i)
$$&lt;p&gt;$\lambda$是权重。这个融合表示放入一个2层感知机预测下一个事件的时间和marker。预测模型通过下面的损失来训练：&lt;/p&gt;
$$
\tag{5} \begin{align} \mathcal{L}\_{Marker} &amp;= - \sum^M\_{c=1} y^i\_{i,c} \log(p^j\_{i,c})\\ \mathcal{L}\_{Time} &amp;= \Vert x^j\_i - {x^j}'\_i \Vert \end{align}
$$&lt;p&gt;$\mathcal{L}_{Marker}$用交叉熵，$\mathcal{L}_{Time}$用MAE损失。事件$j$是序列$i$的一个事件，时间是$x^j_i$，marker是$y^j_i$，预测的类别有$M$个。$y^j_{i,c}$是一个binary变量，表示样本$y^j_i$是否是类别$c$，$p^j_{i,c}$是样本属于类别$c$的概率，${x^j}’_i$是给定事件的预测时间。&lt;/p&gt;
&lt;h2 id="32-implementation-details"&gt;3.2 Implementation Details
&lt;/h2&gt;&lt;p&gt;SSL-MTPP利用了RMTPP的架构。监督部分的RNN是一个5层LSTM模型，无监督部分是2层的RNN编码器和解码器。marker和event prediction模块分别用了2个dense层。RNN后面用了Dropout。$\lambda$设为0.1。Adam，学习率0.01，训练100轮，batch size是1024个sequence。&lt;/p&gt;</description></item><item><title>Individual Mobility Prediction via Attentive Marked Temporal Point Processes</title><link>https://davidham3.github.io/blog/p/individual-mobility-prediction-via-attentive-marked-temporal-point-processes/</link><pubDate>Fri, 20 May 2022 15:35:04 +0000</pubDate><guid>https://davidham3.github.io/blog/p/individual-mobility-prediction-via-attentive-marked-temporal-point-processes/</guid><description>&lt;p&gt;&lt;a class="link" href="https://arxiv.org/pdf/2109.02715.pdf" target="_blank" rel="noopener"
&gt;Individual Mobility Prediction via Attentive Marked Temporal Point Processes&lt;/a&gt;。代码：&lt;a class="link" href="https://github.com/Kaimaoge/AMTPP_for_Mobility" target="_blank" rel="noopener"
&gt;https://github.com/Kaimaoge/AMTPP_for_Mobility&lt;/a&gt;。结合深度学习的TPP，用注意力机制增强对事件的表示，使用混合ALL分布对事件间的时间间隔建模，通过学习OD转移概率矩阵给定O预测D。&lt;/p&gt;
&lt;p&gt;预测用户下一个trip的开始时间$t$，起点$o$，目的地$d$。AMTPP模型用自注意力机制捕获用户travel behavior内的周期性和regularity。使用非对称的log-Laplace mixture distribution建模起始时间$t$的分布。此外，还开发了一个OD矩阵学习模块。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 Introduction
&lt;/h1&gt;&lt;p&gt;本质上，用户的移动数据分为两类：带时间戳的位置序列${ (t_i, l_i) }^n_{i=1}$，表示时间$t_i$的位置$l_i$；trip/activity sequence ${(t_i, o_i, d_i)}^n_{i=1}$，时间$t_i$的从$o_i$出发，目的地是$d_i$。预测下一位置的研究比较多，但是预测下一个trip的工作比较少。相比前者，后者的信息量更大，时空关联更复杂。而且trip记录比轨迹应用的场景更多。但是对OD建模，假设有$S$个位置，那就要$S \times S$这个数量级，考虑到时间和travel的方式，比如car, bike, bus、旅行的目的，work, school, leisure，这个数量级就更大了。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/individual-mobility-prediction-via-attentive-marked-temporal-point-processes/Fig1.jpg"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;本文的目的是给定历史的轨迹${t_i, o_i, d_i}^n_{i=1}$，预测$t_{n+1}, o_{n+1}, d_{n+1}$。如果把时间$t$看作是连续变量，$o, d$看作是离散变量，那么下一个trip的搜索空间是$[t_n, +\infty) \times {1, \dots, S} \times {1, \dots, S}$。现在处理事件序列的方法有两类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HMM，隐马尔可夫模型&lt;/li&gt;
&lt;li&gt;marked temporal point processes。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;HMM的缺点是时间是离散的，不是连续的。TPP相比HMM，更通用。&lt;/p&gt;
&lt;p&gt;最近的工作把深度学习和TPP结合在一起。但是这些方法在建模用户移动或旅行数据上有一些挑战。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;OD数据之间的时空关系太复杂了。而且$S \times S$这个数量级太大。有些工作假设time和marker之间的关系是静态的。当marker的维数比较大的时候，参数会变得很多。&lt;/li&gt;
&lt;li&gt;很多TPP方法是用Hawkes过程来建模的，这种过程没法提现travel中的周期性。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本文提出的AMTPP解决了上述的挑战。用自注意力计算过去的trip对未来trip的影响。并且设计了一个新的position embedding来捕获时间信息。使用asymmetric Log-Laplace mixture distribution建模事件间的时间。ALL分布可以刻画travel behavior的rhythms和regularity。OD矩阵学习模块，可以从数据中学习到一个动态的OD关系矩阵。&lt;/p&gt;
&lt;h1 id="problem-description"&gt;Problem Description
&lt;/h1&gt;&lt;p&gt;一个用户$u$直到时间$t$的trip sequence：&lt;/p&gt;
$$
\tag{1} \mathcal{H}^u\_t = \{(t^u\_1, o^u\_1, d^u\_1), \dots, (t^u\_{n\_u}, o^u\_{n\_u}, d^u\_{n\_u}): t^u\_{n\_u} \leq t \},
$$&lt;p&gt;$n_u$表示序列中的trip个数。$t^u_i, o^u_i, d^u_i$表示第$i$个trip的出发时间、起点和目的地。时间是连续变量，OD是离散变量。后面忽略$u$。$t_i$可以表示为事件间的时间间隔$\tau_i = t_i - t_{i - 1} \in \mathbb{R}^+$。$\tau$可以看作是事件的活动时间。这俩表示是一样的。&lt;/p&gt;
&lt;p&gt;目标：&lt;/p&gt;
$$
\tag{2} p^\ast(\tau\_{n+1}, o\_{n+1}, d\_{n+1}) = p(\tau\_{n+1}, o\_{n+1}, d\_{n+1} \mid \mathcal{H}\_{t\_n}),
$$&lt;p&gt;$\ast$表示条件概率。本文认为$o_{n+1} = d_n$，也就是上一个trip的目的地等于下一个trip的起点，这样的话，trip的TPP就变成普通的TPP了。&lt;/p&gt;
&lt;h1 id="4-methodology"&gt;4 Methodology
&lt;/h1&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/individual-mobility-prediction-via-attentive-marked-temporal-point-processes/Fig2.jpg"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;p&gt;图2是AMTPP的架构。&lt;/p&gt;
&lt;h2 id="41-self-attention-encoder"&gt;4.1 Self-attention Encoder
&lt;/h2&gt;&lt;p&gt;第一步是获得$t_n, o_n, d_n$的嵌入表示：&lt;/p&gt;
$$
\tag{3} e\_n = \text{concat}(emb^t\_n, emb^o\_n, emb^d\_n),
$$&lt;p&gt;为了考虑周期和韵律，引入位置编码：&lt;/p&gt;
$$
\tag{4} \begin{align} pe^n\_n(pos\_n, 2i) &amp;= \text{sin}(pos\_n / L^{2i/J}\_{pos}),\\ pe^h\_n(pos\_n, 2i+1) &amp;= \text{cos}(pos\_n / L^{2i/J}\_{pos}), \end{align},
$$&lt;p&gt;$pos_n \in {0, 1, \dots, 23 }$是第$n$个trip的小时，$J$是嵌入的维数，$i \in {1, \dots, \lfloor J/2 \rfloor }$, $L_{pos}$是缩放因子。很多方法把$L_{pos}$设置成一个定值，比如10000，但是本文把它设定为一个参数。此外，还引入了day of week作为位置编码$pe^w_n$。加上$\tau_n$，最后的事件嵌入表示：&lt;/p&gt;
$$
\tag{5} emb^t\_n = \text{concat}(pe^w\_n, pe^h\_n, \tau\_n).
$$&lt;p&gt;OD嵌入：&lt;/p&gt;
$$
\tag{6} \begin{align} emb^o\_n &amp;= \text{concat}(W^o\_{em} \hat{o}\_n + b^o\_{em}, p^o\_n),\\ emb^d\_n &amp;= \text{concat}(W^d\_{em} \hat{d}\_n + b^d\_{em}, p^d\_n), \end{align}
$$&lt;p&gt;$W^o_{em} \in \mathbb{R}^{J_o \times S}, W^d_{em} \in \mathbb{R}^{J_d \times S}, b^o_{em} \in \mathbb{R}^{J_o}, b^d_{em} \in \mathbb{R}^{J_d}$是参数，$J_o, J_d$是OD嵌入向量的维数，$S$是位置的数量，$p^o_n, p^d_n$是OD的其他信息，比如POI什么的。&lt;/p&gt;
&lt;p&gt;给定历史事件序列的嵌入$E_n = [e_1, e_2, \dots, e_n]^\top \in \mathbb{R}^{n \times J}$，用多头自注意力计算第$n$个trip的隐藏状态。注意力的参数矩阵$Q_l = EW^Q_l, K_l = E W^K_l, V_l = E W^V_l, l = 1, 2, \dots, L$。$W^Q_l, W^K_l \in \mathbb{R}^{J \times c_k}, W^V_l \in \mathbb{R}^{J \times C_v}$。&lt;/p&gt;
&lt;p&gt;注意力：&lt;/p&gt;
$$
\tag{7} \text{Att}(Q\_l, K\_l, V\_l) = \text{softmax}(\frac{Q\_l K^\top\_l}{\sqrt{d\_k}} \cdot M) V\_l,
$$&lt;p&gt;$M \in \mathbb{R}^{n \times n}$是mask矩阵，上三角部分设为$-\infty$，防止信息泄露。&lt;/p&gt;
&lt;p&gt;多头注意力：&lt;/p&gt;
$$
\tag{8} \begin{align} H &amp;= \text{gelu}(\text{concat}(\text{head}\_1, \dots, \text{head}\_L) W^O),\\ \text{head}\_l &amp;= \text{Att}(EW^Q\_l, EW^K\_l, EW^V\_l), \end{align}
$$&lt;p&gt;$W_O \in \mathbb{R}^{L \cdot c_v \times C_{\text{model}}}$, $c_{\text{model}}$是输出的特征数。$\text{gelu}$表示Gaussian Error Linear Unit，非线性激活函数，$H_n = [h_1, h_2, \dots, h_n]^\top \in \mathbb{R}^{n \times c_{\text{model}}}$是输出。因为用了mask，所以$h_i$只是基于历史生成的。&lt;/p&gt;
&lt;h2 id="42-asymmetrical-log-laplace-mixture-for-inter-trip-time"&gt;4.2 Asymmetrical Log-Laplace Mixture for Inter-trip Time
&lt;/h2&gt;&lt;p&gt;这个部分是对事件间的时间间隔的条件概率分布 $p_\theta(\tau_{n+1} \mid h_n)$ 建模，使用参数为$\theta$的深度神经网络建模。《Intensity-Free Learning of Temporal Point Processes》这篇论文认为相比对强度函数建模，直接对时间间隔建模更方便。这篇论文用log-normal mixture model对TPP的条件概率密度$p_\theta(\tau_{n+1} \mid h_n)$建模。但是这个分布对trip之间的interval不适用，因为trip之间的interval表示了事件的持续时间，而且条件分布通常有一些明显的峰。为了更好的刻画trip间的时间间隔，本文用Aysmmetric Log-Laplace分布。这个分布经常用于对非常偏、有峰和长尾的数据建模。ALL分布有三个参数：&lt;/p&gt;
$$
ALL(\tau; \beta, \lambda, \gamma) = \frac{\lambda \gamma}{\tau(\lambda + \gamma)} \begin{cases} (\frac{\tau}{\beta})^\lambda &amp; \text{if} \ 0 &lt; \tau &lt; \beta,\\ (\frac{\beta}{\tau})^\gamma &amp; \text{if} \ \tau \geq \beta, \end{cases}
$$&lt;p&gt;$\beta$控制模式，$\lambda$和$\gamma$分别是左右长尾的正长尾参数。&lt;/p&gt;
&lt;p&gt;理想的$p_\theta(\tau \mid h)$分布能趋近任意分布。因为混合模型有趋近$\mathbb{R}$上任意概率分布的性质：universal approximation(UA)，我们使用$\mathcal{D}$，作为ALL的混合分布，来趋近$p_\theta(\tau \mid h)$：&lt;/p&gt;
$$
\tag{9} \mathcal{D}(\tau; w, \beta, \lambda, \gamma) = \sum^K\_{k=1} w\_k ALL(\tau; \beta\_k, \lambda\_k, \gamma\_k),
$$&lt;p&gt;$w$是混合权重。通过这个混合模型，我们可以近似一个用户的多模态旅行模式。举个例子，如果一个旅行者只有早上的通勤数据，我们在24小时内只能看到一个峰。但是对于来回通勤的人，我们希望看到的是早上一个峰，晚上一个峰。这种混合模型可以表示多个峰。&lt;/p&gt;
&lt;p&gt;ALL混合分布下的变量的对数服从$\text{ALMixture}(w,\hat{\beta}, \hat{\lambda}, \hat{\gamma})$，每个部分是：&lt;/p&gt;
$$
\tag{10} AL(y) = \frac{\hat{\lambda}\_k}{\hat{\gamma}\_k + \frac{1}{\hat{\gamma}\_k}} \begin{cases} \exp (\frac{\hat{\lambda}\_k}{\hat{\gamma}\_k} (y - \hat{\beta}\_k)) &amp; \ \text{if} \ 0 &lt; y &lt; \hat{\beta}\_k,\\ \exp(- \hat{\lambda}\_k \hat{\gamma}\_k (y - \hat{\beta}\_k)) &amp; \ \text{if} \ y \geq \hat{\beta}\_k, \end{cases}
$$&lt;p&gt;$\hat{\beta}_k = \log(\beta_k), \hat{\gamma}_k = \sqrt{\frac{\lambda_k}{\gamma_k}}, \hat{\lambda}_k = \sqrt{\lambda_k \gamma_k}$。&lt;/p&gt;
&lt;p&gt;公式10里面的对数似然比公式9中的原始ALL更容易学习。我们用MDN网络学习ALMixture里面的参数：&lt;/p&gt;
$$
\tag{11} \begin{align} w\_n &amp;= \text{softmax}(\Phi\_w h\_n + b\_w),\\ \hat{\beta}\_n &amp;= \exp(\Phi\_\beta h\_n + b\_\beta),\\ \hat{\lambda}\_n &amp;= \exp(\Phi\_\lambda h\_n + b\_\lambda),\\ \hat{\gamma}\_n &amp;= \exp(\Phi\_\gamma h\_n + b\_\gamma), \end{align}
$$&lt;p&gt;softmax和exp用来约束分布的参数，$\Phi, b$都是learnable parameters。&lt;/p&gt;
&lt;h2 id="43-od-matrix-learning"&gt;4.3 OD Matrix Learning
&lt;/h2&gt;&lt;p&gt;显然OD和$t$是有关的，即$p^\ast(o_{n+1} \mid \tau_{n+1})$，这里我们没有直接对时间$\tau$建模，而是对$\tau$上面的参数${w_n, \beta_n, \lambda_n, \gamma_n }$建模，这样就不用从分布中采样了。因为ALL混合分布中的参数是有物理意义的，从学习到的模型中模拟trip也很容易。通过调整参数就可以看到OD分布的变化。举个例子，减小峰参数$\beta$来观察OD分布的变化，可以理解成一个人的出发时间提前之后他的trip会有什么变化。&lt;/p&gt;
$$
\tag{12} \begin{align} \hat{h}\_n &amp;= \text{concat}(h\_n, w\_n, \hat{\beta}\_n, \hat{\lambda}\_n, \hat{\gamma}\_n),\\ \hat{o}\_{n+1} &amp;= \text{softmax}(\Phi\_o \hat{h}\_n + b\_o), \end{align}
$$&lt;p&gt;下一个位置$\hat{o}_{n+1}$的分布依赖拼接向量$\hat{h}_n$，这个向量由历史编码$h_n$和trip的时间参数组成。&lt;/p&gt;
&lt;p&gt;$p^\ast(d_{n+1})$通过把$\hat{o}_{n+1}$乘以一个OD矩阵$OD_{n+1}$得到，这个矩阵的每一列包含了从一个O转移到所有D的转移概率。这个OD矩阵很有用，有了这个OD矩阵，我们可以知道地铁线路里面哪个结点的人比较多。但是$S \times S$太大了，学一个实时的OD矩阵太难了。我们用下面的方法学习$OD_{n+1}$里面的参数：&lt;/p&gt;
$$
\tag{13}
\begin{align}
D^1\_{n+1} &amp;= \text{reshape}(\Phi^1\_m \hat{h}\_n),\\
D^2\_{n+1} &amp;= \text{reshape}(\Phi^2\_m \hat{h}\_n),\\
OD\_{n+1} &amp;= D^1\_{n+1}{D^2\_{n+1}}^\top \cdot M\_{od},\\
OD\_{n+1} &amp;= \text{softmax}(OD\_{n+1}).\\
\hat{d}\_{n+1} &amp;= OD\_{n+1} \hat{o}\_{n+1}
\end{align}
$$&lt;p&gt;$D^1_{n+1}, D^2_{n+1} \in \mathbb{R}^{S \times r}$, $r \ll S$。$\Phi$是learnable parameters。$M_{od} \in \mathbb{R}^{S \times S}$用来过滤掉不可能的OD pair，方法就是把这些位置设置成 $- \infty$，包括矩阵的对角线。softmax用来约束矩阵的每一列的和都为1。这个矩阵是根据时间编码$\hat{h}_n$动态变化的。输出的$\hat{d}_{n+1}$和输入的$\hat{o}_{n+1}$与参数关联在一起，让网络训练起来更容易。&lt;/p&gt;
&lt;h2 id="44-model-training"&gt;4.4 Model Training
&lt;/h2&gt;&lt;p&gt;负对数似然。随机梯度下降。短的序列要加pad。pad部分会被mask掉。损失函数：&lt;/p&gt;
$$
\tag{14} \mathcal{L} = - \sum^U\_{u=1} \sum^{n\_u}\_{n=1} \log p^\ast\_\Theta(\tau^u\_n) + \log p^\ast\_\Theta(o^u\_n) + \log p^\ast\_\Theta(d^u\_n),
$$&lt;p&gt;$U$是用户数，$n_u$是用户$u$的trip的个数。对于$\tau$的对数似然，在$\tau$上面加一个对数变换，得到非对称Laplace分布：$y = \log(\tau)$，最终得到：&lt;/p&gt;
$$
\tag{15} \begin{align} \log p^\ast\_\Theta(\tau) &amp;= \log p^\ast\_\Theta(y) - \log(\tau), \ \ \ \ y = \log(\tau),\\ p^\ast\_\Theta(y) &amp;= \text{ALMixture}(w, \hat{\beta}, \hat{\lambda}, \hat{\gamma}). \end{align}
$$</description></item><item><title>Unsupervised Scalable Representation Learning for Multivariate Time Series</title><link>https://davidham3.github.io/blog/p/unsupervised-scalable-representation-learning-for-multivariate-time-series/</link><pubDate>Thu, 19 May 2022 14:54:43 +0000</pubDate><guid>https://davidham3.github.io/blog/p/unsupervised-scalable-representation-learning-for-multivariate-time-series/</guid><description>&lt;p&gt;NIPS 2019, &lt;a class="link" href="https://arxiv.org/abs/1901.10738" target="_blank" rel="noopener"
&gt;Unsupervised Scalable Representation Learning for Multivariate Time Series&lt;/a&gt;。T-loss。无监督多元时间序列表示模型。利用word2vec的负样本采样的思想学习时间序列的嵌入表示。代码：&lt;a class="link" href="https://github.com/White-Link/UnsupervisedScalableRepresentationLearningTimeSeries" target="_blank" rel="noopener"
&gt;UnsupervisedScalableRepresentationLearningTimeSeries&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="3-unsupervised-training"&gt;3 Unsupervised Training
&lt;/h1&gt;&lt;p&gt;只想训练编码器，不想搞编码解码结构，因为计算量太大了。因此引入了一个针对时间序列的triplet loss。&lt;/p&gt;
&lt;p&gt;目标是无监督的情况下相似的时间序列具有相似的表示。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/unsupervised-scalable-representation-learning-for-multivariate-time-series/Fig1.jpg"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;图1，给定时间序列$y_i$，一个随机子序列$x^{\text{ref}}$。$x^{\text{ref}}$的表示要与$x^{\text{pos}}$相似，与另一个随机采样的子序列$x^{\text{neg}}$不相似。类比word2vec，$x^{\text{pos}}$是word，$x^{\text{ref}}$是context，$x^{\text{neg}}$是一个随机的词。&lt;/p&gt;
&lt;p&gt;损失函数：&lt;/p&gt;
$$
\tag{1} -\log{( \sigma(f(x^{\text{ref}}, \theta)^\top f(x^{\text{pos}}, \theta)) )} - \sum^K\_{k=1} \log{( \sigma( -f(x^{\text{ref}}, \theta)^\top f(x^{\text{neg}}, \theta) ) )},
$$&lt;p&gt;$f(\cdot, \theta)$是一个神经网络，参数是$\theta$，$\sigma$是sigmoid激活函数。$K$是负样本的个数&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/unsupervised-scalable-representation-learning-for-multivariate-time-series/Alg1.jpg"
loading="lazy"
alt="Algorithm_1"
&gt;&lt;/p&gt;
&lt;h1 id="4-encoder-architecture"&gt;4 Encoder Architecture
&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;必须从时间序列中提取相关信息&lt;/li&gt;
&lt;li&gt;训练和推断的时候时间和空间都要高效&lt;/li&gt;
&lt;li&gt;必须能接受变长的输入&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;使用exponentially dilated causal convolutions处理。相比RNN，这类网络可以在GPU上高效地并行计算。对比fully convolutions，可以捕获更长范围的信息。&lt;/p&gt;
&lt;p&gt;即便LSTM解决了梯度消失和爆炸的问题，在这个梯度问题上仍然比不过卷积网络。&lt;/p&gt;
&lt;p&gt;我们的模型堆叠了多个dilated causal convolutions，如图2a。每个数都是通过前面的数计算出来的，因此称为因果。图2a红色部分展示了输出值的计算路径。&lt;/p&gt;
&lt;p&gt;卷积网络的输出会放入一个max pooling层，把所有时间信息聚合到一个固定大小的向量中。&lt;/p&gt;</description></item><item><title>Fully Neural Network based Model for General Temporal Point Processes</title><link>https://davidham3.github.io/blog/p/fully-neural-network-based-model-for-general-temporal-point-processes/</link><pubDate>Tue, 17 May 2022 13:11:14 +0000</pubDate><guid>https://davidham3.github.io/blog/p/fully-neural-network-based-model-for-general-temporal-point-processes/</guid><description>&lt;p&gt;NIPS 2019: &lt;a class="link" href="https://arxiv.org/abs/1905.09690v3" target="_blank" rel="noopener"
&gt;Fully Neural Network based Model for General Temporal Point Processes&lt;/a&gt;。创新点是之前的条件强度函数有一个积分项，这个积分项不是很好求，本文提出用一个FNN计算累积强度函数，这样条件强度函数的计算只需要计算累积强度函数对事件时间间隔的偏导数就可以得到了。代码：&lt;a class="link" href="https://github.com/omitakahiro/NeuralNetworkPointProcess" target="_blank" rel="noopener"
&gt;https://github.com/omitakahiro/NeuralNetworkPointProcess&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="2-method"&gt;2 Method
&lt;/h1&gt;&lt;p&gt;条件强度函数：&lt;/p&gt;
$$
\tag{5} \lambda(t \mid H\_t) = \phi(t - t\_i \mid h\_i)
$$&lt;p&gt;这里$\phi$是非负函数，表示hazard函数。&lt;/p&gt;
&lt;p&gt;Du et al., 2016提出的近似形式：&lt;/p&gt;
$$
\tag{6} \phi(\tau \mid h\_i) = \exp{(w^t \tau + v^\phi \cdot h\_i + b^\phi)}
$$&lt;p&gt;对数似然函数：&lt;/p&gt;
$$
\tag{8} \log{L(\{ t\_i \})} = \sum\_i[ \log{\phi(t\_{i+1} - t\_i \mid h\_i)} - \int^{t\_{i+1} - t\_i}\_0 \phi(\tau \mid h\_i)d\tau]
$$&lt;p&gt;使用BPTT优化，$h_i$是RNN的隐藏状态。&lt;/p&gt;
&lt;h2 id="24-proposed-model"&gt;2.4 Proposed Model
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/fully-neural-network-based-model-for-general-temporal-point-processes/Fig1.jpg"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;这里建模累积hazard函数：&lt;/p&gt;
$$
\tag{9} \Phi(\tau \mid h\_i) = \int^\tau\_0 \phi(s \mid h\_i)ds
$$&lt;p&gt;hazard函数通过对上式求偏导数得到：&lt;/p&gt;
$$
\tag{10} \phi(\tau \mid h\_i) = \frac{\partial}{\partial \tau} \Phi(\tau \mid h\_i)
$$&lt;p&gt;对数似然函数可以改写为：&lt;/p&gt;
$$
\tag{11} \log{L(\{t\_i\})} = \sum\_i[\log{\{\frac{\partial}{\partial \tau} \Phi(\tau = t\_{i+1} - t\_i \mid h\_i)\}} - \Phi(t\_{i+1} - t\_i \mid h\_i)]
$$&lt;p&gt;累积Hazard函数是一个关于$\tau$的正的单调递增函数。本文用一个FNN网络近似这个函数。为了保证这个性质，只要在$\tau$到最后的损失的路径上权重都是正的就好了。网络架构如图1所示。在模型参数更新的时候，如果权重变成负的，就用绝对值替换它。&lt;/p&gt;
&lt;p&gt;这里看了代码后发现是把RNN的最后一个预测值与$\tau$一起输入到FNN中了。中间这些层的权重都是正的，而且激活函数是$tanh$，最后一层的激活函数是$softplus$，即$\log{(1 + \exp(\cdot))}$。&lt;/p&gt;
&lt;p&gt;预测下一个事件的发生时间的方式是：给定过去事件${t_1, t_2, \dots t_i}$，下一个事件的发生时间$t_{i+1}$的条件密度函数$p^\ast(t \mid t_1, t_2, \dots, t_i)$通过公式2计算。&lt;/p&gt;
&lt;p&gt;\tag{2} p(t_{i+1} \mid t_1, t_2, \dots, t_i) = \lambda(t_{i+1} \mid H_{t_{i+1}}) \exp{{- \int^{t_{i+1}}_{t_i} \lambda(t \mid H_t) dt }}&lt;/p&gt;
&lt;p&gt;这里使用预测分布$p^\ast$的中位数$t^\ast_{i+1}$来预测$t_{i+1}$。这里利用$\Phi(t^\ast_{i+1} - t_i \mid h_i) = \log{(2)}$这个关系来计算中位数$t^\ast_{i+1}$。这个关系通过在$[t_i, t_{i+1})$上面对强度函数积分得到，此时指数分布的均值为1，还可以对公式2直接积分得到。中位数$t^\ast_{i+1}$可以使用root finding方法，比如二分方法直接获得。本文的模型只需要1s就可以给20000个事件生成预测结果。因此累积hazard函数在生成中位数的预测中也很重要。&lt;/p&gt;</description></item><item><title>Recurrent Marked Temporal Point Processes: Embedding Event History to Vector</title><link>https://davidham3.github.io/blog/p/recurrent-marked-temporal-point-processes-embedding-event-history-to-vector/</link><pubDate>Sun, 01 May 2022 09:18:43 +0000</pubDate><guid>https://davidham3.github.io/blog/p/recurrent-marked-temporal-point-processes-embedding-event-history-to-vector/</guid><description>&lt;p&gt;KDD 2016: RMTPP &lt;a class="link" href="https://www.kdd.org/kdd2016/papers/files/rpp1081-duA.pdf" target="_blank" rel="noopener"
&gt;Recurrent Marked Temporal Point Processes: Embedding Event History to Vector&lt;/a&gt;。经典论文，利用RNN近似条件强度函数，将传统点过程带入到神经点过程。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1. Introduction
&lt;/h1&gt;&lt;p&gt;当前方法两类变量马尔可夫模型，把问题看作是discrete-time sequence prediction task。基于观察到的状态序列，预测下一步最可能的状态。缺点是这类模型的时间是单位变化的，在预测下一个事件的时候不能捕获时间的heterogeneity。此外，状态不能太多，要不然算不过来，就没法捕获长距离的依赖关系。半马尔可夫模型&lt;/p&gt;
\[26\]&lt;p&gt;可以用来建模两个连续事件之间的continuous time-interval，通过假设interval之间是一个简单的分布，但是随着阶数的增加，计算仍然会爆炸。&lt;/p&gt;
&lt;p&gt;第二类是marked temporal point processes和intensity function，这两类是建模这种事件数据更通用的数学框架。地震学中，时间点过程广泛用用在建模地震和余震上面。每个地震表示时空空间中的一个点，地震学已经提出不同的公式来捕获这些事件的随机性。金融领域，时间点过程是经济学的活跃方向，可以给出现代金融市场复杂动态性的一些简答解释。&lt;/p&gt;
&lt;p&gt;但是实际中典型的这些点过程模型的假设一般做不到，而且这些模型的表达能力有限。我们提出了新的marked时间点过程模型，RMTPP，同步建模事件的事件和标记。我们方法的核心是将一个时间点过程的intensity function看作是一个历史过程的非线性函数，这个函数用RNN表示。&lt;/p&gt;
&lt;h1 id="2-问题定义"&gt;2. 问题定义
&lt;/h1&gt;&lt;p&gt;一组序列 $\mathcal{C} = { S^1, S^2, \dots, }$&lt;/p&gt;
&lt;p&gt;序列 $\mathcal{S}^i = ((t^i_1, y^i_1), (t^i_2, y^i_2), \dots)$ 是$(t^i_j, y^i_j)$组成的序列，$t^i_j$是事件发生的时间，$y^i_j$是事件的类别。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;给定实体$i$，预测下一个事件pair $(t^i_{n+1}, y^i_{n+1})$&lt;/li&gt;
&lt;li&gt;计算一个给定序列的likelihood&lt;/li&gt;
&lt;li&gt;通过学习得到的模型模拟一个新的序列&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="3-related-work"&gt;3. Related Work
&lt;/h1&gt;&lt;p&gt;现存工作的一个主要的限制就是对latent dynamics的各种参数假设，latent dynamics控制了观测到的点过程的生成。&lt;/p&gt;
&lt;h1 id="4-marked-temporal-point-process"&gt;4. Marked Temporal Point Process
&lt;/h1&gt;&lt;p&gt;这个第四节主要介绍了之前的一些点过程使用的条件强度函数。&lt;/p&gt;
&lt;p&gt;标记时间点过程是一个对观测到的随时间发生的随机事件模式进行建模的强有力工具。因为一个事件的出现与历史发生了什么有关，我们可以指定一些模型，给定我们已知的过去，对下一个事件建模。严格来讲，一个标记时间点过程是一个随机过程，这个随机过程包含了一个带有时间信息的离散事件的列表，${ t_j, y_j }$，$t_j \in \mathbb{R}^+, y_j \in \mathcal{Y}, j \in \mathbb{Z}^+$，$t$是时间，$y$是标记。历史 $\mathcal{H}_t$ 是直到时间$t$的事件时间和标记组成的list。连续事件时间之间的时间差 $d_{j+1} = t_{j+1} - t_j$ 称为事件间的duration。&lt;/p&gt;
&lt;p&gt;给定过去事件的历史，我们可以指定下一个事件类型$y$在时间$t$发生的条件密度函数为 $f^{\ast}(t, y) = f(t, y \mid \mathcal{H}_t)$，$f^{\ast}(t, y)$强调了这个密度是基于历史的这个条件。利用链式法则，可以得到一个序列的联合似然为：&lt;/p&gt;
$$
\tag{1} f(\{ (t\_j, y\_j) \}^n\_{j=1}) = \prod\_j f(t\_j, y\_j \mid \mathcal{H}\_t) = \prod\_j f^{\ast}(t\_j, y\_j)
$$&lt;p&gt;$f^{\ast}(t_j, y_j)$ 有很多种形式可以选择。但是实际中人们一般选择非常简单可分解的形式，比如$f(t_j, y_j \mid \mathcal{H}_t) = f(y_j) f(t_j \mid \dots, t_{j-2}, t_{j-1})$，要不然对事件标记和时间进行联合建模会非常复杂。我们可以认为在$y_j$只可取有限个值且与历史完全无关的时候，$f(y_j)$是一个多项式分布。$f(t_j \mid \dots, t_{j-2}, t_{j-1})$是给定过去事件的时间的序列时，事件发生在$t_j$的条件密度。而且需要注意的是，$f^{\ast}(t_j)$不能捕获过去事件标记的影响。&lt;/p&gt;
&lt;h2 id="41-parametrizations"&gt;4.1 Parametrizations
&lt;/h2&gt;&lt;p&gt;一个标记点过程的时间信息可以通过一个典型的时间点过程来捕获。一个刻画时间点过程的重要方式是通过条件强度函数——给定过去所有事件，对下一个事件建模的随机模型。在一个小的时间窗口$[t, t + dt)$里，$\lambda^{\ast}(t)dt$是一个新事件在给定历史$\mathcal{H}_t$时出现的概率：&lt;/p&gt;
$$
\tag{2} \lambda^{\ast}(t)dt = \mathbb{P}\{ \text{event in }[t, t+dt) \mid \mathcal{H}\_t \}
$$&lt;p&gt;$\ast$是用来提醒我们这个函数是依赖历史的。给定条件密度函数$f^{\ast}(t)$，条件强度函数为：&lt;/p&gt;
$$
\tag{3} \lambda^{\ast}(t)dt = \frac{f^{\ast}(t)dt}{S^{\ast}(t)} = \frac{f^{\ast}(t)dt}{1 - F^{\ast}(t)},
$$&lt;p&gt;$F^{\ast}(t)$是在最后一个事件时间$t_n$之后，一个新事件发生在$t$之前的累积概率，$S^{\ast}(t) = \text{exp}(- \int^t_{t_n} \lambda^{\ast}(\tau) d\tau)$ 是$t_n$到$t$之间没有新事件发生的概率。因此条件密度函数可以写成：&lt;/p&gt;
$$
\tag{4} f^{\ast}(t) = \lambda^{\ast}(t) \text{exp}(- \int^t\_{t\_n} \lambda^{\ast}(\tau) d\tau).
$$&lt;h1 id="5-recurrent-marked-temporal-point-process"&gt;5. Recurrent Marked Temporal Point Process
&lt;/h1&gt;&lt;p&gt;条件强度函数的形式决定了一类点过程的事件性质。但是，为了考虑marker和事件信息，在缺少前沿知识的情况下很难决定使用哪种形式的条件强度函数。为了解决这个问题，作者提出了一个统一的模型，可以在历史的事件和marker信息上对非线性依赖建模的模型。&lt;/p&gt;
&lt;h2 id="51-model-formulation"&gt;5.1 Model Formulation
&lt;/h2&gt;&lt;p&gt;公式5，6，7有不同的表达形式，而且是对过去事件不同类型的依赖结构。受到这一点的启发，我们希望能学习到一个趋近于历史未知依赖结构的通用表示。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/recurrent-marked-temporal-point-processes-embedding-event-history-to-vector/Fig2.jpg"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;p&gt;我们的想法是用RNN来实现这一步。如图2所示，RNN在时间步$t_j$的输入是$(t_j, y_j)$。$y_i$是事件的类型。$h_{j-1}$表示从过去事件和事件得到的影响的memory，在更新的时候会考虑当前的事件和时间。因为$h_j$表示过去一直到第$j$个事件的影响，那么下一个事件时间的条件强度函数就可以表示为：&lt;/p&gt;
$$
\tag{8} f^{\ast}(t\_{j + 1}) = f(t\_{j+1} \mid \mathcal{H}\_t) = f(t\_{j+1} \mid h\_j) = f(d\_{j + 1} \mid h\_j),
$$&lt;p&gt;$d_{j+1} = t_{j+1} - t_j$。因此，我们可以用$h_j$去预测时间$\hat{t}_{j + 1}$和下一事件类型$\hat{y}_{j + 1}$。&lt;/p&gt;
&lt;p&gt;这个公式的好处是我们将历史事件嵌入到了一个隐向量空间，然后通过公式4，我们不用指定一个固定的参数形式来建模历史的依赖结构，可以用一个更简单的形式得到条件强度函数$\lambda^{\ast}(t)$。图3展示了RMTPP模型的架构。给定一个事件序列$\mathcal{S} = ((t_j, y_j)^n_{j=1})$，通过迭代以下组件，得到一个隐藏单元${ h_j }$的序列。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Input Layer&lt;/strong&gt;，先用一个input layer对one-hot的事件表示进行投影。$y_j = W^T_{em} y_j + b_{em}$。然后事件步$t_j$也投影成一个向量$t_j$。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/recurrent-marked-temporal-point-processes-embedding-event-history-to-vector/Fig3.jpg"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hidden Layer&lt;/strong&gt;&lt;/p&gt;
$$
\tag{9} h\_j = \text{max} \{ W^y y\_j + W^t t\_j + W^h h\_{j-1} + b\_h, 0 \}
$$&lt;p&gt;&lt;strong&gt;Marker Generation&lt;/strong&gt;，给定表示$h_j$，我们用一个多项式分布建模marker的生成：&lt;/p&gt;
$$
\tag{10} P(y\_{j+1} = k \mid h\_j) = \frac{ \text{exp}(V^y\_{k,:} h\_j + b^y\_k) }{ \sum^K\_{k=1} \text{exp} ( V^y\_{k,:} h\_j + b^y\_k ) },
$$&lt;p&gt;$K$是marker的个数，$V^y_{k,:}$是矩阵$V^y$的第$k$行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conditional Intensity&lt;/strong&gt;，基于$h_j$，可以得到条件强度函数：&lt;/p&gt;
$$
\tag{11} \lambda^{\ast}(t) = \text{exp}( \mathcal{v}^{t^T} \cdot h\_j + w^t (t - t\_j) + b^t ),
$$&lt;p&gt;$\mathcal{v}^t$是一个列向量，$w^t, b^t$是标量。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一项 $\mathcal{v}^{t^T} \cdot h_j$ 表示过去的事件和时间信息累积的影响。对比公式5，6，7对过去影响固定的公式，现在这个是对过去影响的高度非线性通用表示。&lt;/li&gt;
&lt;li&gt;第二项强调当前事件$j$的影响。&lt;/li&gt;
&lt;li&gt;最后一项对下一个事件的出现给了一个基础的强度等级。&lt;/li&gt;
&lt;li&gt;指数函数是一个非线性函数，且保证强度永远是正的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过公式4，我们可以得到给定历史的情况下，下一个事件在时间$t$出现的likelihood：&lt;/p&gt;
$$
\tag{12} f^{\ast}(t) = \lambda^{\ast}(t) \text{exp}( - \int^t\_{t\_j} \lambda^{\ast}(\tau) d\tau) \ = \text{exp} \{ \mathcal{v}^{t^T} \cdot h\_j + w^t (t - t\_j) + b^t + \frac{1}{w} \text{exp}(\mathcal{v}^{t^T} + b^t) \ - \frac{1}{w} \text{exp}(\mathcal{v}^{t^T} \cdot h\_j + w^t(t - t\_j) + b^t) \}
$$&lt;p&gt;然后，下一个事件的时间可以用期望来计算&lt;/p&gt;
$$
\tag{13} \hat{t}\_{j+1} = \int^\infty\_{t\_j} t \cdot f^{\ast}(t) dt.
$$&lt;p&gt;一般来说，公式13的积分没有解析解，我们可以用&lt;/p&gt;
\[32\]&lt;p&gt;的用于一维函数的数值积分技术来计算公式13。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;。因为我们用RNN表示历史，所以条件强度函数$\lambda^{\ast}(t_{j+1})$的公式11，捕获了过去事件和时间两部分信息。另一方面，因为marker的预测也依赖于过去的时间信息，当时间和事件信息相互关联的时候，就可以提升模型的性能。后面的实验证明了这种互相提升的现象确实存在。&lt;/p&gt;
&lt;h2 id="52-参数学习"&gt;5.2 参数学习
&lt;/h2&gt;&lt;p&gt;最大对数似然&lt;/p&gt;
$$
\tag{14} \ell(\{\mathcal{S}^i \}) = \sum\_i \sum\_j (\text{log} P(y^i\_{j+1} \mid h\_j) + \text{log} f(d^i\_{j+1} \mid h\_j) ),
$$&lt;p&gt;$\mathcal{C} = { \mathcal{S}^i }$是一组序列, $\mathcal{S}^i = ((t^i_j, y^i_j)^{n_i}_{j=1})$。用BPTT训练。&lt;/p&gt;</description></item><item><title>Time-dependent representation for neural event sequence prediction</title><link>https://davidham3.github.io/blog/p/time-dependent-representation-for-neural-event-sequence-prediction/</link><pubDate>Wed, 27 Apr 2022 15:37:58 +0000</pubDate><guid>https://davidham3.github.io/blog/p/time-dependent-representation-for-neural-event-sequence-prediction/</guid><description>&lt;p&gt;ICLR 2018 workshop, &lt;a class="link" href="https://arxiv.org/abs/1708.00065" target="_blank" rel="noopener"
&gt;Time-dependent representation for neural event sequence prediction&lt;/a&gt;。事件序列的表示学习模型。主要是对事件的嵌入表示有了一些创新，加入了对事件duration的考虑。模型整体还是RNN架构。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/time-dependent-representation-for-neural-event-sequence-prediction/Fig1.jpg"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;在事件序列中，有两个时间段(time span)，一个是duration，事件的持续时长，另一个是interval，事件与事件之间的间隔。为了统一这两个时间段，作者将interval看作是一个空闲事件(idle event)。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/time-dependent-representation-for-neural-event-sequence-prediction/Fig2.jpg"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;p&gt;图2是模型架构，就是把事件做嵌入表示，然后把duration考虑进去，得到事件序列里面每个时间步的嵌入表示，然后丢到RNN里面，最后是要预测下一个event是什么，同时可以把下一个event的duration拿进来计算损失，起到一个正则的作用。&lt;/p&gt;
&lt;h2 id="31-contextualizing-event-embedding-with-time-mask"&gt;3.1 Contextualizing Event Embedding With Time Mask
&lt;/h2&gt;&lt;p&gt;这里有一个观点，就是在机器翻译中，RNN会花一定的capacity去区分不同context下一个词的意思。为了解决这个问题，Choi et al., 2016 提出了一个mask计算。本文借助这个思想，提出了一个时间依赖的嵌入表示。&lt;/p&gt;
$$
\tag{1} c^d = \phi (\log(d\_t);\theta)
$$&lt;p&gt;上式是一个FNN，$\phi$是非线性变换，参数是$\theta$。加对数是为了降低$d_t$的范围。然后用一个单层sigmoid的非线性变换把$c^d$映射到$m^d \in \mathbb{R}^E$上，这个就是mask。&lt;/p&gt;
$$
\tag{2} m\_d = \sigma(c^d W\_d + b\_d)
$$&lt;p&gt;然后&lt;/p&gt;
$$
\tag{3} x\_t \leftarrow x\_t \odot m\_d
$$&lt;p&gt;把mask和一个事件的embedding相乘，element-wise production，这个东西放入RNN。&lt;/p&gt;
&lt;h2 id="32-event-time-joint-embedding"&gt;3.2 Event-Time Joint Embedding
&lt;/h2&gt;&lt;p&gt;这里有个观点，我们平时只会说“和谁简单地聊了一会儿”，不会说具体聊了多少分钟。我们对于事件时长的感受依赖事件的类型。基于这个直觉，我们用了一个sort one-hot嵌入表示，做了一个事件的联合表示。&lt;/p&gt;
&lt;p&gt;首先把事件时长映射到一个向量上去&lt;/p&gt;
$$
\tag{4} p^d = d\_t W\_d + b\_d \in \mathbb{R}^P
$$&lt;p&gt;然后在这个向量上面做一个softmax运算&lt;/p&gt;
$$
\tag{5} s^d\_i = \frac{\exp(p^d\_i)}{\sum^P\_{k=1} \exp(p^d\_k)}
$$&lt;p&gt;然后做一个线性变换&lt;/p&gt;
$$
\tag{6} g\_d = s^d E^s \in \mathbb{R}^E
$$&lt;p&gt;然后把事件嵌入和这个时间嵌入求平均，得到事件嵌入表示：&lt;/p&gt;
$$
\tag{7} x\_t \leftarrow \frac{x\_t + g\_d}{2} \in \mathbb{R}^E
$$&lt;h1 id="4-next-event-duration-as-a-regularizer"&gt;4 Next Event Duration as A Regularizer
&lt;/h1&gt;&lt;p&gt;这里讨论的是通过让模型去预测下一事件的时长来增强模型。这个时长是通过对RNN循环层做线性变换得到的。对于时间步$t$，来说，需要预测的duration是$d’_{t+1}$。它的损失回传后会起到一个正则的作用。而且可以对事件预测输出层路径上的多个层进行正则。&lt;/p&gt;
&lt;h2 id="41-negative-log-lieklihood-of-time-prediction-error"&gt;4.1 Negative Log Lieklihood of Time Prediction Error
&lt;/h2&gt;&lt;p&gt;这里说，对于连续值的预测，一般用MSE，但是MSE这个指标需要和事件预测的损失在同一个数量级上。而事件损失，一般是一个log形式的损失，也就是说这个数会比较小。Hinton &amp;amp; van Camp, 1993研究证明最小化平方损失可以写成最大化0均值高斯分布的概率密度，而且不需要duration服从高斯分布，但是预测误差需要。因此正则项要做一个标准化，&lt;/p&gt;
$$
\tag{8} R^N\_t = \frac{(d'\_{t+1} - d\_{t+1})^2}{2\sigma^2\_i}
$$&lt;p&gt;$\sigma_i$是通过训练集的duration算出来的，然后在训练的过程中，通过时长预测误差的分布来更新。&lt;/p&gt;
&lt;h2 id="42-cross-entropy-loss-on-time-projection"&gt;4.2 Cross Entropy Loss on Time Projection
&lt;/h2&gt;&lt;p&gt;这里说，对于时长的损失计算还可以用softmax。&lt;/p&gt;
&lt;p&gt;因为3.2节提到了一个把连续值映射到向量空间的办法，使用同样的办法可以计算另一种损失：&lt;/p&gt;
$$
\tag{9} R^X\_t = - \sum^P\_{k=1} Proj\_k (d\_{t+1}) \log{Proj\_k (d'\_{t+1})}
$$&lt;p&gt;$Proj$就是公式4和5定义的投影函数，$Proj_k$是投影向量中的第$k$项。当3.2节的事件与时间的联合嵌入表示和这个损失都使用的时候，可以把投影函数的权重共享。&lt;/p&gt;
&lt;h1 id="5-experiments"&gt;5 Experiments
&lt;/h1&gt;&lt;p&gt;用了5个数据集。&lt;/p&gt;
&lt;h2 id="51-数据预处理"&gt;5.1 数据预处理
&lt;/h2&gt;&lt;p&gt;做了一些特别稀有的事件的过滤。有些事件少于5次的用OOV代替了。使用MAP@K和Precision@K来评估。&lt;/p&gt;
&lt;p&gt;训练、验证、测试的比例是8:1:1&lt;/p&gt;
&lt;h2 id="52-模型配置"&gt;5.2 模型配置
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;NoTime: 就用一个简单的LSTM&lt;/li&gt;
&lt;li&gt;TimeConcat: 把duration做log变换，与事件嵌入表示拼接，输入RNN&lt;/li&gt;
&lt;li&gt;TimeMask: 3.1节的方法&lt;/li&gt;
&lt;li&gt;TimeJoint: 3.2节的方法&lt;/li&gt;
&lt;li&gt;RMTPP: &lt;a class="link" href="https://www.kdd.org/kdd2016/papers/files/rpp1081-duA.pdf" target="_blank" rel="noopener"
&gt;RMTPP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="54-实验结果"&gt;5.4 实验结果
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/time-dependent-representation-for-neural-event-sequence-prediction/Fig3.jpg"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;Effectiveness of Temporal Representation: 图3展示出了TimeMask和TimeJoint的有效性。MIMIC II数据集上面没效果，可能是加时间本来就没啥用。结论就是，用这两个东西肯定比只加时间的值到RNN里面要有效。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/time-dependent-representation-for-neural-event-sequence-prediction/Table1_2.jpg"
loading="lazy"
alt="Table 1 &amp; 2"
&gt;&lt;/p&gt;
&lt;p&gt;表1和表2也证明了加入时间的有效性。而且有些时候直接加时间可能会伤害模型的效果。&lt;/p&gt;
&lt;p&gt;Effectiveness of Event Duration Regularization: 表1和表2证明了正则的有效性。&lt;/p&gt;
&lt;p&gt;Learned Time Representation: 这段说的不明所以，论文里面还有错误，图画的也不清晰，没懂。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/time-dependent-representation-for-neural-event-sequence-prediction/Fig4.jpg"
loading="lazy"
alt="Figure4"
&gt;&lt;/p&gt;</description></item><item><title>Event sequence metric learning</title><link>https://davidham3.github.io/blog/p/event-sequence-metric-learning/</link><pubDate>Tue, 26 Apr 2022 14:38:05 +0000</pubDate><guid>https://davidham3.github.io/blog/p/event-sequence-metric-learning/</guid><description>&lt;p&gt;这是一篇讲事件序列度量学习的文章，提出的模型叫MeLES，Metric Learning for Event Sequences。&lt;a class="link" href="https://arxiv.org/abs/2002.08232" target="_blank" rel="noopener"
&gt;Event sequence metric learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;事件序列主要是指连续时间下的离散事件。比如用户的信用卡交易、在网站上的点击行为等等。&lt;/p&gt;
&lt;h1 id="1-模型架构"&gt;1. 模型架构
&lt;/h1&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/event-sequence-metric-learning/Fig1.jpg"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;h1 id="2-原理"&gt;2. 原理
&lt;/h1&gt;&lt;h2 id="21-样本构成"&gt;2.1 样本构成
&lt;/h2&gt;&lt;p&gt;每 $N$ 个序列构成一个batch，对这个batch内的序列进行切分，有三种切分方式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;保持事件顺序，随机不放回采样&lt;/li&gt;
&lt;li&gt;随机切分序列，子序列之间不重叠&lt;/li&gt;
&lt;li&gt;随机切分序列，子序列之间重叠&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;不管怎么切，都能把一个序列切成多个子序列，这里将每个序列切成 $K$ 个子序列，那么一个batch就可以得到 $N \times K$ 个子序列。&lt;/p&gt;
&lt;p&gt;在这些子序列中，来自同一个序列的两个子序列组成的pair，为正样本，来自不同序列的两个子序列组成的pair为负样本。这样，对于每个子序列，就有 $K - 1$ 个正样本，$(N - 1) \times K$ 个负样本。&lt;/p&gt;
&lt;h2 id="22-序列表示"&gt;2.2 序列表示
&lt;/h2&gt;&lt;p&gt;然后使用RNN或者是transformer对子序列进行编码，编码后可以得到一个向量，这个向量就是这个子序列的嵌入表示。拿到这个表示，就可以计算损失了。&lt;/p&gt;
&lt;h2 id="23-损失"&gt;2.3 损失
&lt;/h2&gt;&lt;p&gt;计算损失的时候，最简单的想法肯定就是类似交叉熵一样的损失，正样本的损失加上负样本的损失即可。&lt;/p&gt;
&lt;p&gt;但是之前的研究认为，有些嵌入表示，他们的距离过于远，这种样本对模型训练没什么用，因此本文给了两个损失函数来剔除这种情况。一个叫contrastive loss，一个叫margin loss，原理都是一样的。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/event-sequence-metric-learning/loss.jpg"
loading="lazy"
alt="loss"
&gt;&lt;/p&gt;
&lt;p&gt;对于contrastive loss来说，正样本的损失正常计算，而负样本的损失，如果pair中的两个表示的距离大于 $m$ ，就不要了。&lt;/p&gt;
&lt;p&gt;对于margin loss是同样的原理，$b + m$ 和 $b - m$ 构成了损失函数的边界，正样本的距离要大于 $b - m$ 才有意义，而负样本的距离要小于 $b + m$ 才有意义。而且，当 $b &amp;lt; m$ 的时候，这个损失就会变得和上面的contrastive loss一样，只考虑负样本的margin，因为只要距离是欧式距离，在任何情况下 $max(0, D^i_W - b + m) = D^i_W - b + m &amp;gt; 0$。&lt;/p&gt;
&lt;h2 id="24-负样本采样策略"&gt;2.4 负样本采样策略
&lt;/h2&gt;&lt;p&gt;然后，除了上述的损失函数可以控制两个距离过远的负样本不计算损失，还可以做负样本采样，也就是刚才说的 $(N - 1) \times K$ 个负样本，只取出一部分用来训练。这里有4种方式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;随机采样&lt;/li&gt;
&lt;li&gt;难例挖掘，对每个整理生成$k$个难例作为负样本&lt;/li&gt;
&lt;li&gt;负样本采样的时候考虑距离因素&lt;/li&gt;
&lt;li&gt;第四个没看明白他在说啥，倒是给了个参考文献：Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. FaceNet: A&lt;br&gt;
unified embedding for face recognition and clustering. 2015 IEEE Conference on&lt;br&gt;
Computer Vision and Pattern Recognition (CVPR) (2015), 815–823.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;不管是上述哪种负采样方案，除了第一种，都是要算距离的，也就是算两个embedding之间的距离。而且是要算batch内任意两个embedding之间的距离，或者说是算 $(N - 1) \times K$ 个距离。如果用欧氏距离计算嵌入 $A$ 和 $B$ 之间的距离，那么 $D(A, B) = \sqrt{\sum_i (A_i - B_i)^2} = \sqrt{\sum_i A^2_i + \sum_i B^2_i - 2 \sum_i A_i B_i}$，这里为了计算简便，只要让 $\Vert A \Vert = \Vert B \Vert = 1$ 就好了，那就能转换成 $D(A, B) = \sqrt{2 - 2(A \cdot B)}$。所以，为了达成上面的目标，让 $A$ 和 $B$ 的模等于1，只要对这些嵌入表示做标准化，就可以实现了。论文里面说，做了这个操作之后，负样本采样的计算复杂度是 $O(n^2h)$，这个我还没想明白，后面再说吧。&lt;/p&gt;
&lt;h1 id="3-实验"&gt;3. 实验
&lt;/h1&gt;&lt;p&gt;两个数据集都是银行交易数据，主要是通过交易事件序列预测用户的年龄与性别。&lt;/p&gt;
&lt;h2 id="31-baselines"&gt;3.1 Baselines
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;手工特征+GBM，手工构建了近1k个特征，然后用LightGBM。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Contrastive Predictive Coding(CPC)，一个自监督学习方法，Aäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation&lt;br&gt;
Learning with Contrastive Predictive Coding. CoRR abs/1807.03748 (2018).&lt;br&gt;
arXiv:1807.03748 &lt;a class="link" href="http://arxiv.org/abs/1807.03748" target="_blank" rel="noopener"
&gt;http://arxiv.org/abs/1807.03748&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;除了上面两个方法，作者还试了编码器网络+分类网络直接用于监督学习任务，这里就没有预训练了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="32-参数选择"&gt;3.2 参数选择
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/event-sequence-metric-learning/Table4_to_7.jpg"
loading="lazy"
alt="result"
&gt;&lt;/p&gt;
&lt;p&gt;上面4个表的结论：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;不同编码器效果不同&lt;/li&gt;
&lt;li&gt;在训练集上表现最好的损失函数在测试集上不一定是最好的&lt;/li&gt;
&lt;li&gt;随机slice比随机采样更好&lt;/li&gt;
&lt;li&gt;难例挖掘带来的提升是显著的（但是论文前边根本没仔细介绍难例挖掘好吧。。。）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/event-sequence-metric-learning/Fig2.jpg"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;p&gt;图2是说嵌入在800维的时候效果最好，用bias-variance来解释。维数少的时候高bias，信息丢失，维数高的时候高variance，噪声多了。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/event-sequence-metric-learning/Fig3.jpg"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;图3一样，256到2048比较平缓，下游任务的效果没有明显增强。&lt;/p&gt;
&lt;p&gt;作者说嵌入维数的增加，训练时间和显存消耗都是线性增加的。&lt;/p&gt;
&lt;h2 id="33-嵌入可视化"&gt;3.3 嵌入可视化
&lt;/h2&gt;&lt;p&gt;tSNE，染色是用数据集中的target value染色的。学习完全是自监督的。交易序列表示的是用户的行为，因此模型可以捕获行为模式，产出的embedding如果相近，则说明用户的行为模式相似。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/event-sequence-metric-learning/Fig4.jpg"
loading="lazy"
alt="Figure4"
&gt;&lt;/p&gt;
&lt;h2 id="34-结果"&gt;3.4 结果
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/event-sequence-metric-learning/Table8.jpg"
loading="lazy"
alt="Table8"
&gt;&lt;/p&gt;
&lt;p&gt;对比手工构建的特征，模型效果强劲。fine-tuned的表示效果最好。另外可以看到的是，使用手工特征+事件序列嵌入表示的模型效果比纯手工特征效果更好。&lt;/p&gt;
&lt;h3 id="341-关于半监督的实验"&gt;3.4.1 关于半监督的实验
&lt;/h3&gt;&lt;p&gt;只取了一部分标签做实验，就像监督学习一样用手工特征的lightgbm和CPC。对于嵌入生成方法（MeLES和CPC），分别使用lightgbm和fine-tuned模型来评估效果。同时还比了监督模型在这些label上的效果。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/event-sequence-metric-learning/Fig5.jpg"
loading="lazy"
alt="Figure5"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/event-sequence-metric-learning/Fig6.jpg"
loading="lazy"
alt="Figure6"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/event-sequence-metric-learning/Fig78.jpg"
loading="lazy"
alt="Figure7_and_Figure8"
&gt;&lt;/p&gt;
&lt;p&gt;结论就是标签少的时候，效果很好。&lt;/p&gt;
&lt;h1 id="4-结论"&gt;4. 结论
&lt;/h1&gt;&lt;p&gt;提出了MeLES，效果很好，而且还可以在半监督中做预训练。好处是基本不用怎么对数据做处理就可以拿到嵌入表示，获得好的效果。而且在新的事件加入的时候，甚至是可以增量更新已经计算的嵌入表示。另一方面是嵌入表示无法还原原始的事件序列，可以起到数据加密的作用。&lt;/p&gt;
&lt;p&gt;这里提到的增量更新其实就是，RNN的计算只要上一个时间步的信息就好了，不需要从头再训练一次，因此如果有新的事件到来，从最后一次的状态开始算就好了，这就叫增量更新。&lt;/p&gt;</description></item><item><title>Neural Collaborative Filtering</title><link>https://davidham3.github.io/blog/p/neural-collaborative-filtering/</link><pubDate>Wed, 13 May 2020 17:07:14 +0000</pubDate><guid>https://davidham3.github.io/blog/p/neural-collaborative-filtering/</guid><description>&lt;p&gt;《Neural Collaborative Filtering》WWW 2017。这篇论文使用全连接神经网络实现了一个矩阵分解的推荐系统。给定一个user的特征表示，给定一个item的特征表示，通过神经网络输出用户对这个item感兴趣的分数。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1708.05031" target="_blank" rel="noopener"
&gt;Neural Collaborative Filtering&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;最近看了看推荐系统，这篇论文提出了一个 NeuMF 的模型，基于全连接层的一个模型。&lt;/p&gt;
&lt;p&gt;模型很简单，主要分两个部分，一个GMF部分，一个MLP部分。&lt;/p&gt;
&lt;h1 id="generalized-matrix-factorization-gmf"&gt;Generalized Matrix Factorization (GMF)
&lt;/h1&gt;&lt;p&gt;这个部分是 user 的特征向量与 item 的特征向量做 element-wise product。&lt;/p&gt;
&lt;h1 id="multi-layer-perceptron-mlp"&gt;Multi-Layer Perceptron (MLP)
&lt;/h1&gt;&lt;p&gt;多层感知机部分就是将 user 和 item 的特征向量拼接，然后放到多层全连接里面。&lt;/p&gt;
&lt;h1 id="neumf"&gt;NeuMF
&lt;/h1&gt;&lt;p&gt;最后将这两个模块的输出拼接，然后丢到一个全连接层里面，映射到目标的分数，0到1，然后使用对数损失，训练即可。&lt;/p&gt;
&lt;h1 id="experiments"&gt;Experiments
&lt;/h1&gt;&lt;p&gt;实验部分，数据集的划分是，拿到用户所有的正例后，对每个用户，随机取一个作为测试集要预测的正例。然后取一定数量的负样本作为测试集的负例。&lt;/p&gt;
&lt;p&gt;其他的正例作为训练集的正例，训练的时候通过采样的方式取负样本丢到网络中训练。&lt;/p&gt;
&lt;p&gt;评价指标有两个：Hit Ratio 和 Normalized Discounted Cumulative Gain。&lt;/p&gt;
&lt;p&gt;测试的时候，我们上述的“一定数量的负样本作为测试集的负例”，这个“一定数量”取999，那么测试集里面每个用户就有1000个样本，因为还要加那一个正例。让模型对这1000个样本进行预测，预测出1000个分数，取 top10。如果这最大的10个分数对应的 item 中，有那个正例，说明我们的模型在1000个 item 中，预测出的前十名里面成功命中那个正例了，那这个用户的 hit rate 就为1，否则为0，然后算所有用户的平均值即可，就是 hit ratio。&lt;/p&gt;
&lt;p&gt;NDCG 的指标计算我还没有研究，等研究后再写。模型的代码看了 mxnet 官方提供的 example：&lt;a class="link" href="https://github.com/apache/incubator-mxnet/tree/master/example/neural_collaborative_filtering" target="_blank" rel="noopener"
&gt;NeuMF&lt;/a&gt;，速度很快。&lt;/p&gt;
&lt;p&gt;具体的实验结果，我在我自己用爬虫抓的一个数据集上看，效果没有 implicit 这个库里面的 als 效果好。ALS 的 HR@10 能跑到51%，我自己实现的 NeuMF 只能跑到48%，我使用的是 user 和 item 的 id 作为特征。但是 NeuMF 相比 ALS 的一个优势是可以加入 user 和 item 的其他特征信息，效果可能会更好一点，还需要进一步实验论证。&lt;/p&gt;</description></item><item><title>Multi-Range Attentive Bicomponent Graph Convolutional Network for Traffic Forecasting</title><link>https://davidham3.github.io/blog/p/multi-range-attentive-bicomponent-graph-convolutional-network-for-traffic-forecasting/</link><pubDate>Fri, 03 Jan 2020 20:18:29 +0000</pubDate><guid>https://davidham3.github.io/blog/p/multi-range-attentive-bicomponent-graph-convolutional-network-for-traffic-forecasting/</guid><description>&lt;p&gt;AAAI 2020，原文链接：&lt;a class="link" href="https://arxiv.org/abs/1911.12093" target="_blank" rel="noopener"
&gt;https://arxiv.org/abs/1911.12093&lt;/a&gt;。&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;交通预测在运输和公共安全中扮演重要角色，由于复杂的时空依赖和路网和交通状况带来的不确定性使这个问题很有挑战。最新的研究专注于使用图卷积网络 GCNs 对一个固定权重的图进行建模，即对空间依赖建模。然而，边，即两个结点之间的关系更加复杂且两者相互影响。我们提出了 Multi-Range Attentive Bicomponent GCN (MRA-BGCN)，一种新的用于交通预测的深度学习框架。我们先根据路网上结点的距离构建结点图，在根据不同的边的交互模式构造边图。然后，我们使用 bicomponent 图卷积实现结点和边的交互。这个多范围注意力机制用来聚合不同邻居范围的信息，自动地学习不同范围的重要性。大量的实验在两个真实数据集，METR-LA 和 PEMS-BAY 上开展，显示出我们的模型效果很好。&lt;/p&gt;
&lt;h1 id="introduction"&gt;Introduction
&lt;/h1&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/multi-range-attentive-bicomponent-graph-convolutional-network-for-traffic-forecasting/Fig1.png"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;讲了好多历史。。。然后是论点部分：&lt;/p&gt;
&lt;p&gt;我们认为 DCRNN 和 STGCN 虽说集成了 GCN，但是有两个点忽略了：&lt;/p&gt;
&lt;p&gt;首先，这些方法主要关注通过在一个固定权重的图上部署 GCN 对空间依赖建模。然而，边更复杂。图 1a 中，传感器 1 和 3，还有 2 和 3，通过路网连接。显然，这些关联随当前的交通状况改变，他们之间也互相交互。图 1b 所示，现存的方法根据路网距离构建一个固定的带权图，使用 GCN 实现这些结点的交互，但是结点间的关联性在邻接矩阵中通过固定的值表示，这就忽略了边的复杂性和交互性。&lt;/p&gt;
&lt;p&gt;其次，这些方法经常使用一个给定范围内聚合的信息，比如 $k$ 阶邻居，忽略多个范围的信息。然而，不同范围的信息表现出不同的交通属性。小的范围表现出局部依赖，大范围倾向于表现全局的交通模式。此外，不同范围的信息也不是永远都具有相同的分量。举个例子，一次交通事故，一个结点主要受它最近的邻居的影响，这样模型就应该更关注它，而不是给其他的 $k$ 阶邻居相同的关注。&lt;/p&gt;
&lt;p&gt;为了解决上述两点问题，我们提出了 MRA-BGCN，不仅考虑结点关联，也把边作为实体，考虑他们之间的关系，如图 1c，我们还利用了不同的范围信息。我们的贡献：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提出 MRA-BGCN，引入 bicomponent 图卷积，对结点和边直接建模。结点图根据路网距离构建，边图根据边的交互模式、stream connectivity 和 竞争关系构建。&lt;/li&gt;
&lt;li&gt;我们针对 bicomponent 图卷积提出多范围注意力机制，可以聚合不同范围邻居的信息，学习不同范围的重要性。&lt;/li&gt;
&lt;li&gt;我们开展了大量的实验，实验效果很好。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="preliminaries"&gt;Preliminaries
&lt;/h1&gt;&lt;h2 id="problem-definition"&gt;Problem Definition
&lt;/h2&gt;&lt;p&gt;给定历史的数据，预测未来的数据。$N$ 个结点组成的图 $G = (V, E, \bm{A})$。时间 $t$ 路网上的交通数据表示为图信号 $\bm{X}^{(t)} \in \mathbb{R}^{N \times P}$，$P$ 是特征数。交通预测是过去的数据预测未来：&lt;/p&gt;
$$
[\bm{X}^{(t-T'+1):t},G] \xrightarrow{f} [\bm{X}^{(t+1)}:(t+T)],
$$&lt;p&gt;$\bm{X}^{(t-T&amp;rsquo;+1):t} \in \mathbb{R}^{N \times P \times T&amp;rsquo;}$，$\bm{X}^{(t+1):(t+T)} \in \mathbb{R}^{N \times P \times T}$。&lt;/p&gt;
&lt;h2 id="graph-convolution"&gt;Graph Convolution
&lt;/h2&gt;&lt;p&gt;不介绍了。&lt;/p&gt;
&lt;h1 id="methodology"&gt;Methodology
&lt;/h1&gt;&lt;h2 id="model-overview"&gt;Model Overview
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/multi-range-attentive-bicomponent-graph-convolutional-network-for-traffic-forecasting/Fig2.png"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;p&gt;图 2 展示了 MRA-BGCN 的架构，包含两个部分：（1）双组件图卷积模块；（2）多范围注意力层。双组件图卷积模块包含多个结点图卷积层和边图卷积层，直接对结点和边的交互建模。多范围注意力层聚合不同范围的邻居信息，学习不同范围的重要性。此外，我们融合 MRA-BGCN 和 RNN 对时间依赖建模完成交通预测。&lt;/p&gt;
&lt;h2 id="bicomponent-graph-convolution"&gt;Bicomponent Graph Convolution
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/multi-range-attentive-bicomponent-graph-convolutional-network-for-traffic-forecasting/Fig3.png"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;图卷积可以有效聚合结点之间的交互关系，然而，交通预测中边更复杂（这句话说三遍了）。因此我们提出双组件图卷积，直接对结点和边的交互建模。&lt;/p&gt;
&lt;p&gt;Chen 等人提出边的邻近的 line graph 来建模边的关系。$G = (V, E, \bm{A})$ 表示结点有向图，$G_L = (V_L, E_L, \bm{A}_L)$ 是对应的 line graph，$G_L$ 的结点 $V_L$ 是 $E$ 中有序的边。$\bm{A}_L$ 是无权的邻接矩阵，编码了结点图中的边邻接关系，有关系就等于1。&lt;/p&gt;
&lt;p&gt;尽管 line graph 可以考虑边的邻接，它仍然是一个无权图且只认为两条边中的一条边的汇点和另一条边的源点相同时，这两条才相关。然而，对于刻画交通预测中各种各样边的交互关系来说这不够高效。如图 3 所示，我们定义两类边的交互模式来构建边图 $G_e = (V_e, E_e, \bm{A}_e)$。$V_e$ 中的每个节点表示 $E$ 中的边。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stream connectivity&lt;/strong&gt; 在交通网络中，路网可能受它上下游的路段影响。如图 3a 所示，$(i \rightarrow j)$ 是 $(j \rightarrow k)$ 的上游的边，因此他们是相互关联的。直观上来看，如果结点 $j$ 有很多数量的邻居，那么 $(i \rightarrow j)$ 和 $(j \rightarrow k)$ 之间的关系是弱的，因为它还要受其他邻居的影响。我们使用高斯核计算边的权重用来表示 $\bm{A}_e$ 中的 stream connectivity：&lt;/p&gt;
$$\tag{2}
\bm{A}\_{e, (i \rightarrow j), (j \rightarrow k)} = \bm{A}\_{e, (j \rightarrow k), (i \rightarrow j)} = \text{exp}(- \frac{(\text{deg}^-(j) + \text{deg}^+(j) - 2)^2}{\sigma^2})
$$&lt;p&gt;$\text{deg}^-(j)$ 和 $\text{deg}^+(j)$ 分别表示结点 $j$ 的入度和出度，$\sigma$ 是结点度的标准差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Competitive relationship&lt;/strong&gt; 路网&lt;/p&gt;</description></item><item><title>GMAN: A Graph Multi-Attention Network for Traffic Prediction</title><link>https://davidham3.github.io/blog/p/gman-a-graph-multi-attention-network-for-traffic-prediction/</link><pubDate>Thu, 02 Jan 2020 17:03:30 +0000</pubDate><guid>https://davidham3.github.io/blog/p/gman-a-graph-multi-attention-network-for-traffic-prediction/</guid><description>&lt;p&gt;AAAI 2020，使用编码解码+att的架构，只不过编码和解码都使用 attention 组成。主要的论点是空间和时间的关联性是动态的，所以设计这么一个纯注意力的框架。值得注意的点是：由于注意力分数的个数是平方级别的，在计算空间注意力的时候，一旦结点数很大，这里会有超大的计算量和内存消耗，这篇文章是将结点分组后，计算组内注意力和组间注意力。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1911.08415" target="_blank" rel="noopener"
&gt;https://arxiv.org/abs/1911.08415&lt;/a&gt;。&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;长时间范围的交通流预测是个挑战，两方面原因：交通系统的复杂性，很多影响因素的持续变化性。我们在这篇论文中，专注于时空因素，提出了一个图多注意力机智网络（GMAN），预测路网上不同区域的交通状况。GMAN 使用一个编码解码结构，编码解码器都由多个时空注意力块组成，时空注意力块对交通状况上的时空因素的影响建模。编码器将输入的交通特征编码，解码器输出预测序列。编码解码器之间，有一个变换注意力层，用来把编码器编码后的交通特征生成成未来时间步的序列表示，然后把这个表示输入到解码器里面。变换注意力机制对历史和未来时间步的关系建模，可以减轻多步预测中的错误积累。两个真实数据集上的交通预测任务（一个是流量预测，一个是速度预测）显示 GMAN 的效果优越。在1小时的预测上，GMAN 在 MAE 比 state-of-the-art 好4%。源码在：&lt;a class="link" href="https://github.com/zhengchuanpan/GMAN" target="_blank" rel="noopener"
&gt;https://github.com/zhengchuanpan/GMAN&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="introduction"&gt;Introduction
&lt;/h1&gt;&lt;p&gt;交通预测的目标是基于历史观测预测未来的交通状况。在很多应用中扮演着重要的角色。举个例子，精确的交通预测可以帮助交管部门更好的控制交通，减少拥堵。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/gman-a-graph-multi-attention-network-for-traffic-prediction/Fig1.png"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;邻近区域的交通状况会互相影响。大家使用 CNN 捕获这样的空间依赖。同时，一个地方的交通状况和它的历史记录有关。RNN 广泛地用于这样时间相关性的建模。最近的研究将交通预测变为图挖掘问题，因为交通问题受限于路网。使用 GCN 的这些研究在短期预测（5 到 15 分钟）内表现出不错的效果。然而，长期预测（几个小时）仍缺乏令人满意的效果，主要受限于以下几点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;复杂的时空关联：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;动态的空间关联。如图 1 所示，路网中的传感器之间的关联随时间剧烈地变化，比如高峰时段的前后。如何动态地选择相关的检测器数据来预测一个检测器在未来长时间范围的交通状况是一个挑战。&lt;/li&gt;
&lt;li&gt;非线性的时间关联。图 1，一个检测器的交通状况可能变化得非常剧烈，且可能由于事故等因素，突然影响不同时间步之间的关联性。如何自适应地随时间的推移对这种非线性时间关联建模，也是一个挑战。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;对误差传递的敏感。长期预测上，每个时间步上小的错误都会被放大。这样的误差传递对于远期时间预测来说仍具有挑战性。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;为了解决上述挑战，我们提出了一个图多注意力网络（GMAN）来预测未来的交通状况。这里指的交通状况是一个交通系统中可以记录为数值的观测值。为了描述，我们这里专注于流量和速度预测，但是我们的模型是可以应用到其他数值型的交通数据上的。&lt;/p&gt;
&lt;p&gt;GMAN 使用编码解码架构，编码器编码交通特征，解码器生成预测序列。变换注意力层用来把编码历史特征转换为未来表示。编解码器都由一组时空注意力块 &lt;em&gt;ST-Attention blocks&lt;/em&gt; 组成。每个时空注意力块由一个空间注意力、一个时间注意力和一个门控融合机制组成。空间注意力建模动态空间关联，时间注意力建模非线性时间关联，门控融合机制自适应地融合空间和时间表示。变换注意力机制建模历史和未来的关系，减轻错误传播。两个真实世界数据集证明 GMAN 获得了最好的效果。&lt;/p&gt;
&lt;p&gt;我们的贡献&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提出空间注意力和时间注意力对动态空间和非线性时间关联分别建模。此外，我们设计了一个门控融合机制，自适应地融合空间注意力和时间注意力机制的的信息。&lt;/li&gt;
&lt;li&gt;提出一个变换注意力机制将历史交通特征转换为未来的表示。这个注意力机制对历史和未来的关系直接建模，减轻错误传播的问题。&lt;/li&gt;
&lt;li&gt;我们在两个数据集上评估了我们的图多注意力网络，在 1 小时预测问题上比 state-of-the-art 提高了 4%。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="preliminaries"&gt;Preliminaries
&lt;/h1&gt;&lt;p&gt;路网表示为一个带全有向图 $\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathcal{A})$。$\mathcal{V}$ 是 $N$ 个结点的集合；$\mathcal{E}$ 是边集；$\mathcal{A} \in \mathbb{R}^{N \times N}$ 是邻接矩阵，表示结点间的相似性，这个相似性是结点在路网上的距离。&lt;/p&gt;
&lt;p&gt;时间步 $t$ 的路网状况表示为图信号 $X_t \in \mathbb{R}^{N \times C}$，$C$ 是特征数。&lt;/p&gt;
&lt;p&gt;研究的问题：给定 $N$ 个结点历史 $P$ 个时间步的观测值 $\mathcal{X} = (X_{t_1}, X_{t_2}, \dots. X_{t_P}) \in \mathbb{R}^{P \times N \times C}$，我们的目标是预测未来 $Q$ 个时间步所有结点的交通状况，表示为 $\hat{Y} = (\hat{X}_{t_{P+1}}, \hat{X}_{t_{P+2}}, \dots, \hat{X}_{t_{P+Q}}) \in \mathbb{R}^{Q \times N \times C}$。&lt;/p&gt;
&lt;h1 id="graph-multi-attention-network"&gt;Graph Multi-Attention Network
&lt;/h1&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/gman-a-graph-multi-attention-network-for-traffic-prediction/Fig2.png"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;p&gt;图 2 描述了我们模型的架构。编码和解码器都有 STAtt Block 和残差连接。每个 ST-Attention block 由空间注意力机制、时间注意力机制和一个门控融合组成。编码器和解码器之间有个变换注意力层。我们还通过一个时空嵌入 spatial-temporal embedding (STE) 继承了图结构和时间信息到多注意力机制中。此外，为了辅助残差连接，所有层的输出都是 D 维。&lt;/p&gt;
&lt;h2 id="spatio-temporal-embedding"&gt;Spatio-Temporal Embedding
&lt;/h2&gt;&lt;p&gt;因为交通状况的变化受限于路网，集成路网信息到模型中很重要。为此，我们提出一个空间嵌入，把结点嵌入到向量中以此保存图结构信息。我们利用 node2vec 学习结点表示。此外，为了协同训练模型和预学习的向量，这些向量会放入一个两层全连接神经网络中。然后就可以拿到空间表示 $e^S_{v_i} \in \mathbb{R}^D$。&lt;/p&gt;
&lt;p&gt;空间嵌入只提供了固定的表示，不能表示路网中的传感器的动态关联性。我们提出了一个时间嵌入来把每个时间步编码到向量中。假设一天是 T 个时间步。我们使用 one-hot 编码星期、时间到 $\mathbb{R}^7$ 和 $\mathbb{R}^T$ 里面，然后拼接，得到 $\mathbb{R}^{T + 7}$。接下来，使用两层全连接映射到 $\mathbb{R}^D$。在我们的模型里面，给历史的 $P$ 个时间步和未来的 $Q$ 个时间步嵌入时间特征，表示为 $e^T_{t_j} \in \mathbb{R}^D$，$t_j = t_1, \dots, t_P, \dots, t_{P+Q}$。&lt;/p&gt;
&lt;p&gt;为了获得随时间变化的顶点表示，我们融合了上述的空间嵌入和时间嵌入，得到时空嵌入（STE），如图 2b 所示。结点 $v_i$ 在时间步 $t_j$，STE 定义为 $e_{v_i,t_j} = e^S_{v_i} + e^T_{t_j}$。因此，$N$ 个结点在 $P + Q$ 的时间步里的 STE 表示为 $E \in \mathbb{R}^{(P + Q) \times N \times D}$。STE 包含图结构和时间信息。它会用在空间、时间、变换注意力机制里面。&lt;/p&gt;
&lt;h2 id="st-attention-block"&gt;ST-Attention Block
&lt;/h2&gt;&lt;p&gt;我们将第 $l$ 个块的输入表示为 $H^{(l-1)}$，结点 $v_i$ 在时间步 $t_j$ 的隐藏状态表示为 $h^{(l-1)}_{v_i,t_j}$。第 $l$ 块中的空间和时间注意力机制的输出表示为 $H^{(l)}_S$ 和 $H^{(l)}_T$，隐藏状态表示为 $hs^{(l)}_{v_i,t_j}$ 和 $ht^{(l)}_{v_i,t_j}$。门控融合后，第 $l$ 层的输出表示为 $H^{(l)}$。&lt;/p&gt;
&lt;p&gt;我们将非线性变换表示为：&lt;/p&gt;
$$\tag{1}
f(x) = \text{ReLU}(x\mathbf{W} + \mathbf{b}).
$$&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/gman-a-graph-multi-attention-network-for-traffic-prediction/Fig3.png"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Spatial Attention&lt;/strong&gt; 一条路的交通状况受其他路的影响，且影响不同。这样的影响是高度动态的，随时间变化。为了建模这些属性，我们设计了一个空间注意力机制动态地捕获路网中传感器间的关联性。核心点是在不同的时间步动态地给不同的结点分配权重，如图 3 所示。对于时间步 $t_j$ 的结点 $v_i$，我们计算所有结点的带权和：&lt;/p&gt;
$$\tag{2}
hs^{(l)}\_{v\_i,t\_j} = \sum\_{v \in \mathcal{V}} \alpha\_{v\_i, v} \cdot h^{(l-1)}\_{v,t\_j},
$$&lt;p&gt;$\alpha_{v_i, v}$ 是结点 $v$ 对 $v_i$ 的注意力分数，注意力分数之和为1：$\sum_{v \in \mathcal{V}} \alpha_{v_i, v} = 1$。&lt;/p&gt;
&lt;p&gt;在一个确定的时间步，当前交通状况和路网结构能够影响传感器之间的关联性。举个例子，路上的拥挤可能极大地影响它临近路段的交通状况。受这个直觉的启发，我们考虑使用交通特征和图结构两方面来学习注意力分数。我们把隐藏状态和时空嵌入拼接起来，使用 scaled dot-product approach (Vaswani et al. 2017) 来计算结点 $v_i$ 和 $v$ 之间的相关性：&lt;/p&gt;
$$\tag{3}
s\_{v\_i, v} = \frac{&lt; h^{(l-1)}\_{v\_i,t\_j} \Vert\ e\_{v\_i,t\_j}, h^{(l-1)}+{v,t\_j}, \Vert e\_{v,t\_j} &gt;}{\sqrt{2D}}
$$&lt;p&gt;其中，$\Vert$ 表示拼接操作，$&amp;lt; \bullet, \bullet &amp;gt;$ 表示内积，$2D$ 表示 $h^{(l-1)}_{v_i,t_j} \Vert e_{v_i,t_j}$ 的维度。$s_{v_i,v}$ 通过 softmax 归一化：&lt;/p&gt;
$$\tag{4}
\alpha\_{v\_i,v} = \frac{\text{exp}(s\_{v\_i,v})}{\sum\_{v\_r \in \mathcal{V}} \text{exp}(s\_{v\_i,v\_r})}.
$$&lt;p&gt;得到注意力分数 $\alpha_{v_i,v}$ 之后，隐藏状态通过公式 2 更新。&lt;/p&gt;
&lt;p&gt;为了稳定学习过程，我们把空间注意力机制扩展为多头注意力机制。我们拼接 $K$ 个并行的注意力机制，使用不同的全连接映射：&lt;/p&gt;
$$\tag{5}
s^{(k)}\_{v\_i,v} = \frac{&lt; f^{(k)}\_{s,1} (h^{(l-1)}\_{v\_i,t\_j} \Vert e\_{v\_i,t\_j}), f^{(k)}\_{s,2} (h^{(l-1)}\_{v,t\_j} \Vert e\_{v,t\_j}) &gt;}{\sqrt{d}},
$$$$\tag{6}
\alpha^{(k)}\_{v\_i,v} = \frac{\text{exp}(s^{(k)}\_{v\_i,v})}{\sum\_{v\_r \in \mathcal{V}} \text{exp}(s^{(k)}\_{v\_i,v\_r})},
$$$$\tag{7}
hs^{(l)}\_{v\_i,t\_j} = \Vert^K\_{k=1} \lbrace \sum\_{v \in \mathcal{V}} \alpha^{(k)}\_{v\_i,v} \cdot f^{(k)}\_{s,3}(h^{(l-1)}\_{v,t\_j}) \rbrace,
$$&lt;p&gt;其中 $f^{(k)}_{s,1}(\bullet), f^{(k)}_{s,2}(\bullet), f^{(k)}_{s,3}(\bullet)$ 表示第 $k$ 注意力头的三个不同的非线性映射，即公式 1 ，产生 $d = D / K$ 维的输出。&lt;/p&gt;
&lt;p&gt;当结点数 $N$ 很大的时候，时间和内存消耗都会很大，达到 $N^2$ 的数量级。为了解决这个限制，我们提出了组空间注意力，包含了组内注意力分数和组间注意力分数，如图 4 所示。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/gman-a-graph-multi-attention-network-for-traffic-prediction/Fig4.png"
loading="lazy"
alt="Figure4"
&gt;&lt;/p&gt;
&lt;p&gt;我们把 $N$ 个结点随机划分为 $G$ 个组，每个组包含 $M = N / G$ 个结点，如果必要的话可以加 padding。每个组，我们使用公式 5，6，7 计算组内的注意力，对局部空间关系建模，参数是对所有的组共享的。然后，我们在每个组使用最大池化得到每个组的表示。接下来计算组间空间注意力，对组间关系建模，给每个组生成一个全局特征。局部特征和全局特征相加得到最后的输出。&lt;/p&gt;
&lt;p&gt;组空间注意力中，我们每个时间步需要计算 $GM^2 + G^2 = NM + (N / M)^2$ 个注意力分数。通过使梯度为0，我们知道 $M = \sqrt[3]{2N}$ 时，注意力分数的个数达到最大值 $2^{-1/3} N^{4/3} \ll N^2$。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/gman-a-graph-multi-attention-network-for-traffic-prediction/Fig5.png"
loading="lazy"
alt="Figure5"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Temporal Attention&lt;/strong&gt; 一个地点的交通状况和它之前的观测值有关，这个关联是非线性的。为了建模这些性质，我们设计了一个时间注意力机制，自适应地对不同时间步的非线性关系建模，如图 5 所示。可以注意到时间关联受到交通状况和对应的时间环境两者的影响。举个例子，早高峰的拥堵可能会影响交通好几个小时。因此，我们考虑交通特征和时间两者来衡量不同时间步的相关性。我们把隐藏状态和时空嵌入拼接起来，使用多头注意力计算注意力分数。对于结点 $v_i$，时间步 $t_j$ 与 $t$ 的相关性定义为：&lt;/p&gt;
$$\tag{8}
u^{(k)}\_{t\_j,t} = \frac{&lt; f^{(k)}\_{t,1}(h^{(l-1)}\_{v\_i,t\_j} \Vert e\_{v\_i,t\_j}), f^{(k)}\_{t,2}(h^{(l-1)}\_{v\_i,t} \Vert e\_{v\_i,t}) &gt;}{\sqrt{d}},
$$$$\tag{9}
\beta^{(k)}\_{t\_j,t} = \frac{\text{exp}(u^{(k)}\_{t\_j,t})}{\sum\_{t\_r \in \mathcal{N}\_{t\_j}}} \text{exp}(u^{(k)}\_{t\_j,t\_r}),
$$&lt;p&gt;$u^{(k)}_{t_j,t}$ 表示时间步 $t_j$ 和 $t$ 之间的相关性，$\beta^{(k)}_{t_j,t}$ 是第 $k$ 个头的注意力分数，表示时间步 $t$ 对时间步 $t_j$ 的重要性，两个 $f$ 是非线性变换，$\mathcal{N}_{t_j}$ 表示 $t_j$ 前的时间步的集合，即只考虑目标时间步以前的时间步，这样才有因果。一旦获得了注意力分数，时间步 $t_j$ 的结点 $v_i$ 的隐藏状态可以通过下面的公式更新：&lt;/p&gt;
$$\tag{10}
ht^{(l)}\_{v\_i,t\_j} = \Vert^K\_{k=1} \lbrace \sum\_{t \in \mathcal{N}\_{t\_j}} \beta^{(k)}\_{t\_j,t} \cdot f^{(k)}\_{t,3}(h^{(l-1)}\_{v\_i,t}) \rbrace
$$&lt;p&gt;$f$ 是非线性映射。公式 8，9，10 学习到的参数对所有结点和所有时间步共享，且并行计算。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gated Fusion&lt;/strong&gt; 一个时间步一条路上的交通状况与它自身之前的值和相邻道路上的交通状况相关。如图 2c 所示，我们设计了一个门控融合机制自适应地融合空间和时间表示。在第 $l$ 个块，空间和时间注意力的输出表示为 $H^{(l)}_S$ 和 $H^{(l)}_T$，两者的维度在编码器中是 $\mathbb{R}^{P \times N \times D}$，解码器中是 $\mathbb{R}^{Q \times N \times D}$。通过下式融合：&lt;/p&gt;
$$\tag{11}
H^{(l)} = z \odot H^{(l)}\_S + (1 - z) \odot H^{(l)}\_T,
$$$$\tag{12}
z = \sigma(H^{(l)}\_S \mathbf{W}\_{z,1} + H^{(l)}\_T \mathbf{W}\_{z,2} + \mathbf{b}\_z),
$$&lt;p&gt;门控融合机制自适应地控制每个时间步和结点上空间和时间依赖的流动。&lt;/p&gt;
&lt;h2 id="transform-attention"&gt;Transform Attention
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/gman-a-graph-multi-attention-network-for-traffic-prediction/Fig6.png"
loading="lazy"
alt="Figure6"
&gt;&lt;/p&gt;
&lt;p&gt;为了减轻错误传播的问题，我们在编码器和解码器之间加入了一个变换注意力层。它能直接地对历史时间步和未来时间步的关系建模，将交通特征编码为未来的表示，作为解码器的输入。如图 6 所示，对于结点 $v_i$ 来说，预测的时间步 $t_j \ (t_j = t_{P+1}, \dots, t_{P+Q})$ 和历史的时间步 $t \ (t_1, \dots, t_P)$ 通过时空嵌入来衡量：&lt;/p&gt;
$$\tag{13}
\lambda^{(k)}\_{t\_j,t} = \frac{&lt; f^{(k)}\_{tr,1}(e\_{v\_i,t\_j}), f^{(k)}\_{tr,2}(e\_{v\_i,t}) &gt;}{\sqrt{d}},
$$$$\tag{14}
\gamma^{(k)}\_{t\_j,t} = \frac{\text{exp}(\lambda^{(k)}\_{t\_j,t})}{\sum^{t\_P}\_{t\_r=t\_1} \text{exp}(\lambda^{(k)}\_{t\_j,t\_r})}.
$$&lt;p&gt;编码的交通特征通过注意力分数 $\gamma^{(k)}_{t_j,t}$ 自适应地在历史 $P$ 个时间步选择相关的特征，变换到解码器的输入：&lt;/p&gt;
$$\tag{15}
h^{(l)}\_{v\_i,t\_j} = \Vert^K\_{k=1} \lbrace \sum^{t\_P}\_{t=t\_1} \gamma^{(k)}\_{t\_j,t} \cdot f^{(k)}\_{tr,3}(h^{(l-1)}\_{v\_i,t}) \rbrace.
$$&lt;h2 id="encoder-decoder"&gt;Encoder-Decoder
&lt;/h2&gt;&lt;p&gt;如图 2a 所示，GMAN 是编码解码架构。在进入编码器前，历史记录 $\mathcal{X} \in \mathbb{R}^{P \times N \times C}$ 通过全连接变换到 $H^{(0)} \in \mathbb{R}^{P \times N \times D}$。然后 $H^{(0)}$ 输入到 $L$ 个时空注意力块组成的编码器中，产生输出 $H^{(L)} \in \mathbb{R}^{P \times N \times D}$。然后变换注意力层把编码特征从 $H^{(L)}$ 转换为 $H^{(L+1)} \in \mathbb{R}^{Q \times N \times D}$。然后 $L$ 个时空注意力块的解码器产生输出 $H^{(2L + 1)} \in \mathbb{R}^{Q \times N \times D}$。最后，全连接层输出 $Q$ 个时间步的预测 $\hat{Y} \in \mathbb{R}^{Q \times N \times C}$。&lt;/p&gt;
&lt;p&gt;GMAN 可以通过最小化 MAE 来优化：&lt;/p&gt;
$$\tag{16}
\mathcal{L}(\Theta) = \frac{1}{Q} \sum^{t\_{P + Q}}\_{t = t\_P + 1} \vert Y\_t - \hat{Y}\_t \vert,
$$&lt;p&gt;$\Theta$ 表示可学习的参数。&lt;/p&gt;
&lt;h1 id="experiments"&gt;Experiments
&lt;/h1&gt;&lt;h2 id="datasets"&gt;Datasets
&lt;/h2&gt;&lt;p&gt;我们在两个不同规模的交通预测数据集上衡量了模型的效果：（1）厦门数据集，流量预测，包含 95 个传感器从 2015 年 8 月 1 日到 12 月 31 日 5 个月的数据；（2）PeMS 数据集上速度预测，包含 325 个传感器 6 个月的数据。检测器的分布如图 7.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/gman-a-graph-multi-attention-network-for-traffic-prediction/Fig7.png"
loading="lazy"
alt="Figure7"
&gt;&lt;/p&gt;
&lt;h2 id="data-preprocessing"&gt;Data Preprocessing
&lt;/h2&gt;&lt;p&gt;一个时间步表示 5 min，使用 Z-Score 归一化，70% 用于训练，10% 验证，20% 测试。我们计算路网上传感器之间的距离，使用如下的路网构建方法：&lt;/p&gt;
$$\tag{17}
\mathcal{A}\_{v\_i,v\_j} = \begin{cases}
\text{exp}(- \frac{d^2\_{v\_i,v\_j}}{\sigma^2}), &amp; \text{if} \ \text{exp}(-\frac{d^2\_{v\_i,v\_j}}{\sigma^2}) \geq \epsilon\\
0, &amp; \text{otherwise}
\end{cases}
$$&lt;p&gt;$\epsilon$ 设定为 0.1。&lt;/p&gt;
&lt;h2 id="experimental-settings"&gt;Experimental Settings
&lt;/h2&gt;&lt;p&gt;指标：MAE, RMSE, MAPE。&lt;/p&gt;
&lt;p&gt;超参数就不描述了。&lt;/p&gt;
&lt;p&gt;Baselines都是近几年的方法。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/gman-a-graph-multi-attention-network-for-traffic-prediction/Table1.png"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;
&lt;p&gt;这里值得一提的是，最后一个训练和预测时间的比较，我个人认为脱离了框架或软件，单单比较每轮训练时长是毫无意义的，因为有些静态图框架它就是很快，动态图的就是慢，而且代码质量也有区别，有的代码质量高，自然就很快，代码质量低的就很慢。拿 Graph WaveNet 举例，他们公开的代码是 pytorch 的，而且他们在 inference 的时候要对 ground truth 进行反归一化，有的代码人家就不反归一化，这也会造成 inference 的时候有差别，且有的模型是随着结点数 $N$ 的增加模型有显著的耗时增加的现象，没有考虑这些就写 computation time 的比较我觉得没有什么用，何况以 AAAI 7 页的限制来说，完全说清楚这些也毫无意义。&lt;/p&gt;</description></item><item><title>MXNet 与 cuda 版本兼容的问题</title><link>https://davidham3.github.io/blog/p/mxnet-%E4%B8%8E-cuda-%E7%89%88%E6%9C%AC%E5%85%BC%E5%AE%B9%E7%9A%84%E9%97%AE%E9%A2%98/</link><pubDate>Fri, 02 Aug 2019 20:22:10 +0000</pubDate><guid>https://davidham3.github.io/blog/p/mxnet-%E4%B8%8E-cuda-%E7%89%88%E6%9C%AC%E5%85%BC%E5%AE%B9%E7%9A%84%E9%97%AE%E9%A2%98/</guid><description>&lt;p&gt;最近在做实验的时候发现了一个非常神奇的问题，搞得我一度很郁闷。我在 kaggle 上面写了个 mxnet symbolic 的程序，在测试集上效果不错，论文都写完了，结果拿回实验室的 GPU 上一跑，发现结果复现不了了，差了两个点。但我所有的实验都做了 10 次，如果说 1 次实验效果好还可以说是巧合，但这是 10 次实验啊。&lt;/p&gt;
&lt;p&gt;尝试找了一下问题在哪里，首先是 GPU 型号，kaggle 上面提供的是 Tesla P100，非常强劲的 GPU，16G 的显存，而且好像还支持半精度浮点运算。我在实验室使用了 RTX 2080 跑实验。在 4 台 RTX 2080 上面搭建了 OpenPAI，微软的一个开源深度学习资源调度平台。&lt;/p&gt;
&lt;p&gt;我在 kaggle 上跑 10 次，测试集指标是 18.039，做了 10 次实验取的平均值，方差是 0.075，非常稳定，也就是对于随机性不敏感，所以不需要指定随机种子什么的，我也不爱指定随机种子，因为我觉得好的模型就应该对随机性不敏感。&lt;/p&gt;
&lt;p&gt;为了验证是哪里出了问题，我打印了 kaggle 的环境配置，kaggle 使用的 mxnet_cu100 1.5.0，numpy 1.16.4。&lt;/p&gt;
&lt;p&gt;我用 Docker 构建了 4 个镜像：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;softwares&lt;/th&gt;
&lt;th&gt;cuda 100&lt;/th&gt;
&lt;th&gt;cuda 101&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;mxnet 1.41&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;mx1.41_cu100&lt;/td&gt;
&lt;td&gt;mx1.41_cu101&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;mxnet 1.50&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;mx1.50_cu100&lt;/td&gt;
&lt;td&gt;mx1.50_cu101&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;在安装的时候没有安装mkl。&lt;/p&gt;
&lt;p&gt;每个镜像跑同一个实验 3 次吧，最近没什么时间跑 10 次，结果等我跑完了再更新。&lt;/p&gt;
&lt;p&gt;2019年8月9日更新：&lt;/p&gt;
&lt;p&gt;跑完了，发现结果全都一样，和显卡，cuda，mxnet 版本都无关。。。&lt;/p&gt;
&lt;p&gt;后来找了一下问题，问题出在 training set 的 dataloader，忘了给 training set shuffle 了，所以效果变差了。。。&lt;/p&gt;</description></item><item><title>STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting</title><link>https://davidham3.github.io/blog/p/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/</link><pubDate>Fri, 12 Jul 2019 19:57:39 +0000</pubDate><guid>https://davidham3.github.io/blog/p/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/</guid><description>&lt;p&gt;IJCAI 2019. 原文链接：&lt;a class="link" href="https://arxiv.org/abs/1905.10069.pdf" target="_blank" rel="noopener"
&gt;STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger
Demand Forecasting&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;多步乘客需求预测对于按需车辆共享服务来说是个重要的任务。然而，预测多个时刻的乘客需求由于时空依赖的非线性和动态性很有挑战。我们提出了基于图的城市范围的旅客需求预测模型，使用一个层次图卷积同时捕获空间和时间关联性。我们的模型有三部分：1) 长期编码器对历史旅客需求编码；2) 短期编码器推导下一步预测结果来生成多步预测；3) 使用一个基于注意力的输出模块对动态的时间和各通道信息建模。实验在三个数据集上表明我们的方法比很多方法好。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1. Introduction
&lt;/h1&gt;&lt;h1 id="2-notations-and-problem-statement"&gt;2. Notations and Problem Statement
&lt;/h1&gt;&lt;p&gt;假设一个城市分成 $N$ 个小的区域，不考虑是分成网格还是路网。我们将区域的集合表示为 $\lbrace r_1, r_2, \dots, r_i, \dots r_N \rbrace$。在每个时间步 $t$，一个二维矩阵 $\boldsymbol{D_t} \in \mathbb{R}^{N \times d_{in}}$ 表示所有区域在时间 $t$ 的旅客需求。另一个向量 $\boldsymbol{E_t} \in \mathbb{R}^{d_e}$ 表示时间步 $t$ 的时间特征，包含了几点、星期几以及节假日的信息。&lt;/p&gt;
&lt;p&gt;给定城市范围的历史旅客需求序列 $\lbrace \bm{D_0}, \bm{D_1}, \dots, \bm{D_t} \rbrace$ 和时间特征 $\lbrace \bm{E_0}, \bm{E_1}, \dots, \bm{E_{t+\tau}} \rbrace$，目标是学习一个预测函数 $\Gamma(\cdot)$ 来预测接下来的 $\tau$ 个时间步上城市范围的旅客需求序列。我们只使用历史 $h$ 个时间步的需求序列作为输入 $\lbrace \bm{D_{t-h+1}, \bm{D_{t-h+2}}, \dots, \bm{D_t}} \rbrace$。我们的任务描述为：&lt;/p&gt;
$$\tag{1}
(\bm{D\_{t+1}}, \bm{D\_{t+2}}, \dots, \bm{D\_{t+\tau}}) = \Gamma(\bm{D\_{t-h+1}}, \bm{D\_{t-h+2}}, \dots, \bm{D\_t}; \bm{E\_0}, \bm{E\_1}, \dots, \bm{E\_{t+\tau}})
$$&lt;h1 id="3-methodology"&gt;3. Methodology
&lt;/h1&gt;&lt;p&gt;STG2Seq 的架构有三个组件：1. 长期编码器，2. 短期编码器，3.基于注意力的输出模块。长期和短期编码器由多个序列时空门控图卷积模块 (GGCM) 组成，通过在时间维度使用 GCN 可以同时捕获时间和空间相关性。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/Fig2.JPG"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;h2 id="31-passenger-demand-on-graph"&gt;3.1 Passenger Demand on Graph
&lt;/h2&gt;&lt;p&gt;我们先介绍如何将城市范围的旅客需求在图上描述出来。之前的工作假设一个区域的旅客需求会被近邻的区域影响。然而，我们认为空间关系并不是仅依赖空间位置。如果遥远的区域和当前区域有相似的地方，比如具有相似的 POI，那么也可能拥有相同的旅客需求模式。因此，我们将城市看作一个图 $G = (v, \xi, A)$，$v$ 是区域的集合 $v = \lbrace r_i \mid i=1,2,\dots,N \rbrace$，$\xi$ 表示边的集合，$A$ 是邻接矩阵。我们根据区域间旅客需求模式的相似性定义图的边。&lt;/p&gt;
$$\tag{2}
A\_{i, j} = \begin{cases}
1, \quad \text{if} \quad Similarity\_{r\_i, r\_j} &gt; \epsilon\\
0, \quad \text{otherwise}
\end{cases}
$$&lt;p&gt;其中 $\epsilon$ 是阈值，控制 $A$ 的稀疏程度。为了定量区域间的旅客需求模式的相似性，我们使用皮尔逊相关系数。$D_{0\text{\textasciitilde}t}(r_i)$ 表示时间从 0 到 $t$ 的区域 $r_i$ 历史旅客需求序列。$r_i$ 和 $r_j$ 之间的相似度可以定义为：&lt;/p&gt;
$$\tag{3}
Similarity\_{r\_i, r\_j} = Pearson(D\_{0\text{\textasciitilde}t}(r\_i), D\_{0\text{\textasciitilde}t}(r\_j))
$$&lt;h2 id="32-long-term-and-short-term-encoders"&gt;3.2 Long-term and Short-term Encoders
&lt;/h2&gt;&lt;p&gt;很多之前的工作只考虑下一步预测，即预测下一时间步的旅客需求。在训练过程中通过减少下一时间步预测值的误差而不考虑后续时间步的误差来优化模型。因此，这些方法在多步预测的问题上会退化。仅有一些工作考虑了多步预测的问题 [Xingjian et al., 2015; Li et al., 2018]。这些工作采用了基于 RNN 的编码解码器的架构，或是它的变体，比如 ConvLSTM 这样的作为编码解码器。这些方法有两个劣势：1. 链状结构的 RNN 在编码的时候需要遍历输入的时间步。因此他们需要与输入序列等长的 RNN 单元个数（序列多长，RNN单元就有多少个）。在目标需求和前一个需求上的长距离计算会导致一些信息的遗忘。2. 在解码部分，为了预测时间步 $T$ 的需求，RNN 将隐藏状态和前一时间步 $T-1$ 作为输入。因此，前一时间步带来的误差会直接影响到预测，导致未来时间步误差的累积。&lt;/p&gt;
&lt;p&gt;不同于之前所有的工作，我们引入了一个依赖于同时使用长期和短期编码器的架构，不用 RNN 做多步预测。长期编码器取最近的 $h$ 个时间步的城市历史旅客需求序列 $\lbrace \bm{D_{t-h+1}}, \bm{D_{t-h+2}}, \dots, \bm{D_t} \rbrace$ 作为输入来学习历史的时空模式。这 $h$ 步需求合并后组织成三维矩阵，$h \times N \times d_{in}$。长期编码器由一些 GGCM 组成，每个 GCGGM 捕获在所有的 $N$ 个区域上捕获空间关联性，在 $k$ 个时间步上捕获时间关联性。$k$ 是超参数，我们会在 3.3 节讨论。因此，只需要 $\frac{h-1}{k-1}$ 个迭代的步数就可以捕获 $h$ 个时间步上的时间关联性。对比 RNN 结构，我们的基于 GGCM 的长期编码器显著的降低了遍历长度，进一步减少了信息的损失。长期编码器的输出 $Y_h$ 的维数是 $h \times N \times d_{out}$，是输入的编码表示。&lt;/p&gt;
&lt;p&gt;短期编码器用来集成已经预测的需求，用于多步预测。它使用一个长度为 $q$ 的滑动窗来捕获近期的时空关联性。当预测在 $T(T \in [t+1,t+\tau])$ 步的旅客需求时，它取最近的 $q$ 个时间步的旅客需求，即 $\lbrace \bm{D_{T-q}}, \bm{D_{T-q+1}}, \dots, \bm{D_{T-1}} \rbrace$ 作为输入。除了时间步的长度以外，短期编码器和长期编码器一样。短期编码器生成一个维数为 $q \times N \times d_{out}$ 的矩阵 $Y^T_q$ 作为近期趋势表示。和基于 RNN 的解码器不同的是，RNN的解码器只将最后一个时间步的预测结果输入回去。因此，预测误差会被长期编码器小柔，减轻基于 RNN 的解码器会导致误差累积的问题。&lt;/p&gt;
&lt;h2 id="33-gated-graph-convolutional-module"&gt;3.3 Gated Graph Convolutional Module
&lt;/h2&gt;&lt;p&gt;门控图卷积模块是长期编码器和短期编码器的核心。每个 GGCM 由几个 GCN 层组成，沿着时间轴并行。为了捕获时空关联性，每个 GCN 在一定长度的时间窗内操作($k$)。它可以提取 $k$ 个时间步内所有区域的空间关联性。通过堆叠多个 GGCM，我们的模型形成了一个层次结构，可以捕获整个输入的时空关联性。图 3 展示了只使用 GCN 捕获时空关联性，为了简化我们忽略了通道维。Yu et al., 2018 的工作和我们的 GGCM 模块很像。他们的工作首先使用 CNN 捕获时间关联性，然后使用 GCN 捕获空间关联性。我们的方法对比他们的方法极大的简化了，因为我们可以同时捕获时空关联性。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/Fig3.JPG"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/Fig4.JPG"
loading="lazy"
alt="Figure4"
&gt;&lt;/p&gt;
&lt;p&gt;GGCM 模块的详细设计如图 4。第 $l$ 个 GGCM 的输入是一个矩阵，维数为 $h \times N \times C^l$。在第一个 GGCM 模块，$C^l$ 是 $d_{in}$ 维的。第 $l$ 个 GGCM 的输出是 $h \times N \times C^{l+1}$。我们先拼接一个 zero padding，维数为 $(k-1) \times N \times C^l$，得到新的输入 $(h+k-1) \times N \times C^l$，确保变换不会减少序列的长度。接下来，GGCM 中的每个 GCN 取 $k$ 个时间步的数据 $k \times N \times C^l$ 作为输入来提取时空关联性，然后 reshape 成一个二维矩阵 $N \times (k \cdot C^l)$。根据 Kipf &amp;amp; Welling 的 GCN，GCN 层可以描述如下：&lt;/p&gt;
$$\tag{4}
X^{l+1} = (\tilde{P}^{-\frac{1}{2}} \tilde{A} \tilde{P}^{-\frac{1}{2}}) X^l W
$$&lt;p&gt;$\tilde{A} = A + I_n$，$\tilde{P}_{ii} = \sum_j \tilde{A}_{ij}$，$X \in \mathbb{R}^{N \times (k \cdot C^l)}$，$W \in \mathbb{R}^{(k \cdot C^l) \times C^{l+1}}$，$X^{l+1} \in \mathbb{R}^{N \times C^{l+1}}$
。&lt;/p&gt;
&lt;p&gt;除此以外，我们使用了门控机制对旅客需求预测的复杂非线性建模。式 4 重新描述如下：&lt;/p&gt;
$$\tag{5}
X^{l+1} = ((\tilde{P}^{-\frac{1}{2}} \tilde{A} \tilde{P}^{-\frac{1}{2}}) X^l W\_1 + X^l) \otimes \sigma((\tilde{P}^{-\frac{1}{2}} \tilde{A} \tilde{P}^{-\frac{1}{2}}) X^l W\_2)
$$&lt;p&gt;$\otimes$ 是对应元素相乘，$\sigma$是 sigmoid 激活函数。因此输出是一个非线性门 $\sigma((\tilde{P}^{-\frac{1}{2}} \tilde{A} \tilde{P}^{-\frac{1}{2}}) X^l W_2)$ 控制的线性变换 $((\tilde{P}^{-\frac{1}{2}} \tilde{A} \tilde{P}^{-\frac{1}{2}}) X^l W_1 + X^l)$。非线性门控制线性变换的哪个部分可以通过门影响预测。此外，我们使用残差连接来避免式 5 中的网络退化。&lt;/p&gt;
&lt;p&gt;最后，门控机制产生的 $h$ 个输出沿时间轴合并，生成 GGCM 模块的输出 $h \times N \times C^{l+1}$。&lt;/p&gt;
&lt;h2 id="34-attention-based-output-module"&gt;3.4 Attention-based Output Module
&lt;/h2&gt;&lt;p&gt;如 3.2 描述的那样，长期时空依赖和 $T$ 时间步的近期时空依赖通过两个矩阵描述 $Y_h$ 和 $Y^T_q$。我们拼接。我们拼接他们形成联合表示 $Y_{h+q} \in \mathbb{R}^{(h+q) \times N \times d_{out}}$，通过一个基于注意力机制的模块解码获得预测值。这里为了简便忽略 $T$。$Y_{h+q}$ 的三个轴分别是时间、空间、通道。&lt;/p&gt;
&lt;p&gt;我们先引入一个时间注意力机制来解码 $Y_{h+q}$。旅客需求是一个典型的时间序列，前一时刻的需求对后一时刻有影响。然而，之前的每一步对预测目标的影响是不同的，影响随时间变化。我们设计了一个时间注意力机制对每个历史时间步增加注意力分数衡量其影响。分数通过 $Y_{h+q} = [y_1, y_2, \dots, y_{h+q}](y_i \in \mathbb{R}^{N \times d_{out}})$ 和目标时间步的时间特征 $\bm{E}_T$ 生成，这个分数可以自适应地学习之前的时间步随时间的动态影响。我们定义时间注意力分数如下：&lt;/p&gt;
$$\tag{6}
\bm{\alpha} = softmax(tanh(Y\_{h+q} W^Y\_3 + E\_T W^E\_4 + b\_1))
$$&lt;p&gt;$W^Y_3 \in \mathbb{R}^{(h+q) \times (N \times d_{out}) \times 1}$，$W^E_4 \in \mathbb{R}^{d_e \times (h+q)}$，$b_1 \in \mathbb{R}^{(h+q)}$。联合表示 $Y_{h+q}$ 通过注意力分数 $\bm{\alpha}$ 转换：&lt;/p&gt;
$$\tag{7}
Y\_{\alpha} = \sum^{h+q}\_{i=1} \alpha^i y\_i \quad \in \mathbb{R}^{N \times d\_{out}}
$$&lt;p&gt;受到 [Chen et al., 2017] 的启发，每个通道的重要性是不同的，我们在时间注意力后面加了一个通道注意力模块来找到 $Y_\alpha = [y_1, y_2, \dots, y_{d_{out}}]$ 中最重要的那个。计算如下：&lt;/p&gt;
$$\tag{8}
\bm\beta = softmax(tanh(Y\_\alpha W^Y\_5 + E\_T W^E\_6 + b\_2))
$$$$\tag{9}
Y\_{\beta} = \sum^{d\_{out}}\_{i=1} \beta^i y\_i \quad \mathbb{R}^N
$$&lt;p&gt;其中，$W^Y_5 \in \mathbb{R}^{d_{out} \times N \times 1}$，$W^E_6 \in \mathbb{R}^{d_e \times d_{out}}$；$\bm\beta \in \mathbb{R}^{d_{out}}$ 是每个通道的注意力分数。当预测的维度是1时，$Y_\beta$ 就是我们预测的旅客需求 $\bm{D&amp;rsquo;_T}$。当预测维度是 2 时（预测起止需求），我们给每个通道计算注意力分数，将他们拼接起来得到最后的预测值。&lt;/p&gt;</description></item><item><title>Self-Attention Graph Pooling</title><link>https://davidham3.github.io/blog/p/self-attention-graph-pooling/</link><pubDate>Tue, 25 Jun 2019 16:34:02 +0000</pubDate><guid>https://davidham3.github.io/blog/p/self-attention-graph-pooling/</guid><description>&lt;p&gt;ICML 2019，原文地址：&lt;a class="link" href="https://arxiv.org/abs/1904.08082" target="_blank" rel="noopener"
&gt;Self-Attention Graph Pooling&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;这些年有一些先进的方法将深度学习应用到了图数据上。研究专注于将卷积神经网络推广到图数据上，包括重新定义图上的卷积和下采样（池化）。推广的卷积方法已经被证明有性能提升且被广泛使用。但是，下采样的方法仍然是一个难题且有提升空间。我们提出了一个基于自注意力的图的池化方法。使用图卷积的自注意力使得我们的池化方法可以同时考虑顶点特征和图的拓扑结构。为了确保一个公平的对比，我们使用了相同的训练步骤和模型架构。实验结果显示我们的方法有更高的分类精度。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1. Introduction
&lt;/h1&gt;&lt;p&gt;CNN 成功利用了图像、语音、视频数据中的欧氏空间（网格结构）。CNN 由卷积层和下采样层（池化层）组成。卷积层和池化层挖掘了网格数据的平移不变性和compositionality（这个我不知道是什么。。。）。结果是，CNN 用少量的参数就可以表现的很好。&lt;/p&gt;
&lt;p&gt;然而，很多数据是非欧空间上的。社交网络、生物蛋白质网络、分子网络可以表示成网络。将 CNN 应用在非欧空间上的尝试已经获得了成功。很多研究重新定义了图上的卷积和池化。&lt;/p&gt;
&lt;p&gt;对于图卷积的池化操作现在比较少。之前图的池化的研究只考虑图的拓扑结构 (Defferrard et al., 2016; Rhee et al., 2018)。一些方法利用了结点的特征获得一个小图的表示。最近，Ying et al.; Gao &amp;amp; Ji; Cangea et al. 提出了创新的池化方法，可以层级的表示图。这些方法使得图神经网络可以通过端到端的形式，在池化后获得尺寸缩减的图。&lt;/p&gt;
&lt;p&gt;然而，上述的池化方法仍有提升空间。举个例子，Ying et al. 的可微层级池化方法有平方级别的空间复杂度，参数依赖于顶点数。Gao &amp;amp; Ji; Cangea et al. 解决了复杂度的问题，但是没有考虑图的拓扑结构。&lt;/p&gt;
&lt;p&gt;我们提出的 SAGPool 是一个层次的自注意力图池化方法。我们的方法可以通过端到端的方式使用相对较少的参数学习到层次表示。自注意力机制用来区分结点是否丢弃掉还是保留。由于自注意力机制使用图卷积计算注意力分数，结点特征和图的拓扑结构可以被考虑其中。一句话，SAGPool 有前面方法的优点，是第一个使用自注意力用于池化的方法，并且获得了很好的性能。代码已经在 Github 上开源了。&lt;/p&gt;
&lt;h1 id="2-related-work"&gt;2. Related Work
&lt;/h1&gt;&lt;h2 id="21-graph-convolution"&gt;2.1. Graph Convolution
&lt;/h2&gt;&lt;p&gt;图上的卷积要么是基于谱的，要么是非谱的。谱方法专注于在傅里叶域上定义卷积，利用使用图拉普拉斯矩阵的谱滤波器。Kipf &amp;amp; Welling 提出了一个层级传播的规则，简化了使用切比雪夫展开来趋近拉普拉斯矩阵的方法。非谱方法的目标是定义一个卷积操作，可以直接应用在图上。通常来说，非谱方法，中心结点在特征传入下层之前聚合邻接结点的特征。Hamilton et al. 提出了 GraphSAGE，通过采样和聚合学习结点的嵌入。尽管 GraphSAGE 会采样固定数量的邻居，GAT 基于注意力机制，在所有的邻居上计算结点表示。两个方法在图相关的任务上都有提升。&lt;/p&gt;
&lt;h2 id="22-graph-pooling"&gt;2.2. Graph Pooling
&lt;/h2&gt;&lt;p&gt;池化层通过缩减表示的大小，使得 CNN 能减少参数的数量，因此能避免过拟合。为了泛化 CNN，GNN 上的池化是必要的。图的池化方法可以归入三类：基于拓扑的，基于全局的，基于层次的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Topology based pooling&lt;/strong&gt; 早期的工作使用图的缩减算法，而不是神经网络。谱聚类算法使用特征值分解获得缩减的图。然而，特征值分解的时间复杂度高。Graclus (Dhillon et al., 2007) 不使用特征向量计算给定图的聚类结果，而是通过一个谱聚类的目标函数与一个带权的核 k-means 目标函数的等价性。即便在最近的 GNN 模型中，Graclus 也被使用作为一个池化单元。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Global pooling&lt;/strong&gt; 不像之前的方法，全局池化方法考虑图的特征。全局池化方法在每层聚合表示的时候使用加和的方式而不是用神经网络。这个方法可以处理不同结构的图，因为它获得了所有的表示。Gilmer et al. 将 GNN 看作是一种信息的传递规则，提出了一个通用框架用于图分类，整个图的表示可以通过使用 Set2Set (Vinyals et al., 2015) 来获得。SortPool (Zhang et al., 2018b) 根据一个图的结构角色对结点嵌入排序，将排序后的表示传入下一层。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hierarchical pooling&lt;/strong&gt; 全局池化方法不学习层次表示，但是对于捕获图的结构信息来说，层次表示很关键。层次池化的动机在于在每层构建一个模型，这个模型可以学习基于特征的或基于拓扑的顶点分配。Ying et al. 提出了 DiffPool，这是一种可微的图的池化方法，可以以端到端的形式学习分配矩阵。在层 $l$ 学习到的分配矩阵 $S^{(l)} \in \mathbb{R}^{n_l \times n_{l+1}}$ 包含了层 $l$ 中的结点在 $l + 1$ 层被分配到类簇的概率。$n_l$ 表示层 $l$ 的结点数。结点通过下式来分配：&lt;/p&gt;
$$\tag{1}
S^{(l)} = \text{softmax}(\text{GNN}\_l (A^{(l)}, X^{(l)})) \\
A^{(l+1)} = S^{(l)\text{T}} A^{(l)} S^{(l)}
$$&lt;p&gt;$X$ 表示矩阵的结点特征，$A$ 是邻接矩阵。&lt;/p&gt;
&lt;p&gt;Cangea et al. 使用 gPool (Gao &amp;amp; Ji, 2019) 获得了和 DiffPool 相当的性能。gPool 需要 $\mathcal{O}(\vert V \vert + \vert E \vert)$ 的空间复杂度，DiffPool 需要 $\mathcal{O}(k \vert V \vert^2)$ 的空间复杂度。$V$，
$E$，$k$ 分别表示顶点、边、池化比例。gPool 使用一个可学习的向量 $p$ 计算投影分数，然后使用这个分数选择最高的结点。投影分数通过 $p$ 和所有结点的特征向量的内积获得。分数表示结点可以获得的信息量。下面的式子大体的描述了 gPool 中的池化步骤：&lt;/p&gt;
$$\tag{2}
y = X^{(l)} \mathbf{p}^{(l)} / \Vert \mathbf{p}^{(l)} \Vert, \text{idx=top-rank}(y, \lceil kN \rceil) \\
A^{(l+1)} = A^{(l)}\_{\text{idx,idx}}
$$&lt;p&gt;如式 2，图的拓扑结构不影响投影分数。&lt;/p&gt;
&lt;p&gt;为了进一步提高图的池化，我们提出了 SAGPool，可以在可观的时间和空间复杂度上利用特征和拓扑结构生成层次表示。&lt;/p&gt;
&lt;h1 id="3-proposed-method"&gt;3. Proposed Method
&lt;/h1&gt;&lt;p&gt;SAGPool 的关键是它使用了 GNN 得到的注意力分数。SAGPool 层和模型架构分别是图 1 和图 2.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/self-attention-graph-pooling/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/self-attention-graph-pooling/Fig2.JPG"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;h2 id="31-self-attention-graph-pooling"&gt;3.1 Self-Attention Graph Pooling
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Self-attention mask&lt;/strong&gt; 注意力机制广泛应用在最近的深度学习研究中。这样的机制使得模型可以更专注于重要的特征，不那么关注不重要的特征。自注意力一般称为内注意力，允许输入特征作为自身注意力的标准 (Vaswani et al., 2017)。我们使用图卷积获得自注意力分数。举个例子，如果图卷积的公式是 Kipf &amp;amp; Welling 使用的，那么自注意力分数 $Z \in \mathbb{R}^{N \times 1}$ 通过下式计算：&lt;/p&gt;
$$\tag{3}
Z = \sigma(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} X \Theta\_{att})
$$&lt;p&gt;$\sigma$ 是激活函数，如 $tanh$，$\tilde{A} \in \mathbb{R}^{N \times N}$ 是有自连接的邻接矩阵，$\tilde{D} \in \mathbb{R}^{N \times N}$ 是度矩阵，$X \in \mathbb{R}^{N \times F}$ 是图的特征矩阵，$\Theta_{att} \in \mathbb{R}^{F \times 1}$ 是 SAGPool 层仅有的参数。通过利用图卷积获得自注意力分数，池化的结果是同时基于图的特征和拓扑结构的。我们利用 Gao &amp;amp; Ji; Cangea et al. 的结点选择方法，保留了输入的图的一部分结点，甚至当图的尺寸和结构改变时。池化比例 $k \in (0, 1]$ 是一个超参数决定了保留多少结点。基于 $Z$ 的值选择最高的 $\lceil kN \rceil$ 个结点。&lt;/p&gt;
$$\tag{4}
\text{idx = top-rank}(Z, \lceil kN \rceil), Z\_{mask} = Z\_{\text{idx}}
$$&lt;p&gt;$\text{top-rank}$ 返回最高的 $\lceil kN \rceil$ 个值的下标，$\cdot_{\text{idx}}$ 是下标操作，$Z_{mask}$ 是特征的注意力 mask。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Graph pooling&lt;/strong&gt; 输入的图通过图 1 中的 &lt;strong&gt;masking&lt;/strong&gt; 操作。&lt;/p&gt;
$$\tag{5}
X' = X\_{idx,:}, X\_{out} = X' \odot Z\_{mask}, A\_{out} = A\_{\text{idx, idx}}
$$&lt;p&gt;其中 $X_{\text{idx,:}}$ 是指定行下标的特征矩阵，每行表示一个结点，$\odot$ 是 elementwise 乘积，$A_{\text{idx, idx}}$ 是指定行下标和列下标的邻接矩阵。$X_{out}$ 和 $A_{out}$ 是新的特征矩阵和对应的邻接矩阵。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Variation of SAGPool&lt;/strong&gt; 使用图卷积的主要原因是为了反映图的特征和拓扑结构。可以使用不同的图卷积来替换式 3 中的图卷积。计算注意力机制 $Z \in \mathbb{R}^{N \times 1}$ 的泛化公式如下：&lt;/p&gt;
$$\tag{6}
Z = \sigma(\text{GNN}(X, A))
$$&lt;p&gt;$X$ 和 $A$ 是特征矩阵和邻接矩阵。&lt;/p&gt;
&lt;p&gt;除了使用邻接结点还可以使用多跳结点来计算注意力分数。式 7 和式 8 分别使用了两跳连接和堆叠 GNN 层。增加邻接矩阵的平方增加了两条邻居：&lt;/p&gt;
$$\tag{7}
Z = \sigma(\text{GNN}(X, A + A^2))
$$&lt;p&gt;堆叠 GNN 层可以间接的聚合两跳结点。这样的话，非线性层和参数的数量就增加了：&lt;/p&gt;
$$\tag{8}
Z = \sigma(\text{GNN}\_2 (\sigma(\text{GNN}\_1 (X, A)), A))
$$&lt;p&gt;式 7 和式 8 可以利用多跳连接。&lt;/p&gt;
&lt;p&gt;另一个变体是平均多个注意力分数。平均注意力分数通过 $M$ 个 GNN 获得：&lt;/p&gt;
$$\tag{9}
Z = \frac{1}{m} \sum\_m \sigma(\text{GNN}\_m (X, A))
$$&lt;p&gt;在论文中，式 7，8，9 的模型分别记为 $\rm {SAGPool}_{augmentation}$，$\rm {SAGPool}_{serial}$，$\rm {SAGPool}_{parallel}$。&lt;/p&gt;
&lt;h2 id="32-model-architecture"&gt;3.2 Model Architecture
&lt;/h2&gt;&lt;p&gt;根据 Lipton &amp;amp; Steinhardt 的研究，如果对一个模型做很多修改，那很难知道是哪部分改进起的作用。为了一个公平的对比，我们使用了 Zhang et al. 和 Cangea et al. 的模型来对比我们的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Convolution layer&lt;/strong&gt; 如 2.1 节提到的，有很多图卷积的定义。其他类型的图卷积可能也能提升性能，但是我们利用的是 Kipf &amp;amp; Welling 提出的广泛使用的图卷积。式 10 和式3 一样，除了 $\Theta$ 的维度：&lt;/p&gt;
$$\tag{10}
h^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} h^{(l)} \Theta)
$$&lt;p&gt;其中 $h^{(l)}$ 是第 $l$ 层的节点表示，$\Theta \in \mathbb{R}^{F \times F&amp;rsquo;}$ 是卷积核。使用 ReLU 作为激活函数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Readout layer&lt;/strong&gt; 受 JK-net 的启发，Cangea et al. 提出了一个 readout 层，聚合结点的特征生成一个固定大小的表示。readout 层的聚合特征如下：&lt;/p&gt;
$$\tag{11}
s = \frac{1}{N} \sum^N\_{i=1} x\_i \mid \mid \mathop{max}\limits^N\_{i=1} x\_i
$$&lt;p&gt;$N$ 是结点数，$x_i$ 是第 $i$ 个结点的特征向量，$\mid \mid$ 表示拼接。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Global pooling architecture&lt;/strong&gt; 我们实现了 Zhang et al. 提出的全局池化结构。如图 2 所示，全局池化结构由三层图卷积层组成，每层的输出拼接在一起。结点特征在 readout 层聚合，然后接一个池化层。图的特征表示传入线性层用来分类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hierarchical pooling architecture&lt;/strong&gt; 在这部分设置中，我们实现了 Cangea et al. 的层次池化结构。如图 2 所示，结构包含了三个块，每个块由一个卷积层和一个池化层组成。每个块的输出通过一个 readout 层聚合。每个 readout 层的输出之和放入线性层做分类。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/self-attention-graph-pooling/Table1.JPG"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;
&lt;h1 id="4-experiments"&gt;4. Experiments
&lt;/h1&gt;&lt;p&gt;我们在图分类上评估了全局池化和层次池化。&lt;/p&gt;
&lt;h2 id="41-datasets"&gt;4.1. Datasets
&lt;/h2&gt;&lt;p&gt;5 个数据集。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/self-attention-graph-pooling/Table2.JPG"
loading="lazy"
alt="Table2"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/self-attention-graph-pooling/Table3.JPG"
loading="lazy"
alt="Table3"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/self-attention-graph-pooling/Table4.JPG"
loading="lazy"
alt="Table4"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/self-attention-graph-pooling/Fig3.JPG"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;</description></item><item><title>Session-based Social Recommendation via Dynamic Graph Attention Networks</title><link>https://davidham3.github.io/blog/p/session-based-social-recommendation-via-dynamic-graph-attention-networks/</link><pubDate>Wed, 29 May 2019 13:37:55 +0000</pubDate><guid>https://davidham3.github.io/blog/p/session-based-social-recommendation-via-dynamic-graph-attention-networks/</guid><description>&lt;p&gt;WSDM 2019，原文链接：&lt;a class="link" href="https://arxiv.org/abs/1902.09362" target="_blank" rel="noopener"
&gt;Session-based Social Recommendation via Dynamic Graph Attention Networks&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;像 Facebook 和 Twitter 这样的在线社区很流行，已经成为很多用户生活中的重要部分。通过这些平台，用户可以发掘并创建信息，其他人会消费这些信息。在这种环境下，给用户推荐相关信息变得很重要。然而，在线社区的推荐是一个难题：1. 用户兴趣是动态的，2. 用户会受其朋友的影响。此外，影响者与环境相依。不同的朋友可能关注不同的话题。对两者建模对推荐来说是重要的。&lt;/p&gt;
&lt;p&gt;我们提出了一个基于动态图注意力机制的在线社区推荐系统。我们用一个 RNN 对动态的用户行为建模，用图卷积对依赖环境的社交影响建模，可以动态地根据用户当前的兴趣推测影响者。整个模型可以高效地用于大规模的数据。几个真实数据集上的实验结果显示我们的方法很好，源码在：&lt;a class="link" href="https://github.com/DeepGraphLearning/RecommenderSystems" target="_blank" rel="noopener"
&gt;https://github.com/DeepGraphLearning/RecommenderSystems&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 Introduction
&lt;/h1&gt;&lt;p&gt;在线社区已经成为今天在线体验的重要组成部分。Facebook, Twitter, 豆瓣可以让用户创建、分享、消费信息。因此这些平台的推荐系统对平台上的表层信息和维持用户活跃度来说很重要。然而，在线社区对推荐系统提出了一些挑战。&lt;/p&gt;
&lt;p&gt;首先，用户兴趣本质上来说是动态的。一个用户可能一段时间对体育感兴趣，之后呢对音乐感兴趣。其次，因为在线社区里面的用户经常给朋友分享信息，用户也会被他们的朋友影响。举个例子，一个找电影的用户可能会被她的朋友喜欢的电影影响。此外，施加影响的一方组成的集合是动态的，因为这和环境有关。举个例子，一个用户在找一个搞笑电影的时候会听取一群喜欢喜剧的朋友的意见，在找动作电影的时候，会受到另一组朋友的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivating Example.&lt;/strong&gt; 图 1 展示了 Alice 和 她的朋友在一个在线社区的行为。行为通过一个动作（比如点击操作）序列描述。为了捕获用户的动态兴趣，她们的行为被分成了不同的子序列，表示 &lt;em&gt;sessions&lt;/em&gt;。我们感兴趣的是基于 &lt;em&gt;session&lt;/em&gt; 的推荐：我们根据当前情境下 Alice 已经消费过的东西给她推荐下一个她可能消费的东西。图 1 展示出两个情景，a 和 b。此外，Alice 朋友们的消费信息也是可获得的。我们会利用这些信息生成更好的推荐。因此我们在一个基于 session 的社交推荐情景下。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/session-based-social-recommendation-via-dynamic-graph-attention-networks/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;在 session a 中，Alice 浏览了体育的物品。她的两个朋友：Bob 和 Eva，是出了名的体育粉丝（长期兴趣），他们最近正好浏览了体育相关的物品（短期兴趣）。考虑到这个情况，Alice 可能被他们两个影响，比如说接下来她可能会学习乒乓球。在 session b 中，Alice 对文学艺术物品感兴趣。这个环境和刚才不一样了因为她没有最近正在消费这样物品的朋友。但是 David 一直对这个话题感兴趣（长期兴趣）。在这种情况下，对 Alice 来说可能会被 David 影响，可能会被推荐一本 David 喜欢的书。这些例子表明了一个用户当前的兴趣是如何与他不同的朋友的兴趣相融合来提供基于情景的推荐的。我们提出了一个推荐模型来处理这两种情况。&lt;/p&gt;
&lt;p&gt;当前的推荐系统要么对用户的动态兴趣建模，要么对他们的社交影响建模，但是，据我们所知，现存的方法还没有融合过他们。最近的一个研究对 session 级别的用户行为使用 RNN 建模，忽略了社交影响。其他的研究仅考虑社交影响。举个例子，Ma et al. 探索了朋友的长期兴趣产生的社交影响。但是，不同用户的影响是静态的，没有描绘出每个用户当前的兴趣。&lt;/p&gt;
&lt;p&gt;我们提出了一个方法对用户基于 session 的兴趣和动态社交影响同时建模。也就是说，考虑了基于当前用户的 session，他的朋友的哪个子集影响了他。我们的推荐模型基于动态注意力网络。我们的方法先用一个 RNN 对一个 session 内的用户行为建模。根据用户当前兴趣——通过 RNN 的隐藏表示捕获到的——我们使用 GAT 捕获了他的朋友的影响。为了提供 session 级别的推荐，我们区分了短期兴趣和长期兴趣。在给定用户当前兴趣的基础上，每个朋友的影响通过注意力机制自动地决定。&lt;/p&gt;
&lt;p&gt;我们做了大量实验，效果比很多方法好。贡献如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提出了同时对动态用户兴趣和依赖环境的社交影响学习后对在线社区进行推荐的方法。&lt;/li&gt;
&lt;li&gt;提出了基于动态图注意力网络的推荐方法。在大数据集上也有效。&lt;/li&gt;
&lt;li&gt;实验结果比 state-of-the-art 好很多。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="2-related-work"&gt;2 Related Work
&lt;/h1&gt;&lt;p&gt;讨论三条路线：1. 对动态用户行为建模的推荐系统，2. 考虑社交影响的推荐系统，3. 图卷积网络。&lt;/p&gt;
&lt;h2 id="21-dynamic-recommendation"&gt;2.1 Dynamic Recommendation
&lt;/h2&gt;&lt;h2 id="22-social-recommendation"&gt;2.2 Social Recommendation
&lt;/h2&gt;&lt;h2 id="23-graph-convolutional-networks"&gt;2.3 Graph Convolutional Networks
&lt;/h2&gt;&lt;h1 id="3-problem-definition"&gt;3 Problem Definition
&lt;/h1&gt;&lt;p&gt;推荐系统根据历史行为推荐相关的物品。传统的推荐模型，如矩阵分解，忽略了用户的消费顺序。在线社区中，用户兴趣是快速变化的，必须要考虑用户偏好顺序，以便对用户的动态兴趣建模。实际上，因为用户全部的历史记录可以很长（有些社区存在好多年了），用户兴趣切换的很快，一个常用的方法是将用户的偏好分成不同的 session，（使用时间戳，以一个星期为时间段考虑每个用户的行为）并以 session 为级别提供推荐。定义如下：&lt;/p&gt;
&lt;p&gt;DEFINITION 1. (&lt;strong&gt;Session-based Recommendation&lt;/strong&gt;)，$U$ 表示用户的集合，$I$ 表示物品集。每个用户 $u$ 和一组带有时间步 $T$ 的 session 相关，$I^u_T = \lbrace \vec{S}^u_1, \vec{S}^u_2, \dots \vec{S}^u_T \rbrace$，其中 $\vec{S}^u_t$ 是用户 $u$ 的第 $t$ 个 session。在每个 session 内，$\vec{S}^u_t$ 由一个用户行为的序列 $\lbrace i^u_{t,1}, i^u_{t,2}, \dots, i^u_{t,N_{u,t}} \rbrace$ 组成，其中 $i^u_{t,p}$ 是在第 $t$ 个 session 中用户消费的第 $p$ 个物品，$N_{u,t}$ 是 session 中物品的总数。对于每个用户 $u$，给定一个 session $\vec{S}^u_{T+1} = \lbrace i^u_{T+1,1}, \dots i^u_{T+1,n} \rbrace$，基于 session 的推荐系统的目标是从 $I$ 中推荐一组用户可能在下来的 $n+1$ 步时感兴趣的物品，即 $i^u_{T+1, n+1}$。&lt;/p&gt;
&lt;p&gt;在在线社区中，用户的兴趣不仅与他们的历史行为相关，也受他们的朋友的影响。举个例子，一个朋友看电影，我也可能会感兴趣。这就叫社交影响。此外，从朋友那里来的影响是跟环境有关的。换句话说，从朋友那里来的影响是不一样的。如果一个用户想买个笔记本电脑，她可能更倾向于问问她喜欢高科技产品的朋友；如果她要买相机，她可能会被她的摄影师朋友影响。就像图 1，一个用户可能被她朋友的长期兴趣和短期兴趣影响。&lt;/p&gt;
&lt;p&gt;为了提供一个有效的推荐结果，我们提出对动态的用户兴趣和依赖于环境的社交影响建模。我们定义了如下的问题：&lt;/p&gt;
&lt;p&gt;DEFINITION 2. (&lt;strong&gt;Session-based Social Recommendation&lt;/strong&gt;) $U$ 表示用户集，$I$ 表示物品集合，$G=(U, E)$ 是社交网络，$E$ 是社交网络的边。给定用户 $u$ 的一个 session $\vec{S}^u_{T+1} = \lbrace i^u_{T+1,1}, \dots i^u_{T+1,n} \rbrace$，目标是利用她的动态兴趣（$\cup^{T+1}_{t=1} \vec{S}^u_t$）和社交影响（$\cup^{N(u)}_{k=1} \cup^T_{t=1} \vec{S}^k_t$，其中 $N(u)$ 是用户 $u$ 的邻居），从 $I$ 中推荐一组用户 $u$ 可能在下来的 $n+1$ 步时感兴趣的物品，即 $i^u_{T+1, n+1}$。&lt;/p&gt;
&lt;h1 id="4-dynamic-social-recommender-systems"&gt;4 Dynamic Social Recommender Systems
&lt;/h1&gt;&lt;p&gt;我们提出的模型 Dynamic Graph Recommendation (DGREC) 是个动态图注意力模型，可以对用户近期的偏好和他的朋友的偏好建模。&lt;/p&gt;
&lt;p&gt;DGREC 有 4 个模块（图 2）。首先，一个 RNN 对用户当前 session 中的物品序列建模。她朋友的偏好使用长期偏好和短期偏好融合来建模。短期偏好，或是最近一次 session 中的物品也使用 RNN 来编码。朋友的长期偏好通过一个独立的嵌入层编码。模型使用 GAT 融合当前用户的表示和她朋友的表示。这是我们模型的关键：我们提出了基于用户当前的兴趣学习每个朋友的权重的机制。最后一步，模型通过融合用户当前偏好和她的社交影响得到推荐结果。&lt;/p&gt;</description></item><item><title>DeepSTN+: Context-aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis</title><link>https://davidham3.github.io/blog/p/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/</link><pubDate>Wed, 08 May 2019 16:40:48 +0000</pubDate><guid>https://davidham3.github.io/blog/p/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/</guid><description>&lt;p&gt;AAAI 2019，网格流量预测，对比ST-ResNet，抛出三个问题，卷积捕获的空间范围小、人口流动和区域的功能相关、之前的融合机制不好。改了一下残差卷积，给 POI 信息增加了时间维度，多组件的信息提前融合，减少了参数，稳定模型训练。原文链接：&lt;a class="link" href="https://github.com/FIBLAB/DeepSTN/blob/master/docs/5624_AAAI19_DeepSTN%2B_Camera_Ready.pdf" target="_blank" rel="noopener"
&gt;DeepSTN+: Context-aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;人口流量预测在城市规划、交通管控中的很多应用中都很重要。目的是预测流入和流出流量。我们提出了 DeepSTN+，一个基于深度学习的卷积模型，预测超大城市的人口流量。首先，DeepSTN+ 使用 &lt;em&gt;ConvPlus&lt;/em&gt; 结构对大范围的空间依赖建模。此外，POI 分布和时间因素相融合来表达区域属性的影响，以此引入人口流动的先验知识。最后，我们提出了一个有效的融合机制来稳定训练过程，提升了结果。基于两个真实数据集的大量实验结果表明我们模型的先进性，和 state-of-the-art 比高了 8% ~ 13% 左右。&lt;/p&gt;
&lt;h1 id="introduction"&gt;Introduction
&lt;/h1&gt;&lt;p&gt;如图 1 所示，人口流量预测是在给定历史流量信息的前提下，预测城市内每个区域的流入和流出流量。最近，为了解决这个问题，基于深度学习的模型被相继提出，获得了很好的效果。Deep-ST 是第一个使用卷积网络捕获空间信息的模型。ST-ResNet 用卷积模块替换了卷积。通过融合金字塔型的 ConvGRU 模型和周期表示，Periodic-CRN 设计成了捕获人口流动周期性的模型。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;这些方法仍然不够有效且不精确：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;不能捕获区域间的空间依赖。&lt;/em&gt; 由于现代城市中高级的运输系统的存在，人们可以通过地铁或出租车在短时间内移动到很远的地方。因此，区域间的大范围空间依赖在人口移动中逐渐扮演重要的角色。现存的工作使用多层卷积网络来建模。然而，它们只能一步一步地捕获近邻的空间依赖，不能直接地捕获大范围的空间依赖。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;忽略了人口流动的区域功能的影响。&lt;/em&gt; 人口移动是发生在物理世界中的，会直接受到区域属性的影响。举个例子，人们通常早上从家出发到公司，晚上回来。显然，区域的功能（属性）包含了关于人类移动的先验知识。然而，现存的解决方案没有考虑过区域的属性。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;冗余以及不稳定的神经网络结构。&lt;/em&gt; ST-ResNet 利用了三个独立分支，每个分支都是残差卷积单元，用来处理不同的输入，在模型的结尾用一个线性操作融合三个输出。但是，最后的融合机制导致不同组件间的交互产生了缺陷，这个缺陷导致了网络内产生了无效的参数和不稳定的性质。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总结一下，模型应该考虑大范围的空间依赖，区域的影响，更有效的融合机制这三点因素。我们提出的 DeepSTN+ 解决了上述挑战。我们设计了一个 &lt;em&gt;ConvPlus&lt;/em&gt; 结构直接地捕获大范围空间依赖。&lt;em&gt;ConvPlus&lt;/em&gt; 放在残差单元前面作为一个全局特征提取器提取出区域间的全局特征。其次，我们设计了一个 &lt;em&gt;SemanticPlus&lt;/em&gt; 结构来学习人口在区域间移动的先验知识。用静态的 POI 分布作为输入，&lt;em&gt;SemanticPlus&lt;/em&gt; 利用时间因素给不同时间上不同的 POI 分配权重。最后，我们引入早融合和多尺度融合机制来减少训练参数，捕获不同级别特征间的复杂关系。这样，我们的系统可以对更复杂的空间关联性建模，获得更好的效果，我们的贡献有以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我们设计了一个新的残差单元，ResPlus 单元用来替换原始的残差单元。我们指出了典型的卷积模型不能有效地捕获大范围依赖。ResPlus 包含了一个 &lt;em&gt;ConvPlus&lt;/em&gt; 结构，可以捕获人流间的大范围空间依赖。&lt;/li&gt;
&lt;li&gt;我们设计了一个 &lt;em&gt;SemanticPlus&lt;/em&gt; 结构来建模不同区域的影响，学习人口流动的先验知识。我们在模型头部使用早融合机制，在结尾使用多尺度融合机制，提升了模型的精度和稳定性。&lt;/li&gt;
&lt;li&gt;我们在两个数据集上开展了大量的实验，对比了 5 个 baselines，结果显示我们的模型在预测人口流动的错误上减少了 8% ~ 13%。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="preliminaries"&gt;Preliminaries
&lt;/h1&gt;&lt;p&gt;这部分，我们首先介绍人口流量预测问题，简要回顾 ST-ResNet。&lt;/p&gt;
&lt;h2 id="problem-formulation"&gt;Problem Formulation
&lt;/h2&gt;&lt;p&gt;**Definition 1 (Region (Zhang et al. 2016)) 为了表示城市的区域，我们基于经纬度将城市划分成 $H \times W$ 个区域，所有的网格有相同大小且表示一个区域。&lt;/p&gt;
&lt;p&gt;**Definition 2 (Inflow/outflow (Zhang et al. 2016)) 为了表示城市内的人口流动，我们定义了区域 $(h, w)$ 在时段 $i$ 的流入和流出流量：&lt;/p&gt;
&lt;p&gt;$$
x^{h,w,in}_{i} = \sum_{T_{r_k} \in \mathbb{P}} \vert \lbrace j &amp;gt; 1 \mid g_{j-1} \not \in (h, w) \And g_j \in (h, w) \rbrace \vert,\&lt;/p&gt;
&lt;p&gt;x^{h,w,out}_{i} = \sum_{T_{r_k} \in \mathbb{P}} \vert \lbrace j \geq 1 \mid g_{j-1} \in (h, w) \And g_j \not \in (h, w) \rbrace \vert.
$$&lt;/p&gt;
&lt;p&gt;这里 $\mathbb{P}$ 表示时段 $i$ 的轨迹集合。$T_r: g_1 \rightarrow g_2 \rightarrow \cdots \rightarrow g_{\vert T_r \vert}$ 是 $\mathbb{P}$ 中的一条轨迹，$g_j$ 是坐标；
$g_j \in (h, w)$ 表示点 $g_j$ 在网格 $(h, w)$ 内，反之亦然；$\vert \cdot \vert$ 表示集合的基数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Crowd Flow Prediction&lt;/strong&gt;: 给定历史观测值 $\lbrace \mathbf{X}_i \mid i=1,2,\cdots, n-1 \rbrace$，预测 $\mathbf{X}_n$。&lt;/p&gt;
&lt;p&gt;ST-ResNet 包含四个组件，&lt;em&gt;closeness&lt;/em&gt;, &lt;em&gt;period&lt;/em&gt;, &lt;em&gt;trend&lt;/em&gt; 和 外部因素单元。每个组成部分通过一个分支的残差单元或全连接层预测出一个流量地图。然后模型使用一个线性组合作为末端融合方式融合这些预测值。ST-ResNet 的外部因素包含了天气、假期事件、元数据。&lt;/p&gt;
&lt;p&gt;卷积神经网络的卷积核通常很小，意味着他们不能直接捕获远距离的空间依赖。然而，大范围的空间依赖在城市中很重要。另一方面，ST-ResNet 忽略了人口流动的在位置上的影响。此外，ST-ResNet 的末端融合机制导致了模型交互上的缺点以及参数的低效，还有模型的不稳定的问题。&lt;/p&gt;
&lt;h1 id="our-model"&gt;Our Model
&lt;/h1&gt;&lt;p&gt;图 2 展示了我们模型的框架。主要有三个部分：流量输入、SemanticPlus 和 ResPlus 单元。流量慎入包含 &lt;em&gt;closeness, period, terend&lt;/em&gt;，由于数据的时间范围限制可以减少为 &lt;em&gt;closeness, period&lt;/em&gt;。SemanticPlus 包含 POI 分布和时间信息。ResPlus 单元可以捕获远距离空间依赖。每个区域的流入和流出流量通过每小时或者每半小时统计得到流量地图的时间序列。这些流量地图通过 Min-Max 归一化处理到 $[-1, 1]$。如图 2 所示，人口分布地图通过近期时间、近邻历史、远期历史选择后作为输入放入模型。不同类型的 POI 分布通过 Min-Max 归一化到 $[0, 1]$。如图 2 做部分所示，POI 分布地图通过时间信息赋予了不同的权重。之后，POI 信息和人流信息通过早融合后放入堆叠的 ResPlus 单元中。最后，ResPlus 单元不同级别的特征融合后进入卷积部分，然后通过 Tanh 映射到 $[-1, 1]$。下面会介绍细节。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/Fig2.JPG"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;h2 id="resplus"&gt;ResPlus
&lt;/h2&gt;&lt;p&gt;很多处理人口流量预测的深度学习模型主要包含两个部分：基于 RNN 的结构，像 ConvLSTM 和 Periodic-CRN，以及基于 CNN 的结构，如 Deep-ST 和 ST-ResNet。但是，训练基于 RNN 结构的模型费时。因此我们选用基于 CNN 的结构 ST-ResNet 作为我们的基础模型。&lt;/p&gt;
&lt;p&gt;在这篇论文中，我们设计 ConvPlus 来捕获城市内远距离的空间依赖。如图 3，ResPlus 单元使用一个 ConvPlus 和一个典型卷积。我们尝试了 Batch Normalization 和 Dropout，为了简介没有在图里面画出来。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/Fig3.JPG"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;典型卷积的每个通道对应一个卷积核。卷积核使用这些核来计算地图上的互相关系数，比如捕获梯度上的特征。卷积核的大小一般很小。在 ST-ResNet 和 DeepSTN+ 里面，卷积核的大小是 $3 \times 3$。但是城市中存在着远距离的依赖。人们可能坐地铁去上班。我们称这类关系叫远距离空间依赖关系。这种关系使得堆叠卷积难以有效地捕获这个关系。&lt;/p&gt;
&lt;p&gt;如图 3 左部分所示，在 ConvPlus 结构中，我们将典型卷积的一些通道分离来捕获每个区域的远距离空间依赖。然后用一个全连接层直接捕获每两个区域之间的远距离空间依赖，在这层前面用一个池化层来减少参数。因此，在 ConvPlus 的输出有两类通道。ConvPlus 的输出有着和普通卷积一样的输出，可以用于下一个卷积的输入。&lt;/p&gt;
&lt;p&gt;图 4 展示了两个不同区域的空间依赖热力图，分别是红色和黄色的星。这些目标区域不仅有区域上的依赖，还有一些和远处区域的远距离依赖。这也显示出不同的区域和地图上的其他区域有不一样的关系，这很难通过堆叠卷积有效地捕获。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/Fig4.JPG"
loading="lazy"
alt="Figure4"
&gt;&lt;/p&gt;
&lt;p&gt;因为 ConvPlus 有两类不同的输出通道，我们在 ResPlus 单元中使用 ConvPlus + Conv 而不是 ConvPlus + ConvPlus。没有 SemanticPlus 的 DeepSTN+ 形式化为：&lt;/p&gt;
$$
\widehat{\mathbf{X}} = f\_{Res}(f\_{EF}(\mathbf{X}^c + \mathbf{X}^p + \mathbf{X}^t)),
$$&lt;p&gt;三个 $\mathbf{X}$ 表示三种类型的历史地图——&lt;em&gt;closeness, period, trend&lt;/em&gt;。$\widehat{\mathbf{X}}$ 表示预测出的流量地图。$+$ 表示拼接操作。$f_{EF}$ 表示用来早融合不同类型信息的卷积函数，$f_{Res}$ 表示一个堆叠的 ResPlus 单元。&lt;/p&gt;
&lt;h2 id="semanticplus"&gt;SemanticPlus
&lt;/h2&gt;&lt;p&gt;POI 在人口流动上有很强烈的影响，这些影响随时间变化而变化。因此，我们继承这个先验知识到模型内。我们手机了包括类型、数量、位置的 POI 信息。然后统计每个网格内 POI 的数量，使用一个一维向量表示每种 POI 的分布。图 5 展示了北京的流量分布地图和餐饮分布地图。它们的分布很相似，并且互相关系数有 0.87，暗示了它们之间的潜在关系。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/Fig5.JPG"
loading="lazy"
alt="Figure5"
&gt;&lt;/p&gt;
&lt;p&gt;我们使用一个时间向量来表示每个人口流量地图的时间。时间向量包含两个部分：一个 one-hot 向量表示一天中的各个时间，如果时段按小时走，那长度就是 24；另一个 one-hot 向量表示是一周中的哪天，长度是 7。一个时间向量拼接了这两个向量。&lt;/p&gt;
&lt;p&gt;为了建模对流量地图有变化的时间影响的 POI 信息，我们将时间向量转换为 POI 的影响强度。我们使用大小为 $PN \times H \times W$ 的 $\mathbf{X}^s$ 来表示 POI 地图（$PN$ 表示 POI 的类数，$H$ 和 $W$ 是网格的行数和列数，一个向量 $\bf{I}$ 用来表示时间向量，大小为 $PN$ 的向量 $\bf{R}$ 表示 POI 的影响强度。因此，我们有带有时间权重的 POI 分布，形式化如下：&lt;/p&gt;
$$
\mathbf{S} = \mathbf{X}^s \ast \mathbf{R} = \mathbf{X}^s \ast f\_t(\mathbf{I})
$$&lt;p&gt;函数 $f_t()$ 将时间向量转换为表示 POI 影响强度的向量。$\ast$ 表示每个 POI 分布地图会被附上一个权重，表示 POI 的影响强度。我们假设同一类在不同的区域的 POI 有相同的时间模式。因此，一个类别的 POI 分布地图会有相同的权重。图 6 展示了娱乐和居住区的影响强度。影响强度在一周内随时间的变化而变化，每天存在着一些典型的模式。很多人早上去上班，工作结束后回家，所以每天早上和下午住宅区有明显的两个峰。对比居住区，娱乐区的影响相对稳定。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/Fig6.JPG"
loading="lazy"
alt="Figure6"
&gt;&lt;/p&gt;
&lt;h2 id="fusion"&gt;Fusion
&lt;/h2&gt;&lt;p&gt;三组件应该用更复杂的融合方式，而不是线性组合。这些带有 POI 信息的流量信息也有复杂的交互。为了建模这种相互影响，我们使用早融合而不是末端融合使得不同的信息能更早的融合起来。早融合减少了大约三分之二的参数。此外，ST-ResNet 有些时候不能收敛。我们发现这个问题可以通过早融合减少参数来简化模型解决。考虑到不同层的特征有不同的函数，我们在模型末端设定了一个多尺度的融合机制。这里我们形式化描述整个网络：&lt;/p&gt;
$$
\widehat{\mathbf{X}} = f\_{con}(f\_{Res}(f\_{EF}(\mathbf{X}^c + \mathbf{X}^p + \mathbf{X}^t + \mathbf{S}))),
$$&lt;p&gt;函数 $f_{EF}$ 表示一个早融合使用的卷积操作，在早融合之前压缩了通道数。函数 $f_{con}$ 表明了最后的多尺度融合，表示卷积层后的一个拼接层。$\bf{S}$ 表示 SemanticPlus 的输出，即 带有时间权重的 POI 分布。&lt;/p&gt;
&lt;h2 id="training"&gt;Training
&lt;/h2&gt;&lt;p&gt;算法 1 描述了训练过程。前 7 行是构建训练集和 POI 信息，模型通过 Adam 训练（8-12 行）&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/Alg1.JPG"
loading="lazy"
alt="Alg1"
&gt;&lt;/p&gt;
&lt;h1 id="performance-evaluation"&gt;Performance Evaluation
&lt;/h1&gt;&lt;p&gt;这部分，我们在两个数据集上不同城市的不同类型的流量上做了大量的实验，为了回答三个研究问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我们的提出的 DeepSTN+ 是否比现存的方法好？&lt;/li&gt;
&lt;li&gt;ResPlus, SemanticPlus, 早融合是怎么提升预测结果的？&lt;/li&gt;
&lt;li&gt;DeepSTN+ 的超参数如何影响预测结果？&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="datasets"&gt;Datasets
&lt;/h2&gt;&lt;p&gt;表 1 包含了数据。每个数据有两个子集：流量轨迹和 POI 信息。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/Table1.JPG"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;MobileBJ:&lt;/strong&gt;&lt;/em&gt; 数据是中国一个很流行的社交网络应用商提供的，时间范围是4 月 1 日到 4 月 30 日。记录了用户请求区域服务时的位置。我们用定义 2 转换成了网格流量。我们选择最后一周的数据作为测试集，前面的作为训练集。表 2 展示了这个数据集的 17 类 POI 信息。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;BikeNYC:&lt;/strong&gt;&lt;/em&gt; NYC 的自行车数据，2014 年，4 月 1 日到 9 月 30 日。数据包含了旅途时长，出发和到达站的 ID，起始和结束时间。最后 14 天的数据用来测试，其他的训练。我们选了 9 类 POI 信息。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/Table2.JPG"
loading="lazy"
alt="Table2"
&gt;&lt;/p&gt;
&lt;h2 id="baselines"&gt;Baselines
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;HA&lt;/li&gt;
&lt;li&gt;VAR&lt;/li&gt;
&lt;li&gt;ARIMA&lt;/li&gt;
&lt;li&gt;ConvLSTM&lt;/li&gt;
&lt;li&gt;ST-ResNet&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="metrics-and-parameters"&gt;Metrics and Parameters
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;RMSE&lt;/li&gt;
&lt;/ul&gt;
$$
RMSE = \sqrt{\frac{1}{T} \sum^T\_{i=1} \Vert \mathbf{X}\_i - \widehat{X}\_i \Vert^2\_2},
$$&lt;ul&gt;
&lt;li&gt;MAE&lt;/li&gt;
&lt;/ul&gt;
$$
MAE = \frac{1}{T} \sum^T\_{i=1} \vert \mathbf{X}\_i - \widehat{\mathbf{X}}\_i \vert,
$$&lt;p&gt;RMSE 作为 loss function。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/Table3.JPG"
loading="lazy"
alt="Table3"
&gt;&lt;/p&gt;
&lt;p&gt;表 3 展示了不同的参数设置。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/Table4.JPG"
loading="lazy"
alt="Table4"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/Table5.JPG"
loading="lazy"
alt="Table5"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/Fig7.JPG"
loading="lazy"
alt="Figure7"
&gt;&lt;/p&gt;</description></item><item><title>Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning</title><link>https://davidham3.github.io/blog/p/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/</link><pubDate>Fri, 19 Apr 2019 16:40:41 +0000</pubDate><guid>https://davidham3.github.io/blog/p/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/</guid><description>&lt;p&gt;TKDE 2019，网格流量预测，用一个模型同时预测每个网格的流入/流出流量和网格之间的转移流量，分别称为顶点流量和边流量，同时预测这两类流量是本文所解决的多任务预测问题。本文提出的是个框架，所以里面用什么组件应该都是可以的，文章中使用了 FCN。使用两个子模型分别处理顶点流量和边流量预测问题，使用两个子模型的输出作为隐藏状态表示，通过拼接或加和的方式融合，融合后的新表示再分别输出顶点流量和边流量。这篇文章和之前郑宇的文章一样，考虑了三种时序性质、融合了外部因素。损失函数从顶点流量预测值和真值之间的差、边流量预测值和真值之间的差、顶点流量预测值之和与边流量的预测值之差三个方面考虑。数据集是北京和纽约的出租车数据集。 &lt;a class="link" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=8606218" target="_blank" rel="noopener"
&gt;Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;——预测流量（如车流、人流、自行车流）包括结点的流入、流出流量以及不同的结点间的转移，在交通运输系统中的时空网络里扮演着重要的角色。然而，这个问题受多方面复杂因素影响，比如不同地点的空间关系、不同时段的时间关系、还有像活动和天气这样的外部因素，所以这是个有挑战性的问题。此外，一个结点的流量（结点流量）和结点之间的转移（边流量）互相影响。为了解决这个问题，我们提出了一个多任务的深度学习框架可以同时预测一个时空网络上的结点流量和边流量。基于全卷积网络，我们的方法设计了两个复杂的模型分别处理结点流量预测和边流量预测。这两个模型通过组合中间层的隐藏表示连接，而且共同训练。外部因素通过一个门控融合机制引入模型。在边流量预测模型上，我们使用了一个嵌入组件来处理顶点间的系数转移问题。我们在北京和纽约的出租车数据集上做了实验。实验结果显示比11种方法都好。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 Introduction
&lt;/h1&gt;&lt;p&gt;时空网络（ST-networks），如运输网络和传感器网络，在世界上到处都是，每个点有个空间坐标，每个边具有动态属性。时空网络中的流量有两种表示，如图 1，顶点流量（一个结点的流入和流出流量）和边流量（结点间的转移流量）。在运输系统中，这两类流量可通过4种方式测量，1. 近邻道路的车辆数，2. 公交车的旅客数，3. 行人数，4. 以上三点。图1b 是一个示意图。取顶点 $r_1$ 为例，我们可以根据手机信令和车辆 GPS 轨迹分别计算得到流入流量是 3，流出流量是 3。$r_3$ 到 $r_1$ 的转移是 3，$r_1$ 到 $r_2$ 和 $r_4$ 的转移是 2 和 1。因此，如图1c所示，我们能拿到两种类型的流量，四个结点的流入和流出分别是 $(3,3,0,5)$ 和 $(3,2,5,1)$。所有的边转移都看作是在有向图上发生的。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;预测这类的流量对公共安全，交通管理，网络优化很重要。取人口流动做一个例子，2015 年跨年夜的上海，踩踏事故导致 36 人死亡。如果能预测每个区域之间的人流转移，这样的悲剧就可以通过应急预案避免或减轻。&lt;/p&gt;
&lt;p&gt;然而，同时预测所有结点和边的转移是很难的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Scale and complexity&lt;/strong&gt;: 一个地方的流入和流出依赖于它的邻居，有近邻的也有遥远的，因为人们会在这些区域之间转移，尤其是有活动的时候。给定一个城市，有 $N$ 个地点，$N$ 很大，那么就有 $N^2$ 种转移方式，尽管这些转移可能不会同时发生。因此，预测地点的流量，要么是流入、流出或是转移流量，我们需要考虑地点之间的依赖关系。而且，预测也考虑过去时段的流量。此外，我们不能单独地预测每个地点的流量，因为城市内的地点间是相连的，相关的，互相影响的。复杂度和尺度都是传统机器学习模型，如概率图模型在解决这个问题时面临的巨大挑战。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model multiple correlations and external factors&lt;/strong&gt;: 我们需要对三种关系建模来处理预测问题。第一个是不同地点流量的空间相关性，包含近邻和遥远的。第二个是一个地点不同时段的流量间的时间关系，包括时间近邻、周期和趋势性。第三，流入流出流量和转移流量高度相关，互相影响。一个区域的转入流量之和是这个区域的流入流量。精确地预测一个区域的流出流量可以让预测其他区域的转移流量更精确，反之亦然。此外，这些流量受外部因素影响，如活动、天气、事故等。如何整合这些信息还是个难题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamics and sparsity&lt;/strong&gt;: 由于 $N^2$ 种情况，区域间随时间改变的转移流量比流入流出流量要大得多。一个地点和其他地点间的转移会在接下来的时段发生，可能是 $N^2$ 中的很小一部分（稀疏）。预测这样的稀疏转移也是个难题。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;为了解决上述挑战，我们提出了多任务深度学习框架MDL（图4）来同时预测顶点流量和边流量。我们的贡献有三点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDL 设计了一个深度神经网络来预测顶点流量（命名为 NODENET），另一个深度神经网络预测边流量（命名为 EDGENET）。通过将他们的隐藏状态拼接来连接这两个深度神经网络，并一同训练。此外，这两类流量的相关性通过损失函数中的正则项来建模。基于深度学习的模型可以处理复杂性和尺度等问题，同时多任务框架增强了每类流量的预测性能。&lt;/li&gt;
&lt;li&gt;NODENET 和 EDGENET 都是 three-stream 全卷积网络（3S-FCNs），closeness-stream, period-stream, trend-stream 捕获三种不同的时间相关性。每个 FCN 也同时捕获近邻和遥远的空间关系。一个门控组件用来融合时空相关性和外部因素。为了解决转移稀疏的问题，EDGENET 中我们设计了一个嵌入组件，用一个隐藏低维表示编码了稀疏高维的输入。&lt;/li&gt;
&lt;li&gt;我们在北京和纽约的 taxicab data 上评估了方法。结果显示我们的 MDL 超越了其他 11 种方法。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;表 1 列出了这篇文章中出现的数学符号。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Table1.JPG"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;
&lt;h1 id="2-problem-formulation"&gt;2 Problem Formulation
&lt;/h1&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Definition 1(Node).&lt;/strong&gt;&lt;/em&gt; 一个空间地图基于经纬度被分成 $I \times J$ 个网格，表示为 $V = \lbrace r_1, r_2, &amp;hellip;, r_{I\times J} \rbrace$，每个元素表示一个空间节点，如图2(a)。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Fig2.JPG"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;p&gt;令 $(\tau, x, y)$ 为时空坐标，$\tau$ 表示时间戳，$(x, y)$ 表示空间点。一个物体的移动可以记为一个按时间顺序的空间轨迹，起点和终点表示为 $s = (\tau_s, x_s, y_s)$ 和 $e = (\tau_e, x_e, y_e)$，表示出发地和目的地。$\mathbb{P}$ 表示所有的起止对。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Definition 2(In/out flows).&lt;/strong&gt;&lt;/em&gt; 给定一组起止对 $\mathbb{P}$。$\mathcal{T} = \lbrace t_1, \dots t_T\rbrace$ 表示一个时段序列。对于地图上第 $i$ 行第 $j$ 列的顶点 $r_{ij}$，时段 $t$ 流出和流入的流量分别定义为：&lt;/p&gt;
$$\tag{1}
\mathcal{X}\_t(0, i, j) = \vert \lbrace (s,e) \in \mathbb{P} : (x\_s, y\_s) \in r\_{ij} \wedge \tau\_s \in t \rbrace \vert
$$$$\tag{2}
\mathcal{X}\_t(1, i, j) = \vert \lbrace (s,e) \in \mathbb{P} : (x\_e, y\_e) \in r\_{ij} \wedge \tau\_e \in t \rbrace \vert
$$&lt;p&gt;其中 $\mathcal{X}_t(0, :, :)$ 和 $\mathcal{X}_t(1, :, :)$ 表示流出和流入矩阵。$(x, y) \in r_{ij}$ 表示点 $(x, y)$ 在顶点 $r_{ij}$ 上，$\tau_e \in t$ 表示时间戳 $\tau_e$ 在时段 $t$ 内。流入和流出矩阵在特定时间的矩阵如图2。&lt;/p&gt;
&lt;p&gt;考虑两类流量（流入和流出），一个随时间变化的空间地图一般表示一个时间有序的张量序列，每个张量对应地图在特定时间的一个快照。详细来说，每个张量包含两个矩阵：流入矩阵和流出矩阵，如图 2 所示。&lt;/p&gt;
&lt;p&gt;让 $V$ 表示时空网络中的顶点集，$N \triangleq \vert V \vert = I \times J$ 是顶点数。一个时间图包含 $T$ 个离散的不重叠的时段，表示为有向图 $G_{t_1}, \dots G_{t_T}$ 的时间有序序列。图 $G_t = (V, E_t)$ 捕获了时段 $t$ 时空系统上的拓扑状态。对于每个图 $G_t$ (其中 $t = t_1, \dots, t_T$) 存在一个对应的权重矩阵 $\mathbf{S}_t \in \mathbb{R}^{N \times N}$，表示时段 $t$ 的带权有向边。在我们的研究中，时段 $t$ 顶点 $r_s$ 到顶点 $r_e$ 的边的权重，是一个非负标量，表示 $r_s$ 到 $r_e$ 的 &lt;em&gt;transition&lt;/em&gt;，时段 $t$ 上两个顶点间没有连接的话，对应的元素在 $\mathbf{S}_t$ 中为 0。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Definition 3 (Transition).&lt;/strong&gt;&lt;/em&gt; 给定一组起止点对 $\mathbb{P}$。$\mathcal{T} = \lbrace t_1, \dots, t_T \rbrace$ 是一组时段的序列。$\mathbf{S}_t$ 是时段 $t$ 的转移矩阵，$r_s$ 到 $r_e$ 之间的转移表示为 $\mathbf{S}_t(r_s, r_e)$，定义为：&lt;/p&gt;
$$\tag{3}
\mathbf{S}\_t(r\_s, r\_e) = \vert \lbrace (s,e) \in \mathbb{P} : (x\_s, y\_s) \in r\_s \wedge (x\_e, y\_e) \in r\_e \wedge \tau\_s \in t \wedge \tau\_e \in t \rbrace \vert
$$&lt;p&gt;其中 $r_s, r_e \in V$ 是起始顶点和终止顶点。$(x, y) \in r$ 表示点 $(x, y)$ 在网格 $r$ 上。$\tau_s \in t$ 和 $\tau_e \in t$ 表示时间戳 $\tau_s$ 和 $\tau_e$ 都在时段 $t$ 内。我们考虑转移至发生在一个特定的时段内。因此，对于实际应用来说，我们可以预测起始和结束都发生在未来的转移。&lt;/p&gt;
&lt;h2 id="21-converting-time-varying-graphs-into-tensors"&gt;2.1 Converting time-varying graphs into tensors
&lt;/h2&gt;&lt;p&gt;我们将每个时间上的图转为张量。给定时间 $t$ 有向图 $G_t = (V, E_t)$，我们先做展开，然后计算有向带权矩阵（转移矩阵 $\mathbf{S}_t$），最后给定一个张量 $\mathcal{M}_t \in \mathbf{R}^{2N \times I \times J}$。图 3 是示意图。(a)给定时间 $t$ 4 个顶点 6 条边的图。(b)首先展开成有向图。(c)对每个顶点，有一个流入的转移，还有个流出的转移，由一个向量表示（维度是8）。取 $r_1$ 为例，它的流出和流入转移向量分别为 $[0, 2, 0, 1]$ 和 $[0, 0, 3, 0]$，拼接后得到一个向量 $[0, 2, 0, 1, 0, 0, 3, 0]$，包含流出和流入的信息。(d)最后，我们将矩阵 reshape 成一个张量，每个顶点根据原来地图有一个固定的空间位置，保护了空间相关性。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Fig3.JPG"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;h2 id="22-flow-prediction-problem"&gt;2.2 FLow Prediction Problem
&lt;/h2&gt;&lt;p&gt;流量预测，简单来说，是时间序列问题，目标是给定历史 $T$ 个时段的观测值，预测 $T+1$ 时段每个区域的流量。但是我们的文章中流量有两个层次，流入和流出以及区域间的转移流量。我们的目标是同时预测这些流量。此外，我们还融入了外部因素如房价信息，天气状况，温度等等。这些外部因素可以收集并提供一些额外有用的信息。相关的符号在表 1 之中。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Problem 1.&lt;/strong&gt;&lt;/em&gt; 给定历史观测值 $\lbrace \mathcal{X}_t, \mathcal{M}_t \mid t = t_1, \dots, t_T \rbrace$，外部特征 $\mathcal{E}_T$，我们提出一个模型共同预测 $\mathcal{X}_{t_{T+1}}$ 和 $\mathcal{M}_{t_{T+1}}$。&lt;/p&gt;
&lt;h1 id="3-multitask-deep-learning"&gt;3 Multitask Deep Learning
&lt;/h1&gt;&lt;p&gt;图 4 展示了我们的 MDL 框架，包含 3 个组成部分，分别用于数据转换，顶点流量建模，边流量建模。我们首先将轨迹（或订单）数据转换成两类流量，i) 顶点流量表示成有时间顺序的张量序列 $\lbrace\mathcal{X}_t \mid t = t_1, \dots, t_T \rbrace$ (1a); ii) 边流量是一个有时间顺序的图序列（转移矩阵）$\lbrace\mathbf{S}_t \mid t = t_1, \dots, t_T \rbrace$ (2a)，之后再根据 2.1 节的方法转换为张量的序列 $\lbrace\mathcal{M}_t \mid t = t_1, \dots, t_T \rbrace$ (2b)。这两类像视频一样的数据之后放到 NODENET 和 EDGENET 中。以 NODENET 为例，它选了三个不同类型的片段，放入 3S-FCN 中，对时间相关性建模。在这个模型中，每部分的 FCN 可以通过多重卷积捕获空间相关性。NODENET 和 EDGENET 中间的隐藏表示通过一个 BRIDGE 组件连接，使两个模型可以共同训练。我们使用一个嵌入层来处理转移稀疏的问题。一个门控融合组件用来整合外部信息。顶点流量和边流量用一个正则化来建模。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Fig4.JPG"
loading="lazy"
alt="Figure4"
&gt;&lt;/p&gt;
&lt;h2 id="31-edgenet"&gt;3.1 EDGENET
&lt;/h2&gt;&lt;p&gt;根据上述的转换方法，每个时段的转移图可以转换成一个张量 $\mathcal{M}_t \in \mathbb{R}^{2N \times I \times J}$。对于每个顶点 $r_{ij}$，它最多有 $2N$ 个转移概率，包含 $N$ 个流入和 $N$ 个流出。然而，对于一个确定的时段，顶点间的转移是稀疏的。受 NLP 的嵌入方法启发，我们提出了使用空间嵌入方法，解决这样的稀疏和高维问题。详细来说，空间嵌入倾向于学习一个将 $2N$ 维映射到 $k$ 维的函数：&lt;/p&gt;
$$\tag{4}
\mathcal{Z}\_t(:, i, j) = \mathbf{W}\_m \mathcal{M}\_t (:, i, j) + \mathbf{b}\_m, 1 \leq i \leq I, 1 \leq j \leq J
$$&lt;p&gt;其中 $\mathbf{W}_m \in \mathbb{R}^{k \times 2N}$ 和 $\mathbf{b}_m \in \mathbb{R}^k$ 是参数。所有的结点共享参数。$\mathcal{M}_t(:, i, j) \in \mathbb{R}^{2N}$ 表示 $(i, j)$ 的向量。&lt;/p&gt;
&lt;p&gt;流量，比如城市中的交通流，总是受时空依赖关系影响。为了捕获不同的时间依赖（近邻、周期、趋势），Zhang et al. 提出了深度时空残差网络，沿时间轴选择不同的关键帧。受这点的启发，我们选择近邻、较近、远期关键帧来预测时段 $t$，分别表示为 $M^{dep}_t = \lbrace M^{close}_t, M^{period}_t, M^{trend}_t \rbrace$，如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Closeness&lt;/strong&gt; dependents:
$$M^{close}\_t = \lbrace \mathcal{Z}\_{t-l\_c}, \dots, \mathcal{Z}\_{t-1} \rbrace$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Period&lt;/strong&gt; dependents:
$$M^{period}\_t = \lbrace \mathcal{Z}\_{t-l\_p}, \mathcal{Z}\_{t-(l\_p - 1) \cdot p}, \dots, \mathcal{Z}\_{t-p} \rbrace$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Trend&lt;/strong&gt; dependents:
$$M^{trend}\_t = \lbrace \mathcal{Z}\_{t-l\_q \cdot q}, \mathcal{Z}\_{t-(l\_q - 1)\cdot q}, \dots, \mathcal{Z}\_{t-q} \rbrace$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中 $p$ 和 $q$ 是周期和趋势范围。$l_c$, $l_p$ 和 $l_q$ 是三个序列的长度。&lt;/p&gt;
&lt;p&gt;输出（即下个时段的预测）和输入有相同的分辨率。这样的人物和图像分割问题很像，可以通过全卷积网络 (FCN) [22] 处理。&lt;/p&gt;
&lt;p&gt;受到这个启发，我们提出了三组件的 FCN，如图 4，来捕获时间近邻、周期和趋势依赖。每个组件都是个 FCN，包含了很多卷积（图 5）。根据卷积的性质，一个卷积层可以捕获空间近邻关系。随着卷积层数的增加，FCN 可以捕获更远的依赖，甚至是城市范围大小的空间依赖。然而，这样的深层卷积网络很难训练。因此我们使用残差连接来帮助训练。类似残差网络中的残差连接，我们使用一个包含 BN，ReLU，卷积的块。令三个近邻、周期、趋势三组件的输出分别为 $\mathcal{M}_c$, $\mathcal{M}_p$, $\mathcal{M}_q$。不同的顶点在近邻、周期、趋势上可能有不同的性质。为了解决这个问题，我们提出使用一个基于参数矩阵的融合方式（图 4 中的 PM 融合）：&lt;/p&gt;
$$\tag{5}
\mathcal{M}\_{fcn} = \mathbf{W}\_c \odot \mathcal{M}\_c + \mathbf{W}\_p \odot \mathcal{M}\_p + \mathbf{W}\_q \odot \mathcal{M}\_q
$$&lt;p&gt;其中 $\odot$ 是哈达玛积，$\mathbf{W}$ 是参数，调整三种时间依赖关系的影响。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Fig5.JPG"
loading="lazy"
alt="Figure5"
&gt;&lt;/p&gt;
&lt;h2 id="32-nodenet-and-bridge"&gt;3.2 NODENET and BRIDGE
&lt;/h2&gt;&lt;p&gt;类似 EDGENET，NODENET 也是一个 3S-GCN，我们选择近邻、较近、遥远的关键帧作为近邻、周期、趋势依赖。区别是 NODENET 没有嵌入层因为输入的通道数只有 2。这三种不同的依赖放入三个不同的 FCN 中，输出通过 PM 融合组件融合（图 4）。然后，得到 3S-FCN 的输出，表示为 $\mathcal{X}_{fcn} \in \mathbb{R}^{C_x \times I \times J}$。&lt;/p&gt;
&lt;p&gt;考虑顶点流量与边流量的相关性，所以从 NODENET 和 EDGENET 学习到的表示应该被连起来。为了连接 NODENET 和 EDGENET，假设 NODENET 和 EDGENET 的隐藏表示分别为 $\mathcal{X}_{fcn}$ 和 $\mathcal{M}_{fcn}$。我们提出两种融合方法：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SUM Fusion:&lt;/strong&gt; 加和融合方法直接将两种表示相加：&lt;/p&gt;
$$\tag{6}
\mathcal{H}(c, :, :) = \mathcal{X}\_{fcn}(c, :, :) + \mathcal{M}\_{fcn}(c, :, :), c = 0, \dots, C - 1
$$&lt;p&gt;其中 $C$ 是 $\mathcal{X}_{fcn}$ 和 $\mathcal{M}_{fcn}$ 的通道数，$\mathcal{H} \in \mathbb{R}^{C \times I \times J}$。显然这种融合方法受限于两种表示必须有相同的维度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CONCAT Fusion:&lt;/strong&gt; 为了从上述的限制中解脱，我们提出了另一种融合方法。顺着通道拼接两个隐藏表示：&lt;/p&gt;
$$\tag{7}
\mathcal{H}(c, :, :) = \mathcal{X}\_{fcn}(c, :, :), c=0, \dots, C\_x - 1
$$$$\tag{8}
\mathcal{H}(C\_x + c, :, :) = \mathcal{M}\_{fcn}(c, :, :), c=0, \dots, C\_m - 1
$$&lt;p&gt;$C_x$ 和 $C_m$ 分别是两个隐藏表示的通道数。$\mathcal{H} \in \mathbb{R}^{(C_x + C_m) \times I \times J}$。拼接融合实际上可以通过互相强化更好地融合顶点流量和边流量。像 BRIDGE 一样我们也讨论了其他的融合方式（4.3 节）。&lt;/p&gt;
&lt;p&gt;在拼接融合中，我们在 NODENET 和 EDGENET 中分别加了一层卷积。卷积用来将合并的隐藏特征 $\mathcal{H}$ 映射到 不同通道大小的输出上，即 $\mathcal{X}_{res} \in \mathbb{R}^{2 \times I \times J}$ 和 $\mathcal{M}_{res} \in \mathbb{R}^{2N \times I \times J}$，如图 6。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Fig6.JPG"
loading="lazy"
alt="Figure6"
&gt;&lt;/p&gt;
&lt;h2 id="33-fusing-external-factors-using-a-gating-mechanism"&gt;3.3 Fusing External Factors Using a Gating Mechanism
&lt;/h2&gt;&lt;p&gt;外部因素，活动、天气会影响时空网络不同区域的流量。举个例子，一起事故可能会阻塞一个局部区域的交通，一场暴风雨可能会减少整个城市的流量。这样的外部因素就像一个开关，如果它打开了那流量会产生巨大的变化。基于这个思路，我们开发了一种基于门控机制的融合，如图 6 所示。时间 $t$ 的外部因素表示为 $\mathcal{E}_t \in \mathbb{R}^{l_e \times I \times J}$，$\mathcal{E}_t(:, i, j) \in \mathbb{R}^{l_e}$ 表示一个特定顶点的外部信息。我们可以通过下式获得 EDGENET 的门控值：&lt;/p&gt;
$$\tag{9}
\mathbf{F}\_m(i, j) = \sigma(\mathbf{W}\_e(:, i, j) \cdot \mathcal{E}\_t(:, i, j) + \mathbf{b}\_e(i, j)), 1 \leq i \leq I, 1 \leq j \leq J
$$&lt;p&gt;其中 $\mathbf{W}_e \in \mathbb{R}^{l_e \times I \times J}$ 和 $\mathbf{b}_e \in \mathbb{R}^{I \times J}$ 是参数。$\mathbf{F}_m \in \mathbb{R}^{I \times J}$ 是 GATING 的输出，$\mathbf{F}_m(i, j)$ 是对应时空网络中结点 $r_{ij}$ 的门控值。$\sigma(\cdot)$ 是 sigmoid 激活函数，$\cdot$ 是两向量的内积。&lt;/p&gt;
&lt;p&gt;然后我们使用 PRODUCT 融合方式：&lt;/p&gt;
$$\tag{10}
\hat{\mathcal{M}}\_t(c, :, :) = \text{tanh}(\mathbf{F}\_m \odot \mathcal{M}\_{Res}(c, :, :)), c = 0, \dots, 2N - 1
$$&lt;p&gt;类似的，NODENET 最后在时间 $t$ 的预测结果为：&lt;/p&gt;
$$\tag{11}
\hat{\mathcal{X}}\_t(c, :, :) = \text{tanh} (\mathbf{F}\_x \odot \mathcal{X}\_{Res}(c, :, :)), c = 0, 1
$$&lt;p&gt;其中 $\mathbf{F}_x \in \mathbb{R}^{I \times J}$ 是 GATING 的另一个输出。对于顶点流量和边流量使用不同的门控值的一个原因是外部因素对流入/流出流量和不同地点之间的转移流量的影响是不一致的。&lt;/p&gt;
&lt;h2 id="34-losses"&gt;3.4 Losses
&lt;/h2&gt;&lt;p&gt;令 $\phi$ 为 EDGENET 中所有的参数，我们的目标是通过最小化目标函数学习这些参数：&lt;/p&gt;
$$\tag{12}
\mathop{\mathrm{argmin}}\limits\_{\phi} \mathcal{J}\_{edge} = \sum\_{t \in \mathcal{T}}\sum^{2N-1}\_{c=0} \Vert Q^c\_t \odot (\hat{\mathcal{M}}\_t(c, :, :) - \mathcal{M}\_t(c, :, :)) \Vert^2\_F
$$&lt;p&gt;其中 $Q^c_t$ 是指示矩阵，表示 $\mathcal{M}_t(c, :, :)$ 中所有非零元素。$\mathcal{T}$ 是可用的时段，$\Vert \cdot \Vert_F$ 是矩阵的 F 范数。&lt;/p&gt;
&lt;p&gt;类似的，$\theta$ 是 NODENET 的参数，目标函数是：&lt;/p&gt;
$$\tag{13}
\mathop{\mathrm{argmin}}\limits\_{\theta} \mathcal{J}\_{node} = \sum\_{t \in \mathcal{T}}\sum^1\_{c=0} \Vert P^c\_t \odot (\hat{\mathcal{X}}\_t(c, :, :) - \mathcal{X}\_t(c, :, :)) \Vert^2\_F
$$&lt;p&gt;其中 $P^c_t$ 是指示矩阵，表示 $\mathcal{X}_t(c, :, :)$ 中所有非零元素。我们知道对于一个结点来说，它的转入流量之和就是它的流入流量，转出流量之和就是流出流量。定义 2 中定义，$\hat{\mathcal{X}}_t(0, :, :)$ 和 $\hat{\mathcal{X}}_t(1, :, :)$ 分别是流出和流入矩阵。根据 2.1 节定义的方法构建转移矩阵，可知前 $N$ 个通道表示转出流量，后 $N$ 个通道表示转入流量。因此，有下面的损失函数：&lt;/p&gt;
$$\tag{14}
\mathop{\mathrm{argmin}}\limits\_{\theta, \phi} \sum\_{t \in \mathcal{T}} \sum\_i \sum\_j (\Vert \hat{\mathcal{X}}\_t(0, i, j) - \sum^{N-1}\_{c=0} \hat{\mathcal{M}}\_t(c,i,j) \Vert^2 + \Vert \hat{\mathcal{M}}\_t(1,i,j) - \sum^{2N-1}\_{c=N} \hat{\mathcal{M}}\_t(c,i,j) \Vert^2)
$$&lt;p&gt;或者等价的可以写成&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/EQ1.JPG"
loading="lazy"
alt="EQ15"
&gt;&lt;/p&gt;
&lt;p&gt;最后，我们获得融合的损失：&lt;/p&gt;
$$\tag{16}
\mathop{\mathrm{argmin}}\limits\_{\theta, \phi} \lambda\_{node} \mathcal{J}\_{node} + \lambda\_{edge} \mathcal{J}\_{edge} + \lambda\_{mdl} \mathcal{J}\_{mdl}
$$&lt;p&gt;其中，$\lambda_{node}$, $\lambda_{edge}$, $\lambda_{mdl}$ 是可调节的参数。&lt;/p&gt;
&lt;h3 id="341-optimization-algorithm"&gt;3.4.1 Optimization Algorithm
&lt;/h3&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Alg1.JPG"
loading="lazy"
alt="Alg1"
&gt;&lt;/p&gt;
&lt;p&gt;算法 1 是 MDL 的训练过程。1-4 行是构建训练样例。7-8 行是用批量样本优化目标函数。&lt;/p&gt;
&lt;h1 id="4-experiments"&gt;4 Experiments
&lt;/h1&gt;&lt;p&gt;两个数据集 &lt;strong&gt;TaxiBJ&lt;/strong&gt; 和 &lt;strong&gt;TaxiNYC&lt;/strong&gt;，看表 2。我们使用 RMSE 和 MAE 作为评价指标。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Table2.JPG"
loading="lazy"
alt="Table2"
&gt;&lt;/p&gt;
&lt;h2 id="41-settings"&gt;4.1 Settings
&lt;/h2&gt;&lt;h3 id="411-datasets"&gt;4.1.1 Datasets
&lt;/h3&gt;&lt;p&gt;我们使用表 3 中的两个数据集。每个数据集包含两个子集，轨迹/出行和外部因素，细节如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TaxiBJ&lt;/strong&gt;: 北京出租车 GPS 轨迹数据有四个时段：20130101-20131030, 20140301-20140630, 20150501-20150630, 201501101-20160410。我们用最后 4 个星期作为测试集，之前的数据作为训练集。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TaxiNYC&lt;/strong&gt;: NYC 2011 到 2014 年的出租车订单数据。订单数据包含上车和下车的时间。上车和下车地点。最后四个星期作为测试集。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Table3.JPG"
loading="lazy"
alt="Table3"
&gt;&lt;/p&gt;
&lt;h3 id="412-baselines"&gt;4.1.2 Baselines
&lt;/h3&gt;&lt;p&gt;HA, ARIMA, SARIMA, VAR, RNN, LSTM. GRU, ST-ANN, ConvLSTM, ST-ResNet, MRF.&lt;/p&gt;
&lt;h3 id="413-preprocessing"&gt;4.1.3 Preprocessing
&lt;/h3&gt;&lt;p&gt;MDL 的输出，我们用 $\text{tanh}$ 作为最后的激活函数。我们用最大最小归一化。评估的时候，将预测值转换为原来的值。对于外部因素，使用 one-hot，假期和天气放入二值向量中，用最大最小归一化把温度和风速归一化。&lt;/p&gt;
&lt;h3 id="414-hyperparameters"&gt;4.1.4 Hyperparameters
&lt;/h3&gt;&lt;p&gt;$\lambda_{node} = 1$ 和 $\lambda_{edge} = 1$，$\lambda_{mdl} = 0.0005$，$p$ 和 $q$ 按经验设定为一天和一周。三个依赖序列的长度分别为 $l_c \in \lbrace 1, 2, 3\rbrace$, $l_p \in \lbrace 1,2,3 \rbrace$, $l_q \in \lbrace 1,2,3 \rbrace$。卷积的数量是 5 个。训练集的 90% 用来训练，10% 来验证，用早停选最好的参数。然后使用所有的数据训练模型。网络参数通过随机初始化，Adam 优化。batch size 32。学习率 $\lbrace 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005 \rbrace$。&lt;/p&gt;
&lt;h3 id="415-evaluation-metrics"&gt;4.1.5 Evaluation Metrics
&lt;/h3&gt;&lt;p&gt;RMSE 和 MAE。&lt;/p&gt;
&lt;h2 id="42-results"&gt;4.2 Results
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Table4.JPG"
loading="lazy"
alt="Table4"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Node Flow Prediction.&lt;/strong&gt; 我们先比流入和流出流量的预测。表 4 展示了两个数据集上的评价指标结果。MDL 和 MRF 比其他所有的方法多要好。我们的 MDL 在 NYC 的数据集上明显比 MRF 好。BJ 的数据集上，MDL 比 MRF 差不多。原因是 NYC 数据集比 BJ 数据集大了三倍。换句话说，在大的数据集上，我们的方法比 MRF 更好。我们也注意到训练 MRF 很好使，在 BJ 数据集上训练了一个星期。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Table5.JPG"
loading="lazy"
alt="Table5"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Results of Edge Flow Prediction.&lt;/strong&gt; 表6 展示了边流量预测。边流量预测的实验很费时。MDL 比其他的都好。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Table6.JPG"
loading="lazy"
alt="Table6"
&gt;&lt;/p&gt;
&lt;h2 id="43-evaluation-on-fusing-mechanisms"&gt;4.3 Evaluation on Fusing Mechanisms
&lt;/h2&gt;&lt;p&gt;融合 NODENET 和 EDGENET 有 CONCAT 和 SUM 两种方法。融合外部因素有 GATED 和 SIMPLE 融合，或者不使用。因此总共有 6 种方法。如表 7。使用同样的超参数设定。我们发现 CONCAT + GATING 比其他的方法好。&lt;/p&gt;
&lt;h2 id="44-evaluation-on-model-hyper-parameters"&gt;4.4 Evaluation on Model Hyper-parameters
&lt;/h2&gt;&lt;h3 id="441-effect-of-training-data-size"&gt;4.4.1 Effect of Training Data Size
&lt;/h3&gt;&lt;p&gt;我们选了 NYC 3 个月，6 个月，1 年，3 年数据。$l_c = 3$, $l_p = 1$, $l_q = 1$。图 8 是结果。我们观察到数据越多，效果越好。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Fig8.JPG"
loading="lazy"
alt="Figure8"
&gt;&lt;/p&gt;
&lt;h3 id="442-effect-of-network-depth"&gt;4.4.2 Effect of Network Depth
&lt;/h3&gt;&lt;p&gt;图 9 展示了网络深度在 NYC 3 个月数据集上的影响。网络越深，RMSE 会下降，因为网络越深越能捕获更大范围的空间依赖。然而，网络更深 RMSE 就会上升，这是因为网络加深后训练会变得困难。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Fig9.JPG"
loading="lazy"
alt="Figure9"
&gt;&lt;/p&gt;
&lt;h3 id="443-effect-of-multi-task-component"&gt;4.4.3 Effect of multi-task component
&lt;/h3&gt;&lt;p&gt;表 8 和图 10 展示了多任务组件的影响。&lt;/p&gt;
&lt;p&gt;我们可以看到转移流量预测任务大多数情况下可以提升，$\lambda_{node} = \lambda_{edge} = 1$，$\lambda_{mdl}=0.1$，我们的模型获得最好的效果，两种任务都获得更好的结果，证明了多任务可以互相提升。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Table8.JPG"
loading="lazy"
alt="Table8"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Fig10.JPG"
loading="lazy"
alt="Figure10"
&gt;&lt;/p&gt;
&lt;h2 id="45-flow-predictions"&gt;4.5 Flow Predictions
&lt;/h2&gt;&lt;p&gt;图 11 描绘了我们的 MDL 在 NYC 上预测两个节点未来一小时的数据。结点 (10, 1)，总是比 (8, 3) 高。我们的模型在预测曲线上更精确。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/Fig11.JPG"
loading="lazy"
alt="Figure11"
&gt;&lt;/p&gt;</description></item><item><title>Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction</title><link>https://davidham3.github.io/blog/p/revisiting-spatial-temporal-similarity-a-deep-learning-framework-for-traffic-prediction/</link><pubDate>Thu, 21 Mar 2019 10:43:34 +0000</pubDate><guid>https://davidham3.github.io/blog/p/revisiting-spatial-temporal-similarity-a-deep-learning-framework-for-traffic-prediction/</guid><description>&lt;p&gt;AAAI 2019。网格流量预测，两个问题：空间依赖动态性，另一个是周期平移。原文链接：&lt;a class="link" href="http://export.arxiv.org/abs/1803.01254" target="_blank" rel="noopener"
&gt;Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;由于大规模的交通数据越来越多，而且交通预测在实际中很重要，交通预测在 AI 领域引起越来越多的关注。举个例子，一个精确的出租车需求预测可以协助出租车公司预分配出租车。交通预测的关键在于如何对复杂的空间依赖和时间动态性建模。尽管两个因素在建模的时候都会考虑，当前的方法仍会做很强的假设，即空间依赖在时间上是平稳的，时间依赖是严格周期的。然而，实际中的空间依赖可能是动态的（即随时间的变化而变化），而且时间动态性可能从一个时段到另一个时段有波动。在这篇文章中，我们有两个重要发现：（1）区域间的空间依赖是动态的；（2）时间依赖虽说有天和周的模式，但因为有动态时间平移，它不是严格周期的。为了解决这两个问题，我们提出了一个新的时空动态网络（STDN），我们用一个门控机制学习区域间的动态相似性，用一个周期性平移的注意力机制来处理长期周期时间平移现象。据我们所知，这是第一个在一个统一的框架中解决这两个问题的工作。我们的实验结果证明了提出的方法是有效的。&lt;/p&gt;
&lt;h1 id="introduction"&gt;Introduction
&lt;/h1&gt;&lt;p&gt;交通预测是一个时空预测问题。精确的预测模型对很多应用都很重要。在传统交通预测问题中，给定历史数据（比如一个区域的流量，或一个交叉卡前几个月每小时的流量），预测未来一段时间的数据。这方面的研究已经有几十年了。在时间序列社区，ARIMA 和 Kalman filtering被广泛地应用到这一领域。然而这些早期方法是针对每个区域分别预测，最近的方法考虑了空间信息（比如针对近邻区域增加正则项）和外部因素（如地点信息，天气状况，地区活动）。然而，这些方法仍基于机器学习中的传统时间序列模型，不能很好的捕获复杂的非线性时空依赖）。&lt;/p&gt;
&lt;p&gt;最近，深度学习方法在很多任务上取得了成功。比如，一些研究将城市交通看作是热力图的图片，使用 CNN 对非线性空间关系建模。为了对非线性时间关系建模，人们提出了基于 RNN 的框架。Yao et al。 更是提出了用 CNN 和 LSTM 同时处理时间和空间依赖的框架。&lt;/p&gt;
&lt;p&gt;尽管考虑了同时对时空建模，现存的方法主要有两点不足。首先，区域间的空间依赖依赖于历史数据的相似性，模型学习到了一个静态的空间依赖。然而，区域间的依赖随时间是改变的。举个例子，早上，居民区和商业区之间的依赖关系强；深夜，关系就弱了。然而，这样的动态依赖在之前的研究中没有考虑。&lt;/p&gt;
&lt;p&gt;另一个限制是现存的研究忽略了长期周期依赖。交通数据又很强的日和周周期性，基于这种周期性的依赖关系可能用于预测。然而，一个挑战是交通数据不是严格周期的。举个例子，周末的高峰通常发生在下午的后半段，不同的日子时间不一致，从4:30pm到6:00pm变化。尽管之前的研究考虑了周期性，他们没能考虑序列性的依赖和周期性中的时间平移。&lt;/p&gt;
&lt;p&gt;为了解决前面提出的问题，我们提出了新的深度学习框架，时空动态网络用于交通预测。STDN 是基于时空神经网络的，使用局部 CNN 和 LSTM 分别处理时空信息。一个门控局部 CNN 使用区域间的动态相似性对空间依赖建模。一个周期平移的注意力机制用来学习长期周期依赖。通过注意力机制对长期周期信息和时间平移建模。我们的方法还用 LSTM 以层次的方式处理序列依赖。&lt;/p&gt;
&lt;p&gt;我们再大型的真实数据集上做了评测，纽约出租车数据和纽约的共享单车数据。和 state-of-the-art 的全面对比展示了我们模型的性能。我们的贡献如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我们提出了一个流式门控机制对动态空间相似性建模。门控制信息在邻近区域传播。&lt;/li&gt;
&lt;li&gt;我们提出了一个周期平移注意力机制，通过同时时可用长期周期信息和时间平移。&lt;/li&gt;
&lt;li&gt;我们在几个真实数据集上开展了实验，效果好。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="notations-and-problem-formulation"&gt;Notations and Problem Formulation
&lt;/h1&gt;&lt;p&gt;我们将整个城市分为 $a \times b$ 个网格，一共 $n$ 个区域（$n = a \times b$），使用 $\lbrace 1,2,\dots,n \rbrace$ 表示他们。我们将整个时间周期分为 $m$ 个登场的连续时段。任何一个个体的移动，其本质上是整个城市交通的一部分，总是从一个区域出发，过一段时间到达目的地。我们在一个时段内给每个区域定义一个开始/结束流量作为区域出发/到达的移动发生次数。$y^s_{i,t}$ 和 $y^e_{i,t}$ 表示开区域 $i$ 在时段 $t$ 的开始/结束流量。此外，对个体旅行的聚合形成交通流，描述了区域对之间的考虑时间的移动。时段 $t$ 从区域 $i$ 开始的交通流在时段 $\tau$ 于区域 $j$ 结束，表示为 $f^{j,\tau}_{i,t}$。显然，交通流反映了区域间的连通性，也反映了个体的移动。图1（c）给出了流量和流动的展示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem(Traffic Volume Prediction)&lt;/strong&gt; 给定知道时段 $t$ 的数据，交通流预测问题目标是预测时段 $t+1$ 的起始和结束流量。&lt;/p&gt;
&lt;h1 id="spatial-temporal-dynamic-network"&gt;Spatial-Temporal Dynamic Network*
&lt;/h1&gt;&lt;p&gt;这部分，我们讲一下细节。图1是我们模型的架构。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/revisiting-spatial-temporal-similarity-a-deep-learning-framework-for-traffic-prediction/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;h2 id="local-spatial-temporal-network"&gt;Local Spatial-Temporal Network
&lt;/h2&gt;&lt;p&gt;为了捕获时空序列依赖，在出租车需求预测上，融合局部 CNN 和 LSTM 展示出了非常好的表现。我们这里使用局部 CNN 和 LSTM 处理空间和短期时间依赖。为了手动地提升两种流量的预测（起始和结束），我们将他们集成起来。这部分称为 Local Spatial-Temporal Network (LSTN)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Local spatial dependency&lt;/strong&gt; 卷积神经网络用来捕获空间关系。将整个城市看作一张图片，简单地使用 CNN 不能获得最好的性能。包含了弱关系的区域会导致预测性能下降。因此，我们使用局部 CNN 对空间依赖建模。&lt;/p&gt;
&lt;p&gt;对于每个时段 $t$，我们将目标区域 $i$ 和它周围的邻居看作是 $S \times S$ 大小的图片，两个通道 $\mathbf{Y}_{i,t} \in \mathbb{R}^{S \times S \times 2}$。一个通道包含起始流量信息，另一个是结束流量信息。目标区域在图像的中间。局部 CNN 使用 $\mathbf{Y}_{i,t}$ 作为输入 $\mathbf{Y}^{(0)}_{i,t}$，每个卷积层的定义如下：&lt;/p&gt;
$$\tag{1}
\mathbf{Y}^{(k)}\_{i,t} = \text{ReLU}(\mathbf{W}^{(k)} \ast \mathbf{Y}^{(k-1)}\_{i,t} + \mathbf{b}^{(k)}),
$$&lt;p&gt;其中 $\mathbf{W}^{(k)}$ 和 $\mathbf{b}^{(k)}$ 是参数。堆叠 $K$ 层卷积后，用一个全连接来推测区域 $i$ 的空间表示，记为 $\mathbf{y}_{i,t}$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Short-term Temporal Dependency&lt;/strong&gt; 我们使用 LSTM 捕获空间序列依赖。我们使用原始版本的 LSTM：&lt;/p&gt;
$$\tag{2}
\mathbf{h}\_{i,t} = \text{LSTM}([\mathbf{y}\_{i,t};\mathbf{e}\_{i,t}], \mathbf{h}\_{i,t-1}),
$$&lt;p&gt;其中，$\mathbf{h}_{i,t}$ 是区域 $i$ 在时段 $t$ 的输出表示。$\mathbf{e}_{i,t}$ 表示外部因素。因此，$\mathbf{h}_{i,t}$ 包含空间和短期时间信息。&lt;/p&gt;
&lt;h2 id="spatial-dynamic-similarity-flow-gating-mechanism"&gt;Spatial Dynamic Similarity: Flow Gating Mechanism
&lt;/h2&gt;&lt;p&gt;局部 CNN 用于捕获空间依赖。CNN 通过局部连接和权重共享处理局部结构相似性。在局部 CNN 中，局部空间依赖依靠历史交通流量的相似度。然而，流量的空间依赖是平稳的，不能完全地反映目标区域和其邻居间的关系。一个直接的表示区域间关系的方式是交通流。如果两个区域间有很多流量，那么他们之间的关系强烈（也就是他们更相似）。交通流可以用于控制流量信息在区域间的转移。因此，我们设计了一个 Flow Gating Mechanism (FGM)，以层次的方式对动态空间依赖建模。&lt;/p&gt;
&lt;p&gt;类似局部 CNN，我们构建了局部空间流量图来保护流量的空间依赖。一个时段和一个区域相关的流量分两种，流入和流出，两个流量矩阵可以如上构建，每个元素表示对应区域的流入流出流量。图1(c) 给了一个流出矩阵。&lt;/p&gt;
&lt;p&gt;给定一个区域 $i$，我们获得过去 $l$ 个时段的相关的流量（即从 $t-l+1$ 到 $t$）。需要的流量矩阵拼接起来，表示为 $\mathbf{F}_{i,t} \in \mathbb{R}^{S \times S \times 2l}$，$2l$ 是流量矩阵的数量。因为堆叠的流量矩阵包含了过去与区域 $i$ 相关的矩阵，我们使用 CNN 对区域间的空间流量关系建模，将 $\mathbf{F}_{i,t}$ 作为输入 $\mathbf{F}^{(0)}_{i,t}$。对于每层 $k$，公式如下：&lt;/p&gt;
$$\tag{3}
\mathbf{F}^{(k)}\_{i,t} = \text{ReLU}(\mathbf{W}^{(k)}\_f \ast \mathbf{F}^{(k-1)}\_{i,t} + \mathbf{b}^{(k)}\_f),
$$&lt;p&gt;其中 $\mathbf{W}^{(k)}_f$ 和 $\mathbf{b}^{(k)}_f$ 是参数。&lt;/p&gt;
&lt;p&gt;每层，我们使用流量信息对区域间的动态相似性进行捕获，通过一个流量门限制空间信息。特别地，空间表示 $\mathbf{Y}^{i,k}_t$ 作为每层的输出，受流量门调整。我们重写式1为：&lt;/p&gt;
$$\tag{4}
\mathbf{Y}^{(k)}\_{i,t} = \text{ReLU}(\mathbf{W}^{(k)} \ast \mathbf{Y}^{(k-1)}\_{i,t} + \mathbf{b}^{(k)}) \otimes (\mathbf{F}^{i,k-1}\_t),
$$&lt;p&gt;$\otimes$ 是element-wise product。&lt;/p&gt;
&lt;p&gt;$K$ 个门控卷积层后，我们用一个全连接得到流量门控空间表示 $\mathbf{y}_{i,t}$。&lt;/p&gt;
&lt;p&gt;我们将式2中的空间表示 $\mathbf{y}_{i,t}$ 替换为 $\hat{\mathbf{y}}_{i,t}$。&lt;/p&gt;
&lt;h2 id="temporal-dynamic-similarity-periodically-shifted-attention-mechanism"&gt;Temporal Dynamic Similarity: Periodically Shifted Attention Mechanism
&lt;/h2&gt;&lt;p&gt;在上面的局部时空网络中，只有前几个时段用于预测。然而，这会忽略长期依赖（周期），但周期在时空预测问题中又很重要。这部分我们考虑长期依赖。&lt;/p&gt;
&lt;p&gt;训练 LSTM 来处理长期信息不是一个简单的任务，因为序列长度的增加导致梯度消失，因此会减弱周期性的影响。为了解决这个问题，预测目标的相对时段（即昨天的这个时候，前天这个时候）应该被建模。然而，单纯地融入相对时段是不充分的，会忽略周期的平移，即交通数据不是严格周期的。举个例子，周末的高峰通常发生在下午的后半段，不同的日子时间不一致，从4:30pm到6:00pm变化。周期的平移在交通序列中很普遍，因为事故或堵塞的发生。图 2a 和 2b 分别是不同的天和周的时间平移的例子。这两个时间序列是从 NYC 的出租车数据中算的从 Javits Center出发的流量。显然，交通序列是周期性的，但是这些序列的峰值（通过红圈标记）在不同的日子里时间不一样。此外，对比两张图，周期性不是严格按日或按周的。因此，我们设计了一个 Periodically Shifted Attention Mechanism (PSAM) 来解决问题。详细的方法如下。&lt;/p&gt;
&lt;p&gt;我们专注于解决日周期的平移问题。如图 1(a) 所示，从前 $P$ 天获得的相对时段用来处理周期依赖。对于每天，为了解决时间平移问题，我们从每天中额外的选择 $Q$ 个时段。举个例子，如果预测的时间是9:00-9:30pm，我们选之前的一个小时和之后的一个小时，即8:00-19:30pm，$\vert Q \vert = 5$。这些时段 $q \in Q$ 用来解决潜在的时间平移问题。此外，我们使用对每天 $p \in P$ 保护每天的序列信息，公式如下：&lt;/p&gt;
$$\tag{5}
\mathbf{h}^{p,q}\_{i,t} = \text{LSTM}([\mathbf{y}^{p,q}\_{i,t}; \mathbf{e}^{p,q}\_{i,t}], \mathbf{h}^{p,q-1}\_{i,t}),
$$&lt;p&gt;其中，$\mathbf{h}^{p,q}_{i,t}$ 是对于区域 $i$ 的预测时间 $t$，时段 $q$ 在前一天 $p$ 的表示。&lt;/p&gt;
&lt;p&gt;我们用了一个注意力机制捕获时间平移并且获得了前几天的每一天的一个表示。前几天每一天的表示 $\mathbf{h}^p_{i,t}$ 是时段 $q$ 每一个选中时间的带权加和，定义为：&lt;/p&gt;
$$\tag{6}
\mathbf{h}^p\_{i,t} = \sum\_{q \in Q} \alpha^{p,q}\_{i,t} \mathbf{h}^{p,q}\_{i,t},
$$&lt;p&gt;权重 $\alpha^{p,q}_{i,t}$ 衡量了在 $p \in P$ 这天时段 $q$ 的重要性。重要值 $\alpha^{p,q}_{i,t}$ 通过对比从短期记忆（式2）得到的时空表示和前一个隐藏状态 $\mathbf{h}^{p,q}_{i,t}$ 得到。权重定义为：&lt;/p&gt;
$$\tag{7}
\alpha^{p,q}\_{i,t} = \frac{\text{exp}(\text{score}(\mathbf{h}^{p,q}\_{i,t}, \mathbf{h}\_{i,t}))} {\sum\_{q \in Q} \text{exp}(\text{score} (\mathbf{h}^{p,q}\_{i,t}, \mathbf{h}\_{i,t})}
$$&lt;p&gt;类似 (Luong, Pham and Manning 2015)，注意力分数的定义可以看作是基于内容的函数：&lt;/p&gt;
$$\tag{8}
\text{score}(\mathbf{h}^{p,q}\_{i,t}, \mathbf{h}\_{i,t}) = \mathbf{v}^\text{T} \text{tanh} (\mathbf{W\_H} \mathbf{h}^{p,q}\_{i,t} + \mathbf{W\_X} \mathbf{h}\_{i,t} + \mathbf{b\_X}),
$$&lt;p&gt;其中，$\mathbf{W_H}, \mathbf{W_X}, \mathbf{b_X}, \mathbf{v}$ 是参数，$\mathbf{v}^\text{T}$ 是转置。对于前面的每一天 $p$，我们得到一个周期表示 $\mathbf{h}^p_{i,t}$。然后我们使用另一个 LSTM 用这些周期表示作为输入，保存序列信息，即&lt;/p&gt;
$$\tag{9}
\hat{\mathbf{h}}^p\_{i,t} = \text{LSTM}(\mathbf{h}^p\_{i,t}, \hat{\mathbf{h}}^{p-1}\_{i,t}).
$$&lt;p&gt;我们将最后一个时段的输出 $\hat{\mathbf{h}}^P_{i,t}$ 看作是时间动态相似度的表示（即长期周期信息）。&lt;/p&gt;
&lt;h2 id="joint-training"&gt;Joint Training
&lt;/h2&gt;&lt;p&gt;我们拼接把短期表示 $\mathbf{h}_{i,t}$ 和 长期表示 $\hat{\mathbf{h}}^P_{i,t}$ 拼接得到 $\mathbf{h}^c_{i,t}$，对于预测区域和时间来说既保留了短期依赖又保留了长期依赖。我们将 $\mathbf{h}^c_{i,t}$ 输入到全连接中，获得每个区域 $i$ 流入和流出流量的最终预测值，分别表示为 $y^i_{s,t+1}$ 和 $y^i_{e,t+1}$。最终预测函数定义为：&lt;/p&gt;
$$\tag{10}
[y^i\_{s,t+1}, y^i\_{e,t+1}] = \text{tanh}(\mathbf{W}\_{fa} \mathbf{h}^c\_{i,t} + \mathbf{b}\_{fa}),
$$&lt;p&gt;因为我们做了归一化，所以输出的范围在 $(-1, 1)$，输出值会映射回需求值。&lt;/p&gt;
&lt;p&gt;我们同时预测出发和到达流量，损失函数定义为：&lt;/p&gt;
$$\tag{11}
\mathcal{L} = \sum^n\_{i=1} \lambda (y^s\_{i,t+1} - \hat{y}^s\_{i, t+1})^2 + (1 - \lambda) (y^e\_{i,t+1} - \hat{y}^e\_{i, t+1})^2,
$$&lt;p&gt;$\lambda$ 用来平衡流入和流出的影响。区域 $i$ 在时间 $t+1$ 实际的流入和流出流量表示为 $\hat{y}^s_{i, t+1}, \hat{y}^e_{i, t+1}$。&lt;/p&gt;
&lt;h1 id="experiment"&gt;Experiment
&lt;/h1&gt;&lt;h2 id="experiment-settings"&gt;Experiment Settings
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Datasets&lt;/strong&gt; 我们在两个 NYC 的大型数据集上评价了模型。每个数据集包含旅行记录，详情如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NYC-Taxi：2015 年 22349490 个出租车的旅行记录，从 2015年1月1日到2015年3月1日。实验中，我们使用1月1日到2月10日作为训练，剩下20天测试。&lt;/li&gt;
&lt;li&gt;NYC-Bike：2016 年 NYC 自行车轨迹数据，7月1日到8月29日，包含了 2605648 条记录。前 40 天用来训练，后 20 天做测试。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Preprocessing&lt;/strong&gt; 我们将整个城市分成 $10 \times 20$ 个区域。每个区域大小是 $1km \times 1km$。时段长度设为 30min。使用最大最小归一化 volume 和 flow 到 $[0, 1]$。预测后，使用逆变换后的值评价。我们使用滑动窗采样。测试模型的时候，滤掉 volumn 小于 10 的样本，这是工业界和学界常用的技巧 (Yao et al. 2018)。因为真实数据集中，关注较小的交通数据没有太大的意义。我们选择 80% 的数据训练，20% 验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Evaluation Metric &amp;amp; Baselines&lt;/strong&gt; 两个指标：MAPE, RMSE。baselines：HA, ARIMA, Ridge, Lin-UOTD (Tong el al. 2017), XGBoost, MLP, ConvLSTM, DeepSD, ST-ResNet, DMVST-Net。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hyperparameter Settings&lt;/strong&gt; 我们基于验证集设定超参数。对于空间信息，64 个卷积核，$3 \times 3$ 大小。每个邻居的大小设定为 $7 \times 7$。层数 $K = 3$，对于 flow 考虑的时间跨度 $l = 2$。时间信息，短期 LSTM 长度为 7，长期周期信息 $\vert P \vert = 3$，周期平移注意力机制 $\vert Q \vert = 3$，LSTM 中隐藏表示的维度是 128。STDN 通过 Adam 优化，batch size 64，学习率 0.001。LSTM 中的 dropout 0.5。$\lambda$ 取 0.5。&lt;/p&gt;
&lt;h2 id="results"&gt;Results
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;PerformanceComparison&lt;/strong&gt; 表 1 展示了我们的方法对比其他方法在两个数据集上的结果。我们跑每个 baseline 10次，取了平均和标准差。此外，我们也做了 t 检验。我们的方法在两个数据集上指标都很好。&lt;/p&gt;</description></item><item><title>Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction</title><link>https://davidham3.github.io/blog/p/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/</link><pubDate>Fri, 08 Mar 2019 10:26:16 +0000</pubDate><guid>https://davidham3.github.io/blog/p/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/</guid><description>&lt;p&gt;AAAI 2017, ST-ResNet，网格流量预测，用三个相同结构的残差卷积神经网络对近邻时间、周期、趋势（远期）分别建模。与 RNN 相比，RNN 无法处理序列长度过大的序列。三组件的输出结果进行集成，然后和外部因素集成，得到预测结果。原文地址：&lt;a class="link" href="https://arxiv.org/abs/1610.00081" target="_blank" rel="noopener"
&gt;Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;对于交通管理和公共安全来说，预测人流很重要，但这个问题也很有挑战性，因为收到很多复杂的因素影响，如区域内的交通、事件、天气。我们提出了一个基于深度学习的模型 ST-ResNet，对城市内的每个区域的人流的进出一起预测。我们基于时空数据独一的属性，设计了一个端到端的结构。我们使用残差神经网络框架对时间近邻、周期、趋势属性建模。对每个属性，我们设计了残差卷积的一个分支，每个分支对人流的空间属性建模。ST-ResNet 基于数据动态地聚合三个残差神经网络的输出，给不同的分支和区域分配权重。聚合结果还融合了外部因素，像天气或日期。实验在北京和纽约两个数据集上开展。&lt;/p&gt;
&lt;h1 id="introduction"&gt;Introduction
&lt;/h1&gt;&lt;p&gt;对于交通管理和公共安全来说，预测人流很重要（Zheng et al. 2014）。举个例子，2015年新年夜，上海有大量人群涌入一个区域，导致 36 人死亡。2016年六月中旬，数百名 Pokemon Go 玩家冲入纽约中央公园，为了抓一只特别稀有的怪，导致严重的踩踏事故。如果可以预测一个区域的人流，这样的悲剧可以通过应急措施避免，像提前做交通管控，发布预警，疏散人群等。&lt;/p&gt;
&lt;p&gt;我们在这篇文章中预测两类人流（Zhang et al. 2016)：如图 1（a）所示，流入和流出。流入是在给定时间段，从其他区域进入到一个区域的交通运载量。流出表示给定时段内，从一个区域向其他区域的交通运载量。两个流量都是区域间的人口流动。了解这个对风险评估和交通管理有很大帮助。流入/流出可以通过行人数量、邻近道路车辆数、公共运输系统的人数、或是所有的都加起来。图 1（b）展示了一个例子。我们可以使用手机信号测量行人数，$r_2$ 的流入和流出分别为 3 和 1。类似地，使用车辆 GPS 轨迹，分别是 0 和 3。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;然而，同时预测城市每个区域人口的流入和流出是很有难度的，有 3 个复杂的因素：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;空间依赖。区域 $r_2$ 的流入（图1（a））受邻近区域（像 $r_1$）和遥远区域流出的影响。$r_2$ 的流出也受其他区域（$r_3$）流入的影响。$r_2$ 的流入也影响其自身。&lt;/li&gt;
&lt;li&gt;时间依赖。一个区域的人流受到近期和远期时间影响。举个例子，早上8点发生的交通拥堵可能会影响到 9 点。此外，早高峰的交通状况可能在接连的几天都是相似的，每 24 小时一次。而且随着冬天的到来，早高峰时间可能越来越晚。温度下降，日初变晚会使人们起床时间变晚。&lt;/li&gt;
&lt;li&gt;外部影响。一些像天气和事件的外部因素可能会显著地改变城市内不同区域的人口流动。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;为了解决这些问题，我们提出了一个深度深空残差网络 (ST-ResNet) 对每个区域的流入和流出同时预测。我们的贡献有 4 点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ST-ResNet 使用基于卷积的残差神经网络对城市内两个邻近的和遥远的区域的空间依赖建模，同时还确信了模型的预测精度不会因为模型的深度增加而降低。&lt;/li&gt;
&lt;li&gt;我们将人口流动的时间属性分为三种，时间近邻、周期、趋势。ST-ResNet 使用三个残差网络对这些属性建模。&lt;/li&gt;
&lt;li&gt;ST-ResNet 动态地聚合三个上述网络的输出，给不同的分支和区域分配权重。聚合还融合了外部因素。&lt;/li&gt;
&lt;li&gt;我们使用北京出租车的轨迹数据和气象数据，纽约自行车轨迹数据。结果表示我们的方法比 6 个 baseline 都好。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="preliminaries"&gt;Preliminaries
&lt;/h1&gt;&lt;p&gt;简要回顾人流预测问题（Zhang el al. 2016; Hoang, Zheng, and Singh 2016），介绍残差学习（He et al. 2016）。&lt;/p&gt;
&lt;h2 id="formulation-of-crowd-flows-problem"&gt;Formulation of Crowd Flows Problem
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Definition 1(Region (Zhang et al. 2016))&lt;/strong&gt; 根据不同粒度级和语义，一个地点的定义有很多。我们根据经纬度将城市划分成 $I \times J$ 个网格，一个网格表示一个区域，如图 2(a)。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/Fig2.JPG"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;p&gt;**Definition 2(Inflow/outflow (Zhang et al. 2016)) $\mathbb{P}$ 是第 t 时段的轨迹集合。对于第 $i$ 行第 $j$ 列的网格，时段 $t$ 流入和流出的人流分别定义为：&lt;/p&gt;
$$
x^{in,i,j}\_t = \sum\_{T\_r \in \mathbb{P}} \vert \lbrace k &gt; 1 \mid g\_{k-1} \not \in (i, j) \wedge g\_k \in (i,j) \rbrace \vert
\\
x^{out,i,j}\_t = \sum\_{T\_r \in \mathbb{P}} \vert \lbrace k \geq 1 \mid g\_k \in (i,j) \wedge g\_{k+1} \not \in (i,j) \rbrace \vert
$$&lt;p&gt;其中 $T_r: g_1 \rightarrow g_2 \rightarrow \cdots \rightarrow g_{\vert T_r \vert}$ 是 $\mathbb{P}$ 中的轨迹，$g_k$ 是地理坐标；$g_k \in (i,j)$ 表示点 $g_k$ 落在 $(i, j)$ 内；$\vert · \vert$ 表示集合基数。&lt;/p&gt;
&lt;p&gt;时段 $t$ ，所有区域的流入和流出可以表示成 $\mathbf{X}_t \in \mathbb{R}^{2 \times I \times J}$，$(\mathbf{X}_t)_{0,i,j}=x^{in,i,j}_t, (\mathbf{X}_t)_{1,i,j} = x^{out,i,j}_t$。流入矩阵如图2(b)。&lt;/p&gt;
&lt;p&gt;空间区域可以表达成一个 $I \times J$ 的区域，有两类流动，所以观测值可以表示为 $\mathbf{X} \in \mathbb{R}^{2 \times I \times J}$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem 1&lt;/strong&gt; 给定历史观测值 $\lbrace \mathbf{X}_t \mid t = 0,\dots,n-1 \rbrace$，预测 $\mathbf{X}_n$。&lt;/p&gt;
&lt;h2 id="deep-residual-learning"&gt;Deep Residual Learning
&lt;/h2&gt;$$\tag{1}
\mathbf{X}^{(l+1)} = \mathbf{X}^{(l)} + \mathcal{F}(\mathbf{X}^{(l)})
$$&lt;h1 id="deep-spatio-temporal-residual-networks"&gt;Deep Spatio-Temporal Residual Networks
&lt;/h1&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/Fig3.JPG"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;图 3 展示了 ST-ResNet的架构，4 个部分分别对时间近邻、周期、远期、外部因素建模。如图 3 所示，首先将流入和流出作为两个通道放到矩阵中，使用定义 1 和 2 引入的方法。我们将时间轴分为三个部分，表示近期时间、邻近历史、远期历史。三个时段的两通道的流动矩阵分别输入上述模型，对三种时间属性建模。这三个组件结构相同，都是残差网络。这样的结构捕获邻近和遥远区域间的空间依赖。外部组件中，我们手动的从数据集中提取了特征，如天气、事件等，放入两层全连接神经网络中。前三个组件的输出基于参数矩阵融合为 $\mathbf{X}_{Res}$，参数矩阵给不同的区域不同的组件分配权重。$\mathbf{X}_{Res}$ 然后与外部组件 $\mathbf{X}_{Ext}$ 集成。最后，聚合结果通过 Tanh 映射到 $[-1, 1]$，在反向传播会比 logistic function 收敛的更快 (LeCun et al. 2012)。&lt;/p&gt;
&lt;h2 id="structures-of-the-first-three-components"&gt;Structures of the First Three Components
&lt;/h2&gt;&lt;p&gt;如图 4。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/Fig4.JPG"
loading="lazy"
alt="Figure4"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Convolution&lt;/strong&gt;&lt;/em&gt; 一个城市通常很大，包含很多距离不同的区域。直观上来说，邻近区域的人流会影响其他区域，可以通过 CNN 有效地处理，CNN 也被证明在层级地捕获空间信息方面很强 (LeCun et al. 1998)。而且，如果两个遥远地方通过地铁或高速公路连接，那么这两个区域间就有依赖关系。为了捕获任何区域的空间依赖，我们需要设计一个很多层的 CNN 模型，因为一个卷积层只考虑空间近邻，受限于它卷积核的大小。同样的问题在视频序列生成任务中也有，当输入和输出有同样的分辨率的时候(Mathieu, Couprie, and LeCun 2015)。为了避免下采样导致的分辨率损失引入了几种方法，同时还保持遥远的依赖关系(Long, Shelhamer, and Darrell 2015)。与传统的 CNN 不同的是，我们没有使用下采样，而是只使用卷积 (Jain et al. 2007)。如图 4(a)，图中有 3 个多级的 feature map，通过一些卷积操作相连。一个高层次的结点依赖于 9 个中间层次的结点，这些又依赖于低层次的所有结点。这意味着一个卷积可以很自然地捕获空间近邻依赖，堆叠卷积可以更多地捕获遥远的空间依赖。&lt;/p&gt;
&lt;p&gt;图 3 的近邻组件使用了一些 2 通道流动矩阵对近邻时间依赖建模。令最近的部分为 $[\mathbf{X}_{t-l_c}, \mathbf{X}_{t-(l_c-1)}, \dots, \mathbf{X}_{t-1}]$，也称为近邻依赖序列。我们将他们沿第一个轴（时间）拼接，得到一个张量 $\mathbf{X}^{(0)}_c \in \mathbb{R}^{2l_c \times I \times J}$，然后使用卷积（图 3 中的 Conv1）：&lt;/p&gt;
$$\tag{2}
\mathbf{X}^{(1)}\_c = f(W^{(1)}\_c \ast \mathbf{X}^{(0)}\_c + b^{(1)}\_c)
$$&lt;p&gt;其中 $\ast$ 表示卷积；$f$ 是激活函数；$W^{(1)}_c, b^{(1)}_c$ 是参数。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Residual Unit.&lt;/strong&gt;&lt;/em&gt; 尽管有 ReLU 职业那个的激活函数和正则化技巧，深度卷积网络在训练上还是很难。但我们仍然需要深度神经网络捕获非常大范围的依赖。对于典型的流量数据，假设输入大小是 $32 \times 32$，卷积核大小是 $3 \times 3$，如果我们想对城市范围的依赖建模，至少需要连续 15 个卷积层。为了解决这个问题，我们使用残差学习(He et al. 2015)，在训练超过 1000 层的网络时很有效。&lt;/p&gt;
&lt;p&gt;在我们的 ST-ResNet(如图 3)，我们在 Conv1 上堆叠 $L$ 个残差单元如下：&lt;/p&gt;
$$\tag{3}
\mathbf{X}^{(l+1)}\_c = \mathbf{X}^{(l)}\_c + \mathcal{F}(\mathbf{X}^{(l)}\_c; \theta^{(l)}\_c), l = 1, \dots, L
$$&lt;p&gt;$\mathcal{F}$ 是残差函数，即 ReLU + Convolution，如图 4(b)。我们还在 ReLU 之前加了 &lt;em&gt;Batch Normalization&lt;/em&gt;。在第 $L$ 个残差单元前，我们使用了一个卷积层，图 3 中的 Conv2。2 个卷积和 $L$ 个残差单元，图 3 中的近邻组件的输出是 $\mathbf{X}^{(l+2)}_c$。&lt;/p&gt;
&lt;p&gt;同样的，使用上面的操作，我们可以构建 &lt;em&gt;周期&lt;/em&gt; 和 &lt;em&gt;趋势&lt;/em&gt; 组件，如图 3。假设时段 $p$ 有 $l_p$ 个时间间隔。那么 &lt;em&gt;时段&lt;/em&gt; 依赖序列是 $[\mathbf{X}_{t-l_p \cdot p}, \mathbf{X}_{t-(l_p - 1) \cdot p}, \dots, \mathbf{X}_{t-p}]$。使用式 2 和 式 3 那样的卷积和 $L$ 个残差单元，&lt;em&gt;周期&lt;/em&gt; 组件的输出是 $\mathbf{X}^{(L + 2)}_p$。同时，&lt;em&gt;趋势&lt;/em&gt; 组件的输出是 $\mathbf{X}^{(L+2)}_q$，输入是 $[\mathbf{X}_{t-l_q \cdot q}, \mathbf{X}_{t-(l_q - 1) \cdot q}, \dots, \mathbf{X}_{t-q}]$，$l_q$ 是&lt;em&gt;趋势&lt;/em&gt;依赖序列的长度，$q$ 是趋势跨度。需要注意的是 $p$ 和 $q$ 是两个不同类型的周期。在实际的实现中，$p$ 等于一天，描述的是日周期，$q$ 是一周，表示周级别的趋势。&lt;/p&gt;
&lt;h2 id="the-structure-of-the-external-component"&gt;The Structure of the External Component
&lt;/h2&gt;&lt;p&gt;交通流会被很多复杂的外部因素所影响，如天气或事件。图 5(a) 表示假期（春节）时的人流和平时的人流很不一样。图 5(b) 表示相比上周的同一天，突然而来的大雨会减少此时办公区域的人流。令 $E_t$ 为特征向量，表示预测的时段 $t$ 的外部因素。我们的实现中，我们主要考虑天气、假期事件、元数据（工作日、周末）。详细情况见表 1。为了预测时段 $t$ 的交通流，假期事件和元数据可以直接获得。然而，未来时段 $t$ 的天气预报不知道。可以使用时段 $t$ 的天气预报，或是 $t-1$ 时段的天气来近似。我们在 $E_t$ 上堆叠两个全连接层，第一层可以看作是每个子因素的嵌入层。第二层用来从低维映射到和 $\mathbf{X}_t$ 一样的高维上。图 3 中外部组件的输出表示为 $\mathbf{X}_{Ext}$，参数是$\theta_{Ext}$。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/Fig5.JPG"
loading="lazy"
alt="Figure5"
&gt;&lt;/p&gt;
&lt;h2 id="fusion"&gt;Fusion
&lt;/h2&gt;&lt;p&gt;我们先用一个参数矩阵融合前三个组件，然后融合外部组件。&lt;/p&gt;
&lt;p&gt;图6(a)和(d)展示了表1展示的北京轨迹数据的比例曲线，x轴是两个时段的时间差，y轴是任意两个有相同时间差的流入的平均比例。两个不同区域的曲线在时间序列上表现出了时间联系，也就是近期的流入比远期的流入更相关，表现出了事件近邻性。两条曲线有两个不同的形状，表现出不同区域可能有不同性质的近邻性。图6(b)和(e)描绘了7天所有时段的流入。我们可以观察到两个区域明显的日周期性。在办公区域，工作日的峰值比周末的高很多。住宅区在工作日和周末有相似的峰值。图6(c)和(f)描述了2015年3月到2015年6月一个特定时段(9:00pm-9:30pm)的流入。随着时间的推移，办公区域的流入逐渐减少，住宅区逐渐增加。不同的区域表现出了不同的趋势。总的来说，两个区域的流入受到近邻、周期、趋势三部分影响，但是影响程度是不同的。我们也发现其他区域也有同样的性质。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/Fig6.JPG"
loading="lazy"
alt="Figure6"
&gt;&lt;/p&gt;
&lt;p&gt;综上，不同区域受近邻、周期、趋势的影响，但是影响程度不同。受这些观察的启发，我们提出了一个基于矩阵参数的融合方法。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Parametric-matrix-based fusion.&lt;/strong&gt;&lt;/em&gt; 我们融合图 3 中前三个组件：&lt;/p&gt;
$$\tag{4}
\mathbf{X}\_{Res} = \mathbf{W}\_c \odot \mathbf{X}^{(L+2)}\_c + \mathbf{W}\_p \odot \mathbf{X}^{(L+2)}\_p + \mathbf{W}\_q \odot \mathbf{X}^{(L+2)}\_q
$$&lt;p&gt;$\odot$ 是哈达玛乘积，$\mathbf{W}$ 是参数，分别调整三个组件的影响程度。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Fusing the external component.&lt;/strong&gt;&lt;/em&gt; 我们直接地将前三个组件的输出和外部组件融合，如图3。最后，时段 $t$ 的预测值，表示为 $\hat{\mathbf{X}}_t$ 定义为：&lt;/p&gt;
$$\tag{5}
\hat{\mathbf{X}}\_t = \mathrm{tanh}(\mathbf{X}\_{Res} + \mathbf{X}\_{Ext})
$$&lt;p&gt;我们的 ST-ResNet 可以从三个流动与朕和外部因素特征通过最下滑 MSE 来训练：&lt;/p&gt;
$$\tag{6}
\mathcal{L}(\theta) = \Vert \mathbf{X}\_t - \hat{\mathbf{X}}\_t \Vert^2\_2
$$&lt;h2 id="algorithm-and-optimization"&gt;Algorithm and Optimization
&lt;/h2&gt;&lt;p&gt;算法1描述了 ST-ResNet 的训练过程。首先从原始序列构造训练实例。然后通过反向传播，用 Adam 算法训练。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/Alg1.JPG"
loading="lazy"
alt="Algorithm1"
&gt;&lt;/p&gt;
&lt;h1 id="experiments"&gt;Experiments
&lt;/h1&gt;&lt;h2 id="settings"&gt;Settings
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Datasets.&lt;/strong&gt; 我们使用表 1 中展示的两个数据集。每个数据集都包含两个子集，轨迹和天气。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TaxiBJ: 轨迹数据是出租车 GPS 数据和北京的气象数据，2013年7月1日到10月30日，2014年5月1日到6月30日，2015年5月1日到6月30日，2015年11月1日到2016年4月1日。使用定义2，我们获得两类人流。我们选择最后四周作为测试集，之前的都为训练集。&lt;/li&gt;
&lt;li&gt;BikeNYC: 轨迹数据是2014年NYC Bike系统中取的，从4月1日到9月30日。旅行数据包含：持续时间、起点终点站点ID，起始终止时间。在数据中，最后10天选做测试集，其他选做训练集。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/Table1.JPG"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Baselines.&lt;/strong&gt; 我们对比了6个baselines：HA, ARIMA, SARIMA, VAR, ST-ANN, DeepST.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/Table2.JPG"
loading="lazy"
alt="Table2"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/Table3.JPG"
loading="lazy"
alt="Table3"
&gt;&lt;/p&gt;</description></item><item><title>T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction</title><link>https://davidham3.github.io/blog/p/t-gcn-a-temporal-graph-convolutional-network-for-traffic-prediction/</link><pubDate>Thu, 07 Mar 2019 09:03:16 +0000</pubDate><guid>https://davidham3.github.io/blog/p/t-gcn-a-temporal-graph-convolutional-network-for-traffic-prediction/</guid><description>&lt;p&gt;T-GCN，arxiv上面的一篇文章，用 GCN 对空间建模，GRU 对时间建模，很简单的模型。没有对比近几年的图卷积在时空数据挖掘中的模型。原文地址：&lt;a class="link" href="https://arxiv.org/abs/1811.05320" target="_blank" rel="noopener"
&gt;T-GCN: A Temporal Graph ConvolutionalNetwork for Traffic Prediction&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;精确和实时的交通预测在智能交通系统中扮演着重要的角色，对城市交通规划、交通管理、交通控制起着重要的作用。然而，交通预测由于其受限于城市路网且随时间动态变化，即有着空间依赖与时间依赖，早已成为一个公开的科学研究问题。为了同时捕获空间和时间依赖，我们提出了一个新的神经网络方法，时间图卷积网络模型 （T-GCN），将图卷积和门控循环单元融合起来。GCN 用来学习复杂的拓扑结构来捕获空间依赖，门控循环单元学习交通数据的动态变化来捕获时间依赖。实验表明我们的 T-GCN 模型比之前的方法要好。我们的 tf 实现：&lt;a class="link" href="https://github.com/lehaifeng/T-GCN" target="_blank" rel="noopener"
&gt;代码仓库地址&lt;/a&gt;。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 Introduction
&lt;/h1&gt;&lt;p&gt;随着智能交通系统的发展，交通预测受到了越来越多的关注。交通预测是高级交通管理系统中的关键部分，是实现交通规划、交通管理、交通控制的重要部分。交通预测是分析城市路网上交通状况、包括流量、车速、密度，挖掘交通模式，对路网上交通进行预测的一个过程。交通预测不仅能给管理者提供科学依据来预测交通拥挤并提前限制出行，还可以给旅客提供适当的出行路线并提高交通效率。然而，交通由于其空间和时间的依赖至今还是一个有难度的挑战：&lt;/p&gt;
&lt;p&gt;（1）空间依赖。流量的改变主要受路网的拓扑结构控制。上游道路的交通状态通过转移影响下游的道路，下游的交通状态会通过反馈影响上游的状态。如图 1 所示，由于邻近道路的强烈影响，短期相似性从状态 1 （上游与中游相似）转移到 状态 2（上游与下游相似）。&lt;/p&gt;
&lt;p&gt;（2）时间依赖。流量随时间动态改变，主要会出现周期性和趋势。如图 2（a）所示，路 1 的流量在一周内展示出了周期性变化。图 2（b）中，一天的流量也发生变换；举个例子，流量会被其前一时刻或更前的时刻的交通状况所影响。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/t-gcn-a-temporal-graph-convolutional-network-for-traffic-prediction/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;
&lt;img src="https://davidham3.github.io/blog/images/t-gcn-a-temporal-graph-convolutional-network-for-traffic-prediction/Fig2.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;有很多交通预测方法，一些考虑时间依赖，包括 ARIMA，Kalman filtering model, SVR, knn, Beyesian model, partial neural network model.上述方法考虑交通状况在时间上的动态变化，忽略了空间依赖，导致不能精确预测。为了更好地刻画空间特征，一些研究引入了卷积神经网络对空间建模；然而，卷积适用于欧氏空间的数据，如图像、网格等。这样的模型不能在城市路网这样有着复杂拓扑结构的环境下工作，所以他们不能描述空间依赖。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，我们提出了新的交通预测方法，时间图卷积网络 （T-GCN），用于对基于城市路网的交通预测任务。我们的贡献有三点：&lt;/p&gt;
&lt;p&gt;（1） 我们提出的模型结合了 GCN 和 GRU，图卷积捕获路网的拓扑结构做空间建模，GRU 捕获路网上交通数据的时间依赖。T-GCN 模型可以用于其他时空预测任务上。&lt;/p&gt;
&lt;p&gt;（2） T-GCN 的预测结果比其他的方法好，表明我们的 T-GCN 模型不仅可以做短期预测，也可以做长期预测。&lt;/p&gt;
&lt;p&gt;（3）我们使用深圳市罗湖区的出租车速度数据和洛杉矶线圈数据。结果表明我们的预测误差比所有的 baseline 小了 1.5%到57.8%，表明 T-GCN 在交通预测上的优越性。&lt;/p&gt;
&lt;h1 id="2-related-work"&gt;2 Related work
&lt;/h1&gt;&lt;p&gt;智能交通系统交通预测是现在的一个重要研究问题。现存的方法分两类：模型驱动的方法和数据驱动的方法。首先，模型驱动的方法主要解释交通流量、速度、密度的瞬时性和平稳性。这样的方法需要基于先验知识的系统建模。代表方法包括排队论模型，细胞传递模型，交通速度模型，microscopic fundamental diagram model 等等。实际中，交通数据受多种因素影响，很难获得一个精准的交通模型。现存的模型不能精确地描述复杂的现实环境中的交通数据的变化。此外，这些模型的构建需要很强的计算能力，而且很容易收到交通扰乱和采样点空间等问题的影响。&lt;/p&gt;
&lt;p&gt;数据驱动的方法基于数据的规律性，从统计学推测变化局势，然后用于预测。这类方法不分析物理性质和交通系统的动态行为，有很高的灵活性。早期的方法包括历史均值模型，使用历史周期的交通流量均值作为预测值。这个方法不需要假设，计算简单而且还快，但是不能有效地拟合时间特征，预测的精准度低。随着研究的深入，很多高精度的方法涌现出来，主要分为参数模型和非参数模型。&lt;/p&gt;
&lt;p&gt;参数模型提前假设回归函数，参数通过对原始数据处理得到，基于回归函数对交通流预测。时间序列模型，线性回归模型，Kalman filtering model 是常用的方法。时间序列模型将观测到的时间序列拟合进一个模型，然后用来预测。早在 1976 年，Box and Jenkins 提出了 ARIMA，Hamed 等人使用 ARIMA 预测城市内的交通流量。为了提高模型的精度，不同的变体相继被提出，Kohonen ARIMA，subset ARIMA，seasonal ARIMA 等等。Lippi 等人对比支持向量回归和 seasonal ARIM，发现 SARIMA 模型在交通拥堵上的预测有更好的结果。线性回归模型基于历史数据构建模型来预测。2004 年，Sun 等人使用 local linear model 解决了区间预测，在真实数据集上获得了较好的效果。Kalman filtering model 基于前一时刻和当前时刻的交通状态预测未来的状态。1984 年，Okutani 等人使用 Kalman filtering 理论建立了交通流状态预测模型。后续，一些研究使用 Kalman filtering 模型解决交通预测任务。&lt;/p&gt;
&lt;p&gt;传统的参数模型算法简单，计算方便。然而，这些方法依赖平稳假设，不能反映交通数据的非线性和不确定性，也不能克服交通事件这种随机性事件。非参数模型很好地解决这些问题，只需要足够的历史信息能自动地从中学到统计规律即可。常见的非参数模型包括：k近邻，支持向量回归，Fuzzy Logic 模型等。&lt;/p&gt;
&lt;p&gt;近些年，随着深度学习的快速发展，深度神经网络可以捕获交通数据的动态特征，获得很好的效果。根据是否考虑空间依赖，模型可以划分成两类。一些方法只考虑时间依赖，如 Park 等人使用 FNN 预测交通流。Huang 等人使用深度置信网络 DBN 和回归模型在多个数据集上证明可以捕获交通数据中的随机特征，提升预测精度。此外，RNN 及其变体 LSTM, GRU 可以有效地使用自循环机制，他们可以很好地学习到时间依赖并获得更好的预测结果。&lt;/p&gt;
&lt;p&gt;这些模型考虑时间特征但是忽略空间依赖，所以交通数据的变化不受城市路网的限制，因此他们不能精确的预测路上的交通状态。解决交通预测问题的关键是充分利用空间和时间依赖。为了更好的刻画空间特征，很多研究已经在这个基础上进行了提升。Lv 等人提出了一个 SAE 模型从交通数据中捕获时空特征，实现短期交通流的预测。Zhang 等人提出了一个叫 ST-ResNet 的模型，基于人口流动的时间近邻、周期和趋势这些特征设计了残差卷积网络，然后三个网络和外部因素动态地聚合起来，预测城市内每个区域人口的流入和流出。Wu 等人设计了一个特征融合架构通过融合 CNN 和 LSTM 进行短期预测。一个一维的 CNN 用于捕获空间依赖，两个 LSTM 用来挖掘交通流的短期变化和周期性。Cao 等人提出一个叫 ITRCN 的端到端模型，将交互的网络交通转换为图像，使用 CNN 捕获交通的交互式功能，用 GRU 提取时间特征，预测误差比 GRU 和 CNN 分别高了 14.3% 和 13.0%。Ke 等人提出一个新的深度学习方法叫融合卷积长短时记忆网络（FCL-Net），考虑空间依赖、时间依赖，以及异质依赖，用于短期乘客需求预测。Yu 等人用深度卷积神经网络捕获空间依赖，用 LSTM 捕获时间动态性，在北京交通网络数据上展示出了 SRCN 的优越性。&lt;/p&gt;
&lt;p&gt;尽管上述方法引入了 CNN 对空间依赖建模，在交通预测任务上有很大的进步，但 CNN 本质上只适用于欧氏空间，在有着复杂拓扑结构的交通网络上不能刻画空间依赖。因此，这类方法有缺陷。近些年，图卷积网络的发展，可以用来捕获图网络的结构特征，提供更好的解决方案。Li 等人提出了 DCRNN 模型，通过图上的随机游走捕获空间特征，通过编码解码结构捕获时间特征。&lt;/p&gt;
&lt;p&gt;基于这个背景，我们提出了新的神经网络方法捕获复杂的时空特征，可以用于基于城市路网的交通预测任务上。&lt;/p&gt;
&lt;h1 id="3-methodology"&gt;3 Methodology
&lt;/h1&gt;&lt;h2 id="31-problem-definition"&gt;3.1 Problem Definition
&lt;/h2&gt;&lt;p&gt;目标是基于历史信息预测未来。我们的方法中，交通信息是一个通用的概念，可以是速度、流量、密度。我们在实验的时候将交通信息看作是速度。&lt;/p&gt;
&lt;p&gt;定义1：路网 $G$。我们用图 $G = (V, E)$ 描述路网的拓扑结构，每条路是一个顶点，$V$ 顶点集，$V = \lbrace v_1, v_2, \dots, v_N \rbrace$，$N$ 是顶点数，$E$ 是边集。邻接矩阵 $A$ 表示路的关系，$A \in R^{N \times N}$。邻接矩阵只有 0 和 1。如果路之间有连接就为 1， 否则为 0。&lt;/p&gt;
&lt;p&gt;定义2：特征矩阵 $X^{N \times P}$。我们将交通信息看作是顶点的特征。$P$ 表示特征数，$X_t \in R^{N \times i}$ 用来表示时刻 $i$ 每条路上的速度。&lt;/p&gt;
&lt;p&gt;时空交通预测的问题可以看作学习一个映射函数：&lt;/p&gt;
$$\tag{1}
[X\_{t+1}, \dots, X\_{t+T}] = f(G; (X\_{t-n}, \dots, X\_{t-1}, X\_t))
$$&lt;p&gt;$n$ 是历史时间序列的长度，$T$ 是需要预测的长度。&lt;/p&gt;
&lt;h2 id="32-overview"&gt;3.2 Overview
&lt;/h2&gt;&lt;p&gt;T-GCN 模型有两个部分：GCN 和 GRU。图 3 所示，我们使用历史 $n$ 个时刻的时间序列数据作为输入，图卷积网络捕获路网拓扑结构获取空间依赖。然后将带有空间特征的时间序列放入 GRU 中，通过信息在单元间的传递捕获动态变化，获得时间特征。最后，将结果送入全连接层。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/t-gcn-a-temporal-graph-convolutional-network-for-traffic-prediction/Fig3.JPG"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;h2 id="33-methodology"&gt;3.3 Methodology
&lt;/h2&gt;&lt;h3 id="331-spatial-dependence-modeling"&gt;3.3.1 Spatial Dependence Modeling
&lt;/h3&gt;&lt;p&gt;获取复杂的空间依赖在交通预测中是一个关键问题。传统的 CNN 只能用于欧氏空间。城市路网不是网格，CNN 不能反映复杂的拓扑结构。GCN 可以处理图结构，已经广泛应用到文档分类、半监督学习、图像分类中。GCN 在傅里叶域中构建滤波器，作用在顶点及其一阶邻居上，捕获顶点间的空间特征，可以通过堆叠构建 GCN 模型。如图 4 所示，假设顶点 1 是中心道路，GCN 模型可以获取中心道路和它周围道路的拓扑关系，将这个结构和道路属性编码，获得空间依赖。总之，我们用 GCN 模型从交通数据中学习空间特征。两层 GCN 表示为：&lt;/p&gt;
$$\tag{2}
f(X, A) = \sigma(\hat{A} Relu(\hat{A} X W\_0) W\_1)
$$&lt;p&gt;$\hat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$ 表示预处理，$\tilde{A} = A + I_N$ 表示加了自连接的邻接矩阵。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/t-gcn-a-temporal-graph-convolutional-network-for-traffic-prediction/Fig4.JPG"
loading="lazy"
alt="Figure4"
&gt;&lt;/p&gt;
&lt;h3 id="332-temporal-dependence-modeling"&gt;3.3.2 Temporal Dependence Modeling
&lt;/h3&gt;&lt;p&gt;因为 GRU 比 LSTM 参数少，训练快，我们使用 GRU 获取交通数据的时间依赖。如图 5 所示，$h_{t-1}$ 表示时刻 $t-1$ 的隐藏状态；$x_t$ 表示时刻 $t$ 的交通信息；$r_t$ 表示重置门，用来控制忽略前一时刻信息的程度；$u_t$ 是更新门，用来控制将信息从上一时刻拿到这个时刻的程度；$c_t$ 是时刻 $t$ 的记忆内容；$h_t$ 是时刻 $t$ 的输出状态。GRU 通过将时刻 $t-1$ 的隐藏状态和当前时刻的交通信息作为输入，获取时刻 $t$ 的交通状态。在捕获当前时刻的交通信息的时候，模型仍保留着历史信息，且有能力捕获时间依赖。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/t-gcn-a-temporal-graph-convolutional-network-for-traffic-prediction/Fig5.JPG"
loading="lazy"
alt="Figure5"
&gt;&lt;/p&gt;
&lt;h3 id="333-temporal-graph-convolutional-network"&gt;3.3.3 Temporal Graph Convolutional Network
&lt;/h3&gt;&lt;p&gt;为了同时从交通数据中捕获时空依赖，我们提出了时间图卷极网络（T-GCN）。如图6所示，左侧是时空交通预测的过程，右侧是一个 T-GCN 细胞的结构，$h_{t-1}$ 表示 $t-1$ 时刻的输出，GC 是图卷积过程，$u_t, r_t$ 是时刻 $t$ 的更新门和重置门，$h_t$ 表示时刻 $t$ 的输出。计算过程如下。$f(A, X_t)$ 表示图卷积过程，如式 2 定义。$W$ 和 $b$ 表示训练过程的权重与偏置。&lt;/p&gt;
$$\tag{3}
u\_t = \sigma(W\_u[f(A, X\_t), h\_{t-1}] + b\_u)
$$$$\tag{4}
r\_t = \sigma(W\_r[f(A, X\_t), h\_{t-1}] + b\_r)
$$$$\tag{5}
c\_t = tanh(W\_c[f(A, X\_t), (r\_t \ast h\_{t-1})] + b\_c)
$$$$\tag{6}
h\_t = u\_t \ast h\_{t-1} + (1 - u\_t) \ast c\_t
$$&lt;p&gt;总之，T-GCN 能处理复杂的空间依赖和时间动态性。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/t-gcn-a-temporal-graph-convolutional-network-for-traffic-prediction/Fig6.JPG"
loading="lazy"
alt="Figure6"
&gt;&lt;/p&gt;
&lt;h3 id="334-loss-function"&gt;3.3.4 Loss Function
&lt;/h3&gt;&lt;p&gt;损失函数如式 7。第一项用来减小速度的误差。第二项 $L_{reg}$ 是一个 $L2$ 正则项，避免过拟合，$\lambda$ 是超参。&lt;/p&gt;
$$\tag{7}
loss = \Vert Y\_t - \hat{Y}\_t \Vert + \lambda L\_{reg}
$$&lt;h1 id="4-experiments"&gt;4 Experiments
&lt;/h1&gt;&lt;h2 id="41-data-description"&gt;4.1 Data Description
&lt;/h2&gt;&lt;p&gt;两个数据集，深圳出租车和洛杉矶线圈。两个数据集都和车速有关。&lt;/p&gt;
&lt;p&gt;（1）SZ-taxi。数据是2015年1月1日到1月31日的深圳出租车轨迹数据。我们选了罗湖区 156 个主要路段作为研究区域。实验数据主要有两部分。一个是 156 * 156 的邻接矩阵，另一个是特征矩阵，描述了速度随时间的变化。我们将速度以 15 分钟为单位聚合。&lt;/p&gt;
&lt;p&gt;（2）Los-loop。数据集是洛杉矶县高速公路线圈的实时数据。我们选了 207 个监测器，数据是 2012年5月1日到5月7日的数据。我们以5分钟为单位聚合车速。数据也是一个邻接矩阵和一个特征矩阵。我们用线性插值填补了缺失值。&lt;/p&gt;
&lt;p&gt;我们将输入数据归一化到 $[0, 1]$。此外，80% 的数据用来训练，20% 用来测试。我们预测未来15、30、45、60分钟的车速。&lt;/p&gt;
&lt;h2 id="42-evaluation-metrics"&gt;4.2 Evaluation Metrics
&lt;/h2&gt;&lt;p&gt;（1）RMSE:&lt;/p&gt;
$$\tag{8}
RMSE = \sqrt{\frac{1}{n} \sum^n\_{i=1} (Y\_t - \hat{Y}\_t)^2}
$$&lt;p&gt;（2）MAE:&lt;/p&gt;
$$\tag{9}
MAE = \frac{1}{n} \sum^n\_{i=1} \vert Y\_t - \hat{Y}\_t \vert
$$&lt;p&gt;（3）Accuracy:&lt;/p&gt;
$$\tag{10}
Accuracy = 1 - \frac{\Vert Y - \hat{Y} \Vert}{\Vert Y \Vert\_F}
$$&lt;p&gt;（4）Coefficient of Determination (R2):&lt;/p&gt;
$$\tag{11}
R^2 = 1 - \frac{\sum\_{i=1} (Y\_t - \hat{Y}\_t)^2}{\sum\_{i=1}(Y\_t - \bar{Y})^2}
$$&lt;p&gt;（5）Explained Variance Score(Var):&lt;/p&gt;
$$
var = 1 - \frac{Var\lbrace Y - \hat{Y}\rbrace}{Var\lbrace Y\rbrace}
$$&lt;p&gt;RMSE 和 MAE 用来评估预测误差：越小越好。精度衡量预测的精度：越大越好。$R^2$ 和 Var 计算相关系数，评估预测结果表达真实数据的能力，越大越好。&lt;/p&gt;
&lt;h2 id="43-model-parameters-designing"&gt;4.3 Model Parameters Designing
&lt;/h2&gt;&lt;p&gt;(1) Hyperparameter&lt;/p&gt;
&lt;p&gt;学习率、batch size、训练论述，隐藏层数。我们设定的是学习率0.001，batch size 64，轮数 3000 轮。&lt;/p&gt;
&lt;p&gt;隐层单元数对 T-GCN 来说是个重要的参数，因为不同的单元数可能会影响预测精度。我们通过实验选取了最优的隐藏单元数。&lt;/p&gt;
&lt;p&gt;看不下去了。。。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/t-gcn-a-temporal-graph-convolutional-network-for-traffic-prediction/Fig7.JPG"
loading="lazy"
alt="Figure7"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/t-gcn-a-temporal-graph-convolutional-network-for-traffic-prediction/Table1.JPG"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;</description></item><item><title>Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting</title><link>https://davidham3.github.io/blog/p/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/</link><pubDate>Thu, 28 Feb 2019 21:12:58 +0000</pubDate><guid>https://davidham3.github.io/blog/p/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/</guid><description>&lt;p&gt;AAAI 2019，滴滴的网约车需求预测，5个点预测1个点。空间依赖建模上：以图的形式表示数据，从空间地理关系、区域功能相似度、区域交通连通性三个角度构造了三个不同的图，提出了多图卷积，分别用 k 阶 ChebNet 对每个图做图卷积，然后将多个图的卷积结果进行聚合(sum, average 等)成一个图；时间依赖建模上：提出了融合背景信息的 Contextual Gated RNN (CGRNN)，用 ChebNet 对每个结点卷积后，得到他们的邻居表示，即每个结点的背景信息表示，与原结点特征拼接，用一个两层全连接神经网络计算出 T 个权重，将权重乘到历史 T 个时刻的图上，对历史值进行缩放，然后用一个共享的 RNN，针对每个结点形成的长度为 T 的时间序列建模，得到每个结点新的时间表示。最后预测每个点的网约车需求。原文地址：&lt;a class="link" href="http://www-scf.usc.edu/~yaguang/papers/aaai19_multi_graph_convolution.pdf" target="_blank" rel="noopener"
&gt;Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;区域级别的预测是网约车服务的关键任务。精确地对网约车需求预测可以指导车辆调度、提高车辆利用率，减少用户的等待时间，减轻交通拥堵。这个任务的关键在于区域间复杂的时空依赖关系。现存的方法主要关注临近区域的欧式关系建模，但是在距离较远的区域间组成的非欧式关系对精确预测也很关键。我们提出了 &lt;em&gt;spatiotemporal multi-graph convolution network&lt;/em&gt; (ST-MGCN)。我们首先将非欧的关系对编码到多个图中，然后使用 multi-graph convolution 对他们建模。为了在时间建模上利用全局的背景信息，我们提出了 &lt;em&gt;contextual gated recurrent neural network&lt;/em&gt;，用一个注意背景的门机制对不同的历史观测值重新分配权重。在两个数据集上比当前的 state-of-the-art 强 10%。&lt;/p&gt;
&lt;h1 id="introduction"&gt;Introduction
&lt;/h1&gt;&lt;p&gt;我们研究的问题是区域级别网约车需求预测，是智能运输系统的重要部分。目标是通过历史观测值，预测一个城市里面各区域未来的需求。任务的挑战是复杂的时空关系。一方面，不同区域有着复杂的依赖关系。举个例子，一个区域的需求通常受其空间上临近的区域所影响，同时与有着相同背景的较远的区域有联系。另一方面，非线性的依赖关系也存在于不同的时间观测值之间。预测一个时刻通常和多个历史的观测值相关，比如一小时前、一天前、甚至一周前。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;最近在深度学习的进步使得对基于区域级别的时空关系预测有了很好的结果。使用卷积神经网络和循环神经网络，得到了很多非常好的效果(Shi et al. 2015; Yu et al. 2017; Shi et al. 2017; Zhang, Zheng, and Qi 2017; Zhang et al. 2018a; Ma et al. 2017; Yao et al. 2018b; 2018a)。尽管有了很好的效果，但是我们认为在对时空关系建模上有两点被忽略了。其一，这些方法主要对不同区域的欧式关系建模，但是我们发现非欧关系很重要。图 1 是一个例子，对于区域 1，以及邻居区域 2，可能和很远的区域 3 有相似的功能，也就是他们都靠近学校和医院。此外，区域 1 还可能被区域 4 影响，区域 4 是通过高速公路直接与区域 1 相连的。其二：这些方法中，在使用 RNN 对时间关系建模时，每个区域是独立处理的，或者只基于局部信息。然而，我们认为全局和背景信息也很重要。举个例子，网约车需求的一个全局性的增长/减小通常表明一些可能会影响未来需求的活动发生了。&lt;/p&gt;
&lt;p&gt;我们提出了 ST-MGCN 解决这些问题。在 ST-MGCN 中，我们提出了将区域间非欧关系编码进多个图的方法。不同于 Yao et al. 2018b 给每个区域使用图嵌入作为额外的不变特征，我们用图卷积对区域间的关系对直接建模。图卷积在预测的时候可以聚合邻居特征，传统的图嵌入难以做到这一点。此外，在对时间关系建模时，为了聚合全局的背景信息，我们提出了 contextual gated recurrent neural network (CGRNN)。通过一个基于全局信息计算的门机制增强 RNN，对不同时间步的观测值重新赋权重。我们在两个大型的真实数据集上做了测试，ST-MGCN 比 baselines 好了一大截。我们主要的贡献是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;识别了网约车需求预测中的非欧关系，将他们编码进多个图。利用多图卷积对这些关系建模。&lt;/li&gt;
&lt;li&gt;对时间依赖，提出了 Contextual Gated RNN (CGRNN) 来集成全局背景信息。&lt;/li&gt;
&lt;li&gt;在两个大型真实数据集上做了实验，提出的方法比 state-of-the-art 在相对误差上小了 10%.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="related-work"&gt;Related work
&lt;/h1&gt;&lt;h2 id="spatiotemporal-prediction-in-urban-computing"&gt;Spatiotemporal prediction in urban computing
&lt;/h2&gt;&lt;p&gt;时空预测是数据驱动的城市管理的基础问题。有很多关于这方面的工作，自行车流量预测(Zhang, Zheng, and Qi 2017)，出租车需求(Ke et al. 2017b; Yao et al. 2018b)，到达时间(Li et al. 2018b)，降雨量(Shi et al. 2015; 2017)，对矩形区域的聚合值进行预测，区域关系通过地理距离建模。具体来讲，城市数据的空间结构通过矩阵形式表示，每个元素表示一个矩形区域。在之前的工作中，区域和他们的关系对一般表示成欧式结构，使得卷积神经网络可以有效地利用这个结构来预测。&lt;/p&gt;
&lt;p&gt;非欧结构的数据也存在于城市计算。通常，基于站点或点的预测任务，像流量预测(Li et al. 2018c; Yu, Yin, and Zhu 2018; Yao et al. 2018a)，基于点的出租车需求预测(Tong et al. 2017)以及基于站点的自行车流量预测(Chai, Wang, and Yang 2018)是很自然的非欧结构，数据不再是矩阵形式，卷积神经网络也不那么有效了。人工定制的特征工程或图卷积网络是处理非欧结构数据目前最好的方法。不同于之前的工作，ST-MGCN 将区域间的关系对编码进语义图中。尽管 ST-MGCN 是对基于区域的预测设计的，但是区域关系的非规整性使得它实际是对非欧数据进行预测。&lt;/p&gt;
&lt;p&gt;在 (Yao et al. 2018b)，作者提出 DMVST-Net，将区域间关系编码进图中来预测出租车需求。DMVST-Net 主要使用图嵌入作为额外特征来预测，没有使用相关区域的需求值（目标值）。在 (Yao et al. 2018a) 的工作中，作者通过注意力机制对周期性的平移问题建模提升了性能。但是，这些方法都没有直接对区域间的非欧关系建模。我们的工作中，ST-MGCN 使用提出的多图卷积从相关区域聚合特征，从不同角度的相关区域的预测值中做预测。&lt;/p&gt;
&lt;p&gt;最近在对帕金森的神经图像分析 (Zhang et al. 2018b) 的研究中，图卷积在空间特征提取上很有效。他们使用 GCN 从最相似的区域中学习特征，提出了多视图结构融合了不同的 MRI。然而，上述工作没有考虑时间依赖。ST-GCN 用于基于骨骼的动作识别(Li et al. 2018a; Yan, Xiong, and Lin 2018)。ST-GCN 的变换是一个空间依赖和局部时间循环的组合。然而，我们认为这些模型，在时间依赖建模上，背景信息或全局信息被忽略了。&lt;/p&gt;
&lt;h2 id="graph-convolution-network"&gt;Graph convolution network
&lt;/h2&gt;&lt;p&gt;图卷积网络定义在图 $\mathcal{G} = (V, \boldsymbol{A})$ 上，$V$ 是顶点集，$\boldsymbol{A} \in \mathbb{R}^{\vert V \vert \times \vert V \vert}$ 是邻接矩阵，元素表示顶点间是否相连。GCN 可以用不同的感受野从不同的非欧结构中提取局部特征(Hammond et al. 2011)。令 $\boldsymbol{L} = \boldsymbol{I} - \boldsymbol{D}^{-1/2} \boldsymbol{A} \boldsymbol{D}^{-1/2}$ 表示图拉普拉斯矩阵，$\boldsymbol{D}$ 是度矩阵，图卷积操作 (Defferrard, Bresson, and Vandergheynst 2016) 定义为：&lt;/p&gt;
$$
\boldsymbol{X}\_{l+1} = \sigma (\sum^{K-1}\_{k=0} \alpha\_k \boldsymbol{L}^k \boldsymbol{X}\_l),
$$&lt;p&gt;$\boldsymbol{X}_l$ 表示第 $l$ 层的特征，$\alpha_k$ 表示可学习的参数，$\boldsymbol{L}^k$ 是图拉普拉斯矩阵的 $k$ 次幂，$\sigma$ 是激活函数。&lt;/p&gt;
&lt;h2 id="channel-wise-attention"&gt;Channel-wise attention
&lt;/h2&gt;&lt;p&gt;Channel-wise attention (Hu, Shen, and Sun 2018; Chen et al. 2017) 在 cv 的论文中提出。本质是给每个通道学习一个权重，为了找到最重要的帧，然后基于他们更高的权重。$\boldsymbol{X} \in \mathbb{R}^{W \times H \times C}$ 表示输入，$W$ 和 $H$ 是输入图像的维度，$C$ 表示通道数，channel-wise attention 计算方式如下：&lt;/p&gt;
$$\tag{1}
z\_c = F\_{pool}(\boldsymbol{X}\_{:,:,c}) = \frac{1}{WH} \sum^W\_{i=0} \sum^H\_{j=0} X\_{i,j,c} \quad \text{for} c=1,2,\dots,C \\
\boldsymbol{s} = \sigma(\boldsymbol{W}\_2 \delta (\boldsymbol{W}\_1 \boldsymbol{z})) \\
\tilde{\boldsymbol{X}}\_{:,:,c} = \boldsymbol{X}\_{:,:,c} \circ s\_c \quad \text{for} c=1,2,\dots,C
$$&lt;p&gt;$F_{pool}$ 是全局池化操作，把每个通道聚合成一个标量 $\boldsymbol{z}_c$，$c$ 是通道的下标。用一个注意力机制对聚合的向量 $\boldsymbol{z}$ 使用非线性变换生成自适应的通道权重 $\boldsymbol{s}$，$\boldsymbol{W}_1, \boldsymbol{W}_2$ 是对应的权重，$\delta, \sigma$ 是 ReLU 和 sigmoid 激活函数。$\boldsymbol{s}$ 通过矩阵乘法乘到输入上。最后，输入通道基于学习到的权重得到了缩放。我们使用这个方法，针对一系列的图生成了时间依赖的注意力分数。&lt;/p&gt;
&lt;h1 id="methodology"&gt;Methodology
&lt;/h1&gt;&lt;h2 id="region-level-ride-hailing-demand-forecasting"&gt;Region-level ride-hailing demand forecasting
&lt;/h2&gt;&lt;p&gt;我们将城市分为相同大小的网格，每个格子定义为一个区域 $v \in V$，$V$ 表示城市内所有不相交的区域。$\boldsymbol{X}^{(t)}$ 表示第 $t$ 个时段所有区域的订单。&lt;em&gt;区域级别的网约车需求预测&lt;/em&gt; 问题定义为：给定一个定长的输入，对单个时间步进行时空预测，也就是学习一个函数 $f: \mathbb{R}^{\vert V \vert \times T} \rightarrow \mathbb{R}^{\vert V \vert}$，将所有区域的历史需求映射到下一个时间步上。&lt;/p&gt;
$$
[\boldsymbol{X}^{(t-T+1)}, \dots, \boldsymbol{X}^{(t)}]
$$&lt;p&gt;&lt;strong&gt;Framework overview&lt;/strong&gt; ST-MGCN 的系统架构如图2。我们从不同的角度表示区域间的关系，顶点表示区域，边对区域间的关系编码。首先，我们使用提出的 CGRNN，考虑全局背景信息对不同时间的观测值进行聚合。然后，使用多图卷积捕获区域间不同类型的关系。最后，使用全连接神经网络将特征映射到预测上。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Fig2.JPG"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;h2 id="spatial-dependency-modeling"&gt;Spatial dependency modeling
&lt;/h2&gt;&lt;p&gt;我们用图将区域间关系建模成三种类型，（1）邻居图 $\mathcal{G}_N = (V, \boldsymbol{A}_N)$，编码了空间相近程度，（2）功能相似度图 $\mathcal{G}_F = (V, \boldsymbol{A}_F)$，编码了区域的 POI 的相似度，（3）连接图 $\mathcal{G}_T = (V, \boldsymbol{A}_T)$，编码了距离较远的区域的连通性。我们的方法可以轻易地扩展到其他的图上。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Neighborhood&lt;/strong&gt; 区域的邻居基于空间近邻程度定义。我们将 $3 \times 3$ 区域中的最中间的那个区域与他邻接的 8 个区域相连。&lt;/p&gt;
$$\tag{3}
A\_{N, ij} = \begin{cases}
1, \quad v\_i \quad \text{and} \quad v\_j \quad \text{are} \quad \text{adjacent}\\
0, \quad \text{otherwise}
\end{cases}
$$&lt;p&gt;&lt;strong&gt;Functional similarity&lt;/strong&gt; 对一个区域做预测的时候，很自然的会想到和这个区域在功能上相似的区域会有帮助。区域功能可以由 POI 刻画，两个顶点间的边定义为 POI 的相似度：&lt;/p&gt;
$$\tag{3}
A\_{S,i,j} = \text{sim}(P\_{v\_i}, P\_{v\_j}) \in [0, 1]
$$&lt;p&gt;其中 $P_{v_i}, P_{v_j}$ 是区域 $v_i$ 和 $v_j$ 的 POI 向量，维度等于 POI 种类的个数，每个分量表示这个区域内这个 POI 类型的数量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transportation connectivity&lt;/strong&gt; 运输系统也是一个重要因素。一般来说，这些空间距离上相距较远但是可以很方便到达的区域可以关联起来。这种连接包含高速公路、公路、地铁这样的公共运输。我们定义：如果两个区域间通过这些路直接相连，那么他们之间有边：&lt;/p&gt;
$$\tag{4}
A\_{C,i,j} = max(0, \text{conn}(v\_i, v\_j) - A\_{N,i,j}) \in \lbrace 0, 1\rbrace
$$&lt;p&gt;$\text{conn}(u, v)$ 表示 $v_i$ 和 $v_j$ 之间的连通性。邻居的边在这个图中移除掉了，减少冗余的关系，所以这个图最后是一个稀疏图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multi-graph convolution for spatial dependency modeling&lt;/strong&gt; 有了这些图，我们提出了多图卷积对空间关系建模：&lt;/p&gt;
$$\tag{5}
\boldsymbol{X}\_{l+1} = \sigma(\bigsqcup\_{\mathbf{A} \in \mathbb{A}} f(\mathbf{A; \theta\_i}) \boldsymbol{X}\_l \mathbf{W}\_l)
$$&lt;p&gt;其中 $\boldsymbol{X}_l \in \mathbb{R}^{\vert V \vert \times P_l}, \boldsymbol{X}_{l+1} \in \mathbb{R}^{\vert V \vert \times P_{l+1}}$ 是第 $l$ 和 $l+1$ 层的特征向量，$\sigma$ 是激活函数，$\bigsqcup$ 表示聚合函数，如 sum, max, average etc. $\mathbb{A}$ 表示图的集合，$f(\mathbf{A}; \theta_i) \in \mathbb{R}^{\vert V \vert \times \vert V \vert}$ 表示参数为 $\theta_i$ 的基于图 $\mathbf{A} \in \mathbb{A}$ 的不同样本组成的矩阵的聚合值，$\mathbf{W}_l \in \mathbb{R}^{P_l \times P_{l+1}}$ 表示特征变换矩阵，举个例子，如果 $f(\mathbf{A}, \theta_i)$ 是拉普拉斯矩阵 $\mathbf{L}$ 的多项式，那么这就是多图上的 ChebNet。如果是 $\mathbf{I}$，那就是全连接神经网络。&lt;/p&gt;
&lt;p&gt;我们实现的是 $K$ 阶 拉普拉斯 $\mathbf{L}$ 多项式，图 3 是一个中心区域通过图卷积层变换后的例子。假设邻接矩阵中的值不是 0 就是 1，$L^k_{ij} \not = 0$ 表示 $v_i$ 在 $k$ 步内可达 $v_j$。根据卷积操作，$k$ 是空间特征提取时的感受野范围。使用图 1 的道路连通性图 $\mathcal{G}_C = (V, \boldsymbol{A}_C)$ 来说明。在邻接矩阵 $\boldsymbol{A}_C$ 中，我们有：&lt;/p&gt;
$$
A\_{C,1,4} = 1; A\_{C,1,6} = 0; A\_{C,4,6} = 1,
$$&lt;p&gt;在 1 度拉普拉斯矩阵中对应的分量是：&lt;/p&gt;
$$
L^1\_{C,1,4} \not = 0; L^1\_{C,1,6} = 0; L^1\_{C,4,6} \not = 0
$$&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Fig3.JPG"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;如果拉普拉斯矩阵的最大度数 $K$ 设为 $1$，那么区域 1 变换的特征向量，即 $\boldsymbol{X}_{l+1, 1,:}$ 不会包含区域 6: $\boldsymbol{X}_{l,6,:}$，因为 $L^1_{C,1,6}=0$。当 $K$ 增大到 2 的时候，对应的元素 $L^2_{C,1,6}$ 变成非零，$\boldsymbol{X}_{l+1,1,:}$ 就可以利用 $\boldsymbol{X}_{l,6,:}$ 的信息了。&lt;/p&gt;
&lt;p&gt;基于多图卷积的空间依赖建模不限于上述三种图，可以轻易地扩展到其他的图上，适用于其他的时空预测问题上。多图卷积对区域间的关系进行特征提取。感受野小的时候，专注于近邻的区域。增大拉普拉斯阶数，或者堆叠卷积层可以增加感受野的范围，鼓励模型捕获全局依赖关系。&lt;/p&gt;
&lt;p&gt;图嵌入是另一种对区域间关系建模的方法。在 DMVST-Net (Yao et al. 2018b)，作者使用图嵌入表示区域间关系，然后将嵌入作为额外特征加到每个区域上。我们认为 ST-MGCN 中的空间依赖建模方法比之前的方法好，因为：ST-MGCN 将区域间关系编码到图中，通过图卷积从相关区域聚合需求值。但是在 DMVST-Net 中区域关系是嵌入到一个基于区域的不随时间变化的特征中，作为的模型的输入，&lt;/p&gt;
&lt;p&gt;尽管 DMVST-Net 也捕获了拓扑结构信息，但是它很难从相关的区域中通过区域关系聚合需求值。而且不变的特征对模型训练的贡献有限。&lt;/p&gt;
&lt;h2 id="temporal-correlation-modeling"&gt;Temporal correlation modeling
&lt;/h2&gt;&lt;p&gt;我们提出 Contextual Gated Recurrent Neural Network (CGRNN) 对不同时间步上的样本建模。CGRNN 通过使用一个上下文注意的门机制增强 RNN 将背景信息集成到时间建模中，结构如图 4。假设我们有 $T$ 个观测样本，$\boldsymbol{X}^{(t)} \in \mathbb{R}^{\vert V \vert \times P}$ 表示第 $t$ 个样本，$P$ 是特征数，如果特征只包含订单数，那就是 1。上下文门控机制如下：&lt;/p&gt;
$$tag{6}
\hat{\boldsymbol{X}}^{(t)} = [\boldsymbol{X}^{(t)}, F^{K'}\_\mathcal{G}(\boldsymbol{X}^{(t)})] \quad \text{for} \quad t = 1,2,\dots,T
$$&lt;p&gt;首先，上下文门控机制通过将临近区域的历史信息和当前区域拼接，得到了区域的描述信息。从相邻区域来的信息看作是环境信息，通过图卷积 $F^{K&amp;rsquo;}_\mathcal{G}$ 使用最大阶数为 $K&amp;rsquo;$ 的拉普拉斯矩阵提取。上下文门控机制用来用图卷积操作集成临近区域的信息，然后使用一个池化：&lt;/p&gt;
$$\tag{7}
z^{(t)} = F\_{pool}(\hat{\boldsymbol{X}}^{(t)}) = \frac{1}{\vert V \vert} \sum^{\vert V \vert}\_{i=1} \hat{X}^{(t)}\_{i,:} \quad \text{for} \quad t=1,2,\dots,T
$$&lt;p&gt;然后，我们在所有的区域上使用全局平均池化 $F_{pool}$ 生成每个时间步观测值的平均值。&lt;/p&gt;
$$tag{8}
\boldsymbol{s} = \sigma(\boldsymbol{W}\_2 \delta(\boldsymbol{W}\_1) \boldsymbol{z})
$$&lt;p&gt;然后使用一个注意力机制，$\boldsymbol{W}_1, \boldsymbol{W}_2$ 是参数，$\delta, \sigma$ 分别是 ReLU 和 sigmoid 激活。&lt;/p&gt;
$$\tag{9}
\tilde{\boldsymbol{X}^{(t)}} = \boldsymbol{X}^{(t)} \circ s^{(t)} \quad \text{for} \quad t=1,2,\dots,T
$$&lt;p&gt;最后，$\boldsymbol{s}$ 用来对每个时间样本进行缩放：&lt;/p&gt;
$$tag{10}
\boldsymbol{H}\_{i,:} = \text{RNN}(\tilde{\boldsymbol{X}}^{(1)}\_{i,:}, \dots, \tilde{\boldsymbol{X}}^{(T)}\_{i,:}; \boldsymbol{W}\_3) \quad \text{for} \quad i=1,\dots,\vert V \vert
$$&lt;p&gt;在上下文门控之后，使用一个共享的 RNN 对所有的区域进行计算，将每个区域聚合成单独的向量 $\boldsymbol{H}_{i,:}$。使用共享 RNN 的原因是我们想找到一个对所有区域通用的聚合规则，这个规则鼓励模型泛化且减少模型的复杂度。&lt;/p&gt;
&lt;h1 id="experiments"&gt;Experiments
&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt; 北京和上海。时间是从2017年5月1日到2017年12月31日。5月1日到7月31日训练、8月1日到9月30日验证，剩下的测试。POI 数据是2017年的，包含13个类别。每个区域和一个 POI 向量相关，分量是这个 POI 类型在这个区域的个数。用来评估运输可达性的路网使用的是 OpenStreetMap (Haklay and Weber 2008)。&lt;/p&gt;
&lt;h2 id="experimental-settings"&gt;Experimental Settings
&lt;/h2&gt;&lt;p&gt;学习任务是：$f: \mathbb{R}^{\vert V \vert \times T} \rightarrow \mathbb{R}^{\vert V \vert}$。实验中，我们将区域以 $1km \times 1km$ 的大小划分成网格。北京和上海分别 1296 和 896 个区域。就像 Zhang, Zheng, and Qi 2017 做的那样，网络的输入包含 5 个历史观测值，三个最近邻的部分，1个周期部分，一个最新的趋势部分。在构建运输可达性网络的时候，我们考虑了高速公路、公路、地铁。两个区域间只要有这样的路直接相连就认为是连通的。&lt;/p&gt;
&lt;p&gt;$f(\mathbf{A}; \theta_i)$ 选择的是 $K = 2$ 时的切比雪夫多项式，$\bigsqcup$ 是 sum 函数。隐藏层为3，每层 64 个隐藏单元，L2 正则，weight decay 是 $1e-4$。CGRNN 中的图卷积 $K&amp;rsquo;$ 是 1。&lt;/p&gt;
&lt;p&gt;我们使用 ReLU 作为图卷积的激活函数。ST-MGCN 的学习率是 $2e-3$，使用验证集上的早停。所有的算法都用 tf 实现，adam 优化 RMSE。ST-MGCN 训练时用了 10G 内存，9G GPU 显存。在 Tesla P40 单卡上训练了一个半小时。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Methods for evaluation&lt;/strong&gt; HA, LASSO, Ridge, Auto-regressive model(VAR, STAR), Gradient boosted machine (GBM), ST-ResNet (Zhang, Zheng and Qi 2017), DMVST-Net (Yao et al. 2018b), DCRNN, ST-GCN。&lt;/p&gt;
&lt;h2 id="performance-comparison"&gt;Performance comparison
&lt;/h2&gt;&lt;p&gt;我们在验证集上用网格搜索调整了所有模型的参数，在测试集上跑了多次得到的最后的结果。我们使用 RMSE 和 MAPE 作为评价指标。表 1 展示了不同方法在 10 次以上的预测中的对比结果。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Table1.JPG"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;
&lt;p&gt;我们在两个数据集上观测到了几个现象：（1）基于深度学习的方法能够对非线性的时空依赖关系建模，比其他的方法好；（2）ST-MGCN 在两个数据集上比其他的方法都好，比第二好的高出 10%；（3）对比其他的深度学习方法，ST-MGCN 的方差更小。&lt;/p&gt;
&lt;h2 id="effect-of-spatial-dependency-modeling"&gt;Effect of spatial dependency modeling
&lt;/h2&gt;&lt;p&gt;为了研究空间和时间依赖建模的效果，我们通过减少模型中的组成部分评估了 ST-MGCN 的几个变体，包括：（1）邻居图，（2）功能相似性图，（3）运输连通性图。结果如表 2 所示。移除任何一个图都会造成性能损失，证明了每种关系的重要性。这些图编码了重要的先验知识，也就是区域间的相关性。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Table2.JPG"
loading="lazy"
alt="Table2"
&gt;&lt;/p&gt;
&lt;p&gt;为了评估集成多个区域关系的效果，我们扩展了基于单个图的模型，包括 DCRNN 和 STGCN，分别记为 DCRNN+ 和 ST-GCN+。结果如图 3，两个算法都得到了提升。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Table3.JPG"
loading="lazy"
alt="Table3"
&gt;&lt;/p&gt;
&lt;h2 id="effect-of-temporal-dependency-modeling"&gt;Effect of temporal dependency modeling
&lt;/h2&gt;&lt;p&gt;我们使用不同的方法对时间建模，评估 ST-GCN 对时间关系建模的效果。（1）平均池化：通过平均池化对历史观测值进行聚合，（2）RNN：使用 RNN 对历史观测值聚合，（3）CG：使用上下文门对不同的历史观测值赋权，不适用 RNN，（4）GRNN：不用图卷积的 CGRNN。结果如表 4。我们观察到了以下现象：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;平均池化会盲目地平均不同的样本，导致性能下降，能做上下文依赖非线性时间聚合的 RNN 能显著地提升性能。&lt;/li&gt;
&lt;li&gt;CGRNN 增强了 RNN。移除 RNN 和 图卷积都导致性能下降，证明了每个部件的有效性。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Table4.JPG"
loading="lazy"
alt="Table4"
&gt;&lt;/p&gt;
&lt;h2 id="effect-of-model-parameters"&gt;Effect of model parameters
&lt;/h2&gt;&lt;p&gt;我们调整了两个最重要的参数来看不同参数对模型的影响，$K$ 和图卷积层数。图 5 展示了测试集上的结果。可以观察到随着层数的增加，错误先降后增。但是随着 $K$ 的增加，错误是先减小，后不变。越大的 $K$ 或层数使得模型能捕获全局关联性，代价是模型的复杂度会增加，更易过拟合。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Fig5.JPG"
loading="lazy"
alt="Figure5"
&gt;&lt;/p&gt;
&lt;h1 id="conclusion-and-future-work"&gt;Conclusion and Future work
&lt;/h1&gt;&lt;p&gt;我们研究的是网约车需求预测，要找寻这个问题唯一的时空依赖关系。我们提出的深度学习模型使用多个图对区域间的非欧关系建模，使用多图卷积明显的捕获了这个关系。然后用上下文门控机制增强了 RNN，在时间建模上集成了全局背景信息。在两个大型真实数据集上评估了模型，比 state-of-the-art好。未来的工作是：（1）在其他的时空预测任务上评估模型；（2）将提出的模型扩展到多步预测上。&lt;/p&gt;</description></item><item><title>Multistep Speed Prediction on Traffic Networks: A Graph Convolutional Sequence-to-Sequence Learning Approach with Attention Mechanism</title><link>https://davidham3.github.io/blog/p/multistep-speed-prediction-on-traffic-networks-a-graph-convolutional-sequence-to-sequence-learning-approach-with-attention-mechanism/</link><pubDate>Mon, 21 Jan 2019 15:17:52 +0000</pubDate><guid>https://davidham3.github.io/blog/p/multistep-speed-prediction-on-traffic-networks-a-graph-convolutional-sequence-to-sequence-learning-approach-with-attention-mechanism/</guid><description>&lt;p&gt;AGC-Seq2Seq，投的是TRC。清华大学和高德地图合作的一项研究。作者采用了 GCN + Seq2Seq + Attention 的混合模型，将路网中的边构建成图中的结点，在 GCN 上做了改进，将邻接矩阵扩展到 k 阶并与一个权重矩阵相乘，类似 HA-GCN(2016)，实现了邻居信息聚合时权重的自由调整，可以处理有向图。时间关系上使用 Seq2Seq + Attention 建模，完成了北京市二环线的多步的车速预测，对比的方法中没有近几年出现的时空预测模型。&lt;/p&gt;
&lt;h1 id="摘要"&gt;摘要
&lt;/h1&gt;&lt;p&gt;为了在多步交通预测的任务中，捕获复杂的非平稳的时间动态性和空间依赖关系，我们提出了一个叫注意力图卷积序列到序列模型（AGC-Seq2Seq）。空间和时间用图卷积和序列到序列模型分开建模。注意力机制用来解决序列到序列模型在多步预测上的困难，同时来捕获交通流的异质性。&lt;/p&gt;
&lt;h1 id="2-literature-review"&gt;2. LITERATURE REVIEW
&lt;/h1&gt;&lt;p&gt;如 Li et al. 2017 所述，统计模型、shallow machine learning models 和 深度学习模型是三个主要的方法。&lt;/p&gt;
&lt;p&gt;统计模型基于过去的时间序列观测值对未来进行预测。ARIMA 模型，Kalman filter，还有它们衍生出的算法。然而，简单的时间序列模型通常依赖平稳假设，这与城市交通的动态性不符。特别是对于多步时间预测，后面的预测值是基于前面的预测值的，因此，预测的误差会逐渐的传播。使用简单的时间序列预测模型很难满足高精度的预测需求。&lt;/p&gt;
&lt;p&gt;同时，机器学习方法在交通预测研究中表现的很好。神经网络模型，贝叶斯网络，支持向量机模型，K 近邻摩西那个，随机森林模型在交通流预测中表现的很好。然而，机器学习算法的表现依赖于手工选取特征，而且选取特征的方法是不存在的，因为关键特征一般因问题而异。因此，使用元素级别的机器学习方法在复杂的预测任务上不会产生好的效果。&lt;/p&gt;
&lt;p&gt;最近，深度学习算法成功的应用在计算机科学中，同时，它在运输学科也吸引了很多人的注意。Huang et al. 2014 使用深度置信网络用于无监督学习，证明了在交通流预测上的有效性。Lv et al. 2015 使用一个堆叠的自编码器模型学习交通流特征。Ma et al 2015 使用 LSTM 有效地捕获了交通流的动态性。Polson and Sokolov 2017 融合了 $L_1$ 正则和 $\text{tanh}$ 激活的多层网络来检测交通流的极端的非线性。然而，这些方法主要聚焦于对单个序列建模，不能反映交通网络的空间关系。&lt;/p&gt;
&lt;p&gt;卷积神经网络提供了一个有效的架构来提取大尺度、高维的数据集中有效的统计模式。在学习局部平稳结构中，CNN 的能力在图像和视频识别任务中获得了很大的突破。在运输领域，也有学者使用 CNN 捕获交通网络上的空间关系。Ma et al. (2017) 提出了一个预测车速的深度卷积神经网络，将交通的时空动态性转换成图像。Wang et al. (2017) 将高速公路处理成一个 band image，提出了误差回传的循环卷积神经网络结构用于连续的交通速度预测。Ke et al. (2017) 将城市区域划分成均匀的网格，通过将卷积和 LSTM 层合并来预测每个网格内的乘客需求。上述的研究将交通网络转换为网格是因为 CNN 受限于处理欧氏空间的数据。然而，在交通预测上，路网上的时间序列是分布在一个拓扑图上连续的序列，是一种非欧式结构数据的典型 (Narang et al., 2013)；原本的 CNN 结构是不能使用的。为了解决这个问题，基于谱图理论的图卷积网络 (GCN) 可以用于在非欧式空间上使用卷积 (Kipf and Welling, 2016)。几个刚刚发表的研究在交通预测上使用了图卷积模型。基于谱的图卷积和时间上的卷积相结合 (Yu et al., 2017)，还有图卷积与循环神经网络 (RNN) 的结合 (Li et al., 2017) 来用于预测交通状态。之后，Cui et al. (2018) 使用高阶图卷积来学习路网上不同路段间的交互关系。上述研究没有在路网上直接定义图卷积，而是通过高斯核根据任意两个监测器间的距离构建了监测器之间的网络。此外，交通状况的时间关联也没有考虑。&lt;/p&gt;
&lt;p&gt;总结一下，城市路网上交通状况的变化展示出了时空的依赖性。我们提出了一个定制版的深度学习框架，在 Seq2Seq 框架中继承了注意力机制和图卷积模型，同时捕获复杂的非平稳的空间动态性和多步交通预测的空间依赖性。&lt;/p&gt;
&lt;h1 id="3-agc-seq2seq-deep-learning-framework"&gt;3. AGC-SEQ2SEQ DEEP LEARNING FRAMEWORK
&lt;/h1&gt;&lt;h2 id="31-preliminaries"&gt;3.1 Preliminaries
&lt;/h2&gt;&lt;p&gt;&lt;em&gt;(1) Road network topology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;路网根据驾驶方向构建成有向图 $\mathcal{G}(\mathcal{N}, \mathcal{L})$ ，顶点集 $\mathcal{N}$ 表示路口 (监测器或选择的高速公路的划分点)，边集 $\mathcal{L}$ 表示路段，如图1所示。$\boldsymbol{A}$ 是边集的邻接矩阵，$\boldsymbol{A}(i, j)$ 表示边 $i$ 和 $j$ 是否相连，即&lt;/p&gt;
$$
\boldsymbol{A}(i, j) = \begin{cases}
1, &amp;\text{if } \quad l\_i \quad \text{and} \quad l\_j \quad \text{are} \quad \text{connected} \quad \text{along} \quad \text{driving} \quad \text{direction}\\
0, &amp;\text{if } \quad \text{otherwise}
\end{cases}
$$&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/multistep-speed-prediction-on-traffic-networks-a-graph-convolutional-sequence-to-sequence-learning-approach-with-attention-mechanism/Fig1.jpg"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(2) Traffic speed&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;路段 $l_i (\forall l_i \in \mathcal{L})$ 的第 $t$ 个时段（比如 5 分钟）定义为路段上这个时间段浮动车的平均速度，表示为 $v^i_t$。路网在第 $t$ 个时段的速度定义为向量 $\boldsymbol{V}_t \in \mathbb{R}^{\vert \mathcal{L} \vert}$（$\vert \mathcal{L} \vert$ 是边集 $\mathcal{L}$ 的基数），第 $i$ 个元素是 $(\boldsymbol{V}_t)_i = v^i_t$。&lt;/p&gt;
&lt;p&gt;作为典型的时间序列预测问题，最近邻的 $m$ 步观测值可以对多步预测提供有价值的信息。除了实时的车速信息，一些外部变量，如时间、工作日还是周末，历史的统计信息也对预测有帮助。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(3) Time-of-day and weekday-or-weekend&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;因为路段的车速是聚合 5 分钟得到的平均值，时间会被转化为一个有序的实数，比如 00:00-00:05 转化为 $N_t = 1$，7:00-7:05 转化为 $N_t = 85(7 * 12 + 1)$，工作日或周末表示为 $p_t$，区分工作日和周末的不同特性。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(4) Historical statistic information&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;交通状态的每日趋势可以通过引入历史的统计数据捕获。历史的平均车速，中值车速，最大车速，最小车速，路段 $l_i$ 的 $t$ 时段的标准差，分别定义为训练集中的平均值、中位数、最大、最小、标准差，表示为 $v^i_{t,average}, v^i_{t,median}, v^i_{t,max}, v^i_{t,min}, d^i_t$。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(5) Problem formulation&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;车速预测是用之前观测到的速度预测一个确定时段每个路段上的车速。多步速度预测问题定义为：&lt;/p&gt;
$$\tag{1}
\hat{V}\_{t+n} = \mathop{\arg\max}\limits\_{V\_{t+n}} \text{Pr}(V\_{t+n} \mid V\_t, V\_{t-1}, \dots, V\_{t-m};\mathcal{G})
$$&lt;p&gt;其中 $\hat{V}_{t+n}(n=1,2,3,\dots)$ 表示第 $n$ 步的预测速度，$\lbrace V_t, V_{t-1}, \dots, V_{t-m} \mid m=1,2,\dots \rbrace$ 是之前观测到的值。$\text{Pr}(·\mid·)$ 是条件概率。&lt;/p&gt;
&lt;h2 id="32-graph-convolution-on-traffic-networks"&gt;3.2 Graph Convolution on Traffic Networks
&lt;/h2&gt;&lt;p&gt;图卷积通过谱域，将传统的卷积从网格上扩展到了图上。为了引入一般的 $K$ 阶图卷积，我们首先给每个路段 $l_i \in \mathcal{L}$ 定义了 $K$ 阶邻居 $\mathcal{H}_i(K) = \lbrace l_j \in \mathcal{L} \mid d(l_i, l_j) \leq K \rbrace$，其中 $d(l_i, l_j)$ 表示所有从 $l_i$ 到 $l_j$ 的路径中最短路径的长度。&lt;/p&gt;
&lt;p&gt;邻接矩阵就是一阶邻居，$K$ 次幂就是 $K$ 阶邻居。为了模仿拉普拉斯矩阵，我们在对角线上加了1，定义为：&lt;/p&gt;
$$\tag{2}
\boldsymbol{A}^K\_{GC} = \text{Ci}(\boldsymbol{A}^K + \boldsymbol{I})
$$&lt;p&gt;其中 $\text{Ci}(·)$ 是clip function，将非0元素变成1；因此 $\boldsymbol{A}^K_{GC}(i, j) = 1 \quad for \quad l_j \in \mathcal{H}_i(K) \quad or \quad i = j$；否则 $\boldsymbol{A}^K_{GC}(i, j) = 0$。单位阵 $\boldsymbol{I}$ 增加了自连接，卷积的时候可以考虑到自身。&lt;/p&gt;
&lt;p&gt;基于上述的邻居矩阵，一个简单版本的图卷积(e.g., Cui et al., 2018)可以定义为：&lt;/p&gt;
$$\tag{3}
\boldsymbol{V}\_t(K) = (\boldsymbol{W}\_{GC} \odot \boldsymbol{A}^K\_{GC})\cdot \boldsymbol{V}\_t
$$&lt;p&gt;其中 $\boldsymbol{W}_{GC}$ 是一个和 $\boldsymbol{A}$ 一样大小的可训练的矩阵。$\odot$ 表示哈达玛积。通过哈达玛乘积，$(\boldsymbol{W}_{GC} \odot \boldsymbol{A}^K_{GC})$ 可以得到一个在 $K$ 阶邻居上有参数，其他地方为0的新矩阵。因此，$(\boldsymbol{W}_{GC} \odot \boldsymbol{A}^K_{GC})\cdot \boldsymbol{V}_t$ 可以理解成是一个对 $\boldsymbol{V}_t$ 的空间离散的卷积。结果就是，$\boldsymbol{V}_t(K)$ 是时间 $t$ 的融合空间的速度向量。它的第 $i$ 个元素 $v^i_t(K)$ 表示路段 $l_i \in \mathcal{L}$ 在时间 $t$ 的空间融合速度，这个速度集成了其邻居路段 $\mathcal{H}_i(K)$ 的信息。&lt;/p&gt;
&lt;p&gt;此外，式3可以分解成一个一维卷积。&lt;/p&gt;
$$\tag{4}
v^i\_t(K) = (\boldsymbol{W}\_{GC}[i] \odot \boldsymbol{A}^K\_{GC}[i])^T \cdot \boldsymbol{V}\_t
$$&lt;p&gt;$\boldsymbol{W}_{GC}[i]$ 和 $\boldsymbol{A}^K_{GC}[i]$ 分别是 $\boldsymbol{W}_{GC}$ 和 $\boldsymbol{A}^K_{GC}$ 的第 $i$ 行。图2是路网上 $\boldsymbol{A}^K_{GC}[i]$ 的一个例子，路段 $i$ 在红线，邻居是蓝线。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/multistep-speed-prediction-on-traffic-networks-a-graph-convolutional-sequence-to-sequence-learning-approach-with-attention-mechanism/Fig2.jpg"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;h2 id="33-attention-graph-convolutional-sequence-to-sequence-model-agc-seq2seq"&gt;3.3 Attention Graph Convolutional Sequence-to-Sequence Model (AGC-Seq2Seq)
&lt;/h2&gt;&lt;p&gt;我们提出了 AGC-Seq2Seq 模型将时空变量和外部信息集成至深度学习架构中，用来做多步车辆速度预测。&lt;/p&gt;
&lt;p&gt;为了捕获时间序列特征和获得多步输出，我们使用 Seq2Seq 作为整个方法的基础结构，由两个参数独立的 RNN 模块组成(Sutskever et al., 2014; Cho et al., 2014)。为了克服 RNN 输出的长度不可变，Seq2Seq 模型将输入进编码器的时间序列编码，解码器从 &lt;em&gt;context vector&lt;/em&gt; 中解码出预测值。我们提出的 AGC-Seq2Seq 模型如图3所示。首先用图卷积来捕获空间特征，然后将时空变量 $v^i_{t-j}(K)$ 和外部信息 $\boldsymbol{E}_{t-j}$（包括时间和工作日或周末信息）融合构成输入向量，然后放入 Seq2Seq的编码模型中。上述过程如下：&lt;/p&gt;
$$\tag{5}
v^i\_{t-j}(K) = (\boldsymbol{W}\_{GC}[i] \odot \boldsymbol{A}^K\_{GC}[i])^T \cdot \boldsymbol{V}\_{t-j}, \quad 0 \leq j \leq m
$$$$\tag{6}
\boldsymbol{E}\_{t-j} = [N\_{t-j};p\_{t-j}]
$$$$\tag{7}
\boldsymbol{X}^i\_{t-j} = [v^i\_{t-j}(K);\boldsymbol{E}\_{t-j}]
$$&lt;p&gt;其中 $N_{t-j}$ 和 $p_{t-j}$ 如3.1节定义，$[·;·]$ 操作是将两个张量拼接。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/multistep-speed-prediction-on-traffic-networks-a-graph-convolutional-sequence-to-sequence-learning-approach-with-attention-mechanism/Fig3.jpg"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;编码部分如式8-9，在时间步 $t-j, j\in \lbrace 0, \dots, m \rbrace$，前一个隐藏状态 $\boldsymbol{h}_{t-j-1}$ 传入到当前时间戳和 $\boldsymbol{X}_{t-j}$ 计算得到 $\boldsymbol{h}_{t-j}$。因此，背景向量 $\boldsymbol{C}$ 存储了包括隐藏状态 $(\boldsymbol{h}_{t-m}, \boldsymbol{h}_{t-m+1}, \boldsymbol{h}_{t-1})$ 和输入向量 $(\boldsymbol{X}_{t-m}, \boldsymbol{X}_{t-m+1}, \boldsymbol{X}_t)$ 的信息。&lt;/p&gt;
$$\tag{8}
\boldsymbol{h}\_{t-j} = \begin{cases}
\text{Cell}\_{encoder}(\boldsymbol{h}\_0, \boldsymbol{X}\_{t-j}), \quad &amp;j = m\\
\text{Cell}\_{encoder}(\boldsymbol{h}\_{t-j-1}, \boldsymbol{X}\_{t-j}), \quad &amp;j \in \lbrace 0, \dots, m-1 \rbrace
\end{cases}
$$$$\tag{9}
\boldsymbol{C} = \boldsymbol{h}\_t
$$&lt;p&gt;其中 $\boldsymbol{h}_0$ 是初始隐藏状态，通常是 0 向量；$\text{Cell}_{encoder}(·)$ 是编码器的计算函数，由使用的 RNN 结构决定。&lt;/p&gt;
&lt;p&gt;在解码器的部分，关键是利用背景向量 $\boldsymbol{C}$ 作为初始的隐藏向量，一步一步地解码。时间 $t+j, j \in \lbrace 1, \dots, n \rbrace$ 步，隐藏状态 $\boldsymbol{h}_{t+j}$ 不仅包括输入信息，还考虑之前的输出状态 $(\boldsymbol{h}_{t+1}, \boldsymbol{h}_{t+2}, \dots, \boldsymbol{h}_{t+j-1})$。&lt;/p&gt;
&lt;p&gt;解码器的输入依赖于训练方法。&lt;em&gt;Teacher forcing&lt;/em&gt; 在 NLP 中是一个流行的训练策略。在 teacher-forcing 训练策略中，真值在训练的时候输入到解码器，测试的时候将预测值输入进解码器。这种方法不适合时间序列预测主要是因为在训练和测试的时候，输入到解码器的分布不一致。Li et al. (2017) 使用 &lt;em&gt;scheduled sampling&lt;/em&gt; 缓解了这个问题，通过设定概率 $\epsilon$，随机的将真值或预测值放入到解码器中。但这会增加模型的复杂度，给计算造成负担。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，我们提出了一个新的训练策略，将历史的统计信息和时间信息作为输入。在时间序列预测问题中，历史信息可以通过训练和测试阶段获得；这样解码器在训练和测试的时候，其输入的分布就可以相互同步，解决 &lt;em&gt;teacher forcing&lt;/em&gt; 的问题。此外，因为历史统计信息在多步预测中很重要，增加这个可以提高模型的预测精度。下面的等式用来计算 $t+j,j \in \lbrace 1, \dots, n \rbrace$ 这个时间步解码器的隐藏状态。&lt;/p&gt;
$$\tag{10}
\boldsymbol{v}^i\_{t+j}(H) = [N\_{t+j}; v^i\_{t+j, average};v^i\_{t+j, median}; v^i\_{t+j, max}; v^i\_{t+j, min}; d^i\_{t+j}]
$$$$\tag{11}
\boldsymbol{h}\_{t+j} = \begin{cases}
\text{Cell}\_{decoder}(\boldsymbol{C}, \boldsymbol{v}^i\_{t+j}(H)), \quad &amp;j = 1\\
\text{Cell}\_{decoder}(\boldsymbol{h}\_{t+j-1}, \boldsymbol{v}^i\_{t+j}(H)), \quad &amp;j \in \lbrace 2, \dots, n \rbrace
\end{cases}
$$&lt;p&gt;其中 $\text{Cell}_{decoder}$ 是解码器的计算公式，与编码器类似。&lt;/p&gt;
&lt;p&gt;我们使用 GRU (Chung et al., 2014) 作为编码和解码的结构，如图4。实验效果比标准的 LSTM 好很多。编码器和解码器的计算过程如式12-17所示：&lt;/p&gt;
$$\tag{12}
z\_t = \sigma(\boldsymbol{W}\_z \cdot [\boldsymbol{h}\_{t-1}; x\_t] + b\_z)
$$$$\tag{13}
r\_t = \sigma(\boldsymbol{W}\_r \cdot [\boldsymbol{h}\_{t-1}; x\_t] + b\_r)
$$$$\tag{14}
c\_t = \text{tanh}(\boldsymbol{W}\_c \cdot [r\_t \odot \boldsymbol{h}\_{t-1}; x\_t] + b\_c)
$$$$\tag{15}
\boldsymbol{h}\_t = (1 - z\_t) \odot \boldsymbol{h}\_{t-1} + z\_t \odot c\_t
$$$$\tag{16}
\sigma(x) = \frac{1}{1 + e^{-x}}
$$$$\tag{17}
\text{tanh}(x) = \frac{e^x - e^{-x}}{e^x+e^{-x}}
$$&lt;p&gt;在上式中，$z_t$ 和 $r_t$ 分别是更新门和重置门。$c_t$ 是候选输出，$\sigma(\cdot)$ 和 $\text{tanh}(\cdot)$ 是两个激活函数。$W_z$，$W_r$ 和 $W_c$ 是权重矩阵，$b_z$，$b_r$ 和 $b_c$ 是偏置。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/multistep-speed-prediction-on-traffic-networks-a-graph-convolutional-sequence-to-sequence-learning-approach-with-attention-mechanism/Fig4.jpg"
loading="lazy"
alt="Figure4"
&gt;&lt;/p&gt;
&lt;p&gt;为了捕获交通模式的外部信息，我们还集成了注意力机制 (Bahdanau et al., 2014; Luong et al., 2015)。注意力机制的关键在于在每个时间步增加捕获了源信息的相关性的注意力向量来帮助交通速度。在时间步 $t+j, j \in \lbrace 1, \dots, n \rbrace$，注意力函数定义为式 18-20，将 query $\boldsymbol{h}_{t+j}$ 和 一组 key $(\boldsymbol{h}_{t-m}, \dots, \boldsymbol{t-1}, \boldsymbol{h}_t)$ 映射起来组成注意力向量 $\boldsymbol{S}_{t+j}$。如下式 18-20，$\boldsymbol{S}_{t+j}$ 通过计算这些 key 的带权和得到，权重通过计算得到：&lt;/p&gt;
$$\tag{18}
u^{t-i}\_{t+j} = \boldsymbol{q}^T \text{tanh} (\boldsymbol{h}\_{t+j} \boldsymbol{W}\_f \boldsymbol{h}\_{t-j}), \quad i = 0,1,\dots,m
$$$$\tag{19}
a^{t-i}\_{t+j} = \text{softmax}(u^{t-i}\_{t+j}) = \frac{\text{exp}(u^{t-i}\_{t+j})}{\sum^m\_{r=1} \text{exp} (u^{t-r}\_{t+j})}, \quad i=0,1,\dots,m
$$$$\tag{20}
\boldsymbol{S}\_{t+j} = \sum^m\_{i=1} a^{t-i}\_{t+j} \boldsymbol{h}\_{t-i}
$$&lt;p&gt;其中，式 18 计算出的 $u^{t-i}_{t+j}$ 可以用来衡量 $\boldsymbol{h}_{t+j}$ 和 $\boldsymbol{h}_{t-i}$ 之间的相似性，我们使用 &lt;em&gt;Luong Attention form&lt;/em&gt; (Luong et al., 2015) 作为注意力的计算公式，$\boldsymbol{W}_f$ 和 $\boldsymbol{q}^T$ 是参数，用来调节结果的维数；$a^{t-i}_{t+j}$ 是 $u^{t-i}_{t+j}$ 归一化的结果，用作对应编码器隐藏状态 $\boldsymbol{h}_{t-i}$ 的权重来计算 $\boldsymbol{S}_{t+j}$。&lt;/p&gt;
&lt;p&gt;如图3所示，注意力隐藏状态 $\tilde{\boldsymbol{h}}_{t+j}$ 由注意力向量 $\boldsymbol{S}_{t+j}$ 和原始隐藏状态 $\boldsymbol{h}_{t+j}$ 通过一个简单拼接组成，如式 21 所示。式 22 表示从隐藏状态到输出的线性变换。参数 $\boldsymbol{W}_v$ 和 $b_v$ 的维度与输出一致。&lt;/p&gt;
$$\tag{21}
\tilde{\boldsymbol{h}}\_{t+j} = \text{tanh} (\boldsymbol{W}\_h \cdot [\boldsymbol{S}\_{t+k};\boldsymbol{h}\_{t+j}])
$$$$\tag{22}
\hat{v}\_{t+j} = \boldsymbol{W}\_v \tilde{h}\_{t+j} + b\_v
$$&lt;p&gt;为了减少多步预测中的误差，我们定义了所有要预测的时间步上的平均绝对误差：&lt;/p&gt;
$$\tag{23}
loss = \frac{1}{n} \sum^n\_{j=1} \vert \hat{v}^i\_{t+j} - v^i\_{t+j} \vert
$$&lt;p&gt;所有的参数通过随机梯度下降训练。&lt;/p&gt;
&lt;h1 id="4-numerical-examples"&gt;4 NUMERICAL EXAMPLES
&lt;/h1&gt;&lt;h2 id="41-dataset"&gt;4.1 Dataset
&lt;/h2&gt;&lt;p&gt;数据集是从 A-map 的用户收集的，是中国的一个手机导航应用提供的 (Sohu, 2018)。研究范围选择在了北京 2 环，是北京最堵的地方。如图5(a)所示，我们将 33km 长的二环以 200m 一段分成 163 个路段。此外，我们通过用户的轨迹点计算每个路段上 5 分钟的平均速度。2环上工作和和周末的车速如图5(b)(c)所示，x 轴是经度，y 轴是纬度，z 轴是时间和速度的颜色表。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/multistep-speed-prediction-on-traffic-networks-a-graph-convolutional-sequence-to-sequence-learning-approach-with-attention-mechanism/Fig5_a.jpg"
loading="lazy"
alt="“Figure5 a”"
&gt;
&lt;img src="https://davidham3.github.io/blog/images/multistep-speed-prediction-on-traffic-networks-a-graph-convolutional-sequence-to-sequence-learning-approach-with-attention-mechanism/Fig5_bc.jpg"
loading="lazy"
alt="“Figure5 bc”"
&gt;&lt;/p&gt;
&lt;p&gt;数据范围是2016年10月1日到2016年11月30日。10月1日到11月20日做训练，11月21日到27日做测试。预测的范围是 06:00 到 22:00，因此，每条路段每天包含 192 个数据点。图6展示了划分的数据集。在数据清理后，缺失值通过线性插值的方法填补。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/multistep-speed-prediction-on-traffic-networks-a-graph-convolutional-sequence-to-sequence-learning-approach-with-attention-mechanism/Fig6.jpg"
loading="lazy"
alt="Figure6"
&gt;&lt;/p&gt;
&lt;h2 id="42-model-comparisons"&gt;4.2 Model comparisons
&lt;/h2&gt;&lt;p&gt;在每个部分，提出的模型对比的是其他的 benchmark 模型，包括传统的时间序列分析方法（如 HA 和 ARIMA），还有一些先进的机器学习方法（ANN, KNN, SVR, XGBOOST），深度学习模型（LSTM, GCN, Seq2Seq-Att）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HA：历史均值模型通过训练集的统计值预测测试集的未来车速。举个例子，路段 $l_i \in \mathcal{L}$ 在 8:00-8:05 的平均车速通过训练集同时段同路段的历史速度均值估计。&lt;/li&gt;
&lt;li&gt;ARIMA：$(p, d, q)$ 模型 (Box and Pierce, 1970)，差分的阶数设定为 $d = 1$，自回归部分的阶数和移动平均部分的阶数 $(p, q)$ 通过计算对应的 Akaike information criterion 决定，$p \in [0, 2], q \in [7, 12]$。&lt;/li&gt;
&lt;li&gt;ANN：我们用了三层神经网络，sigmoid 激活，隐藏单元数是特征数的 2 倍。因为 ANN 不能区分时间步上的变量，所以它不能捕获时间依赖。&lt;/li&gt;
&lt;li&gt;KNN：k 近邻，获取训练集中特征空间最相近的 k 个观测值。预测值通过对应的特征向量进行线性组合得到。超参数 $K$ 通过 5 到 25 折的交叉验证选定。&lt;/li&gt;
&lt;li&gt;SVR：支持向量回归 (Suykens and Vandewalle, 1999)，通过核函数将特征向量映射到高维空间得到拟合曲线。核函数和超参数通过交叉验证选定。&lt;/li&gt;
&lt;li&gt;XGBOOST：(Chen and Guestrin, 2016) 在很多机器学习任务上表现出了很好的效果；基于树结构可以扩展成端到端的系统。所有的特征 reshape 后输入到 XGBOOST 来训练。&lt;/li&gt;
&lt;li&gt;LSTM：(Hochreiter and Schmidhuber, 1997)，每个路段的所有特征都 reshape 成一个矩阵，一个轴是时间，另一个轴是特征。LSTM 考虑时间依赖，但是没有捕获空间依赖。&lt;/li&gt;
&lt;li&gt;GCN：GCN 中所有的路段的特征 reshape 成一个矩阵，一个轴是路段，另一个轴是特征。GCN 通过拉普拉斯矩阵将卷积泛化到非欧空间；因此，只考虑了空间关联，没有捕获时间依赖。&lt;/li&gt;
&lt;li&gt;Seq2Seq-Att: 和 AGC-Seq2Seq 的区别是图卷积层。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了保证共鸣，之前提到的预测模型都有和 AGC-Seq2Seq 同样的输入特征（特征类型和窗口长度），尽管传统的时间序列模型利用了训练集的全部速度记录。窗口长度为 12，也就是用过去一小时预测未来。19 维特征如表 1 所示。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/multistep-speed-prediction-on-traffic-networks-a-graph-convolutional-sequence-to-sequence-learning-approach-with-attention-mechanism/Table1.jpg"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;
&lt;p&gt;所有的符号如 3.1 节定义，$n$ 是定值。我们通过三个错误指标评价模型，MAPE, MAE, RMSE, $\text{MAPE} = \frac{1}{Q} \sum^Q_{i=1} \frac{\vert v_i - \hat{v}_i \vert}{v_i}$, $\text{MAE} = \frac{1}{Q} \sum^Q_{i=1} \vert v_i - \hat{v}_i \vert$, $\text{RMSE} = \sqrt{\frac{1}{Q} \sum^Q_{i=1} (v_i - \hat{v}_i)^2}$，其中 $v_i$ 和 $\hat{v}^i$ 分别是真值和预测值；$Q$ 是测试集大小。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/multistep-speed-prediction-on-traffic-networks-a-graph-convolutional-sequence-to-sequence-learning-approach-with-attention-mechanism/Table2_a.jpg"
loading="lazy"
alt="“Table2 a”"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/multistep-speed-prediction-on-traffic-networks-a-graph-convolutional-sequence-to-sequence-learning-approach-with-attention-mechanism/Table2_b.jpg"
loading="lazy"
alt="“Table2 b”"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/multistep-speed-prediction-on-traffic-networks-a-graph-convolutional-sequence-to-sequence-learning-approach-with-attention-mechanism/Table2_c.jpg"
loading="lazy"
alt="“Table2 c”"
&gt;&lt;/p&gt;</description></item><item><title>Geometric deep learning on graphs and manifolds using mixture model CNNs</title><link>https://davidham3.github.io/blog/p/geometric-deep-learning-on-graphs-and-manifolds-using-mixture-model-cnns/</link><pubDate>Tue, 18 Dec 2018 20:49:15 +0000</pubDate><guid>https://davidham3.github.io/blog/p/geometric-deep-learning-on-graphs-and-manifolds-using-mixture-model-cnns/</guid><description>&lt;p&gt;CVPR 2017. 这篇论文有点难，没看下去。。。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1611.08402.pdf" target="_blank" rel="noopener"
&gt;Geometric deep learning on graphs and manifolds using mixture model CNNs&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;大部分深度学习处理的是 1D，2D，3D 欧式结构数据，音频信号、图像、视频。最近大家开始研究在非欧氏空间上的数据，如复杂网络、计算社会科学、计算机图形学。我们提出了一个统一的框架，让 CNN 可以泛化到非欧氏空间上，学习局部的、平稳的、针对任务可分解的特征。我们发现之前提出的一些方法都可以放到我们的框架中。我们发现我们的方法效果比前人的方法都要好。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1. Introduction
&lt;/h1&gt;&lt;h1 id="2-deep-learning-on-graphs"&gt;2. Deep learning on graphs
&lt;/h1&gt;&lt;p&gt;无向带权图 $\mathcal{G} = (\lbrace 1, \dots, n\rbrace, \mathcal{E}, \mathbf{W})$，邻接矩阵 $\mathbf{W} = (w_{ij})$，其中 $w_{ij} = w_{ji}$，如果 $(i, j) \notin \mathcal{E}$，则 $w_{ij} = 0$，否则 $w_{ij} &amp;gt; 0$。未归一化的拉普拉斯矩阵是个 $n \times n$ 的 实对称半正定矩阵 $\Delta = \bf D - W$，其中 $\mathbf{D} = \text{diag}(\sum_{j = \not i} w_{ij})$ 是度矩阵。&lt;/p&gt;
&lt;p&gt;拉普拉斯矩阵有特征值分解 $\bf \Delta = \Phi \Lambda \Phi^T$，其中 $\Phi = (\phi_1, \dots, \phi_n)$ 是相互正交的特征向量，$\Lambda = \text{diag}(\lambda_1, &amp;hellip;, \lambda_n)$ 特征值组成的对角矩阵。在传统的谐波分析中，特征向量是拉普拉斯算子，特征值可以看作是频率。给定图上的一个信号 $\mathbf{f} = (f_1, \dots, f_n)^T$，它的图傅里叶变换是 $\hat{\mathbf{f}} = \Phi^T \mathbf{f}$。给定两个信号 $\bf f, g$，他们的谱卷积定义为傅里叶变换的 element-wise product：&lt;/p&gt;
$$\tag{1}
\mathbf{f} \star \mathbf{g} = \Phi (\Phi^T \mathbf{f}) \odot (\Phi^T g) = \Phi \ \text{diag}(\hat{g}\_1, \dots, \hat{g}\_n) \hat{f},
$$&lt;p&gt;对应了欧氏空间卷积理论。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;其实这里我没理解啊，我记得卷积的定义不是傅里叶变换的乘积的逆变换吗，所以感觉说的有点不对，但公式倒是对了。。。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Spectral CNN.&lt;/strong&gt; Bruna et al. 使用卷积在谱上的定义将 CNN 泛化到图上，得到一个谱卷积层的定义：&lt;/p&gt;
$$\tag{2}
\mathbf{f}^{out}\_l = \xi (\sum^p\_{l'=1} \Phi\_k \hat{G}\_{l,l'} \Phi^T\_k \mathbf{f}^{in}\_{l'})
$$&lt;p&gt;这里维数为 $n \times p$ 和 $n \times q$ 的矩阵 $\mathbf{F}^{in} = (\mathbf{f}^{in}_1, \dots, \mathbf{f}^{in}_p)$，$\mathbf{F}^{out} = (\mathbf{f}^{out}_1, \dots, \mathbf{f}^{out}_q)$ 分别表示 $p$ 维和 $q$ 维的图上的输入和输出信号，$\Phi = (\phi_1, \dots, \phi_k)$ 是前几个特征向量组成的 $n \times k$ 的矩阵，$\hat{\mathbf{G}_{l,l&amp;rsquo;}} = \text{diag}(\hat{g}_{l,l&amp;rsquo;,1}, \dots, \hat{g}_{l,l&amp;rsquo;,k})$ 是一个 $k \times k$ 的对角矩阵，表示频域内一个可学习的滤波器，$\xi$ 是一个非线性激活单元（e.g. ReLU）。这个框架的池化操作在图上的模拟是一个图的缩减操作，给定一个 $n$ 个结点的图，生成一个 $n&amp;rsquo; &amp;lt; n$ 个结点的图，将信号从原来的图上变换到缩减后的图上。&lt;/p&gt;
&lt;p&gt;这个框架有几个缺点。首先，谱滤波器的系数是 &lt;em&gt;basis dependent&lt;/em&gt;，而且，在一个图上学习到的基于谱的 CNN 模型不能应用在其他的图上。其次，图傅里叶变换的计算因为 $\bf \Phi$ 和 $\bf \Phi^T$ 的乘法，会达到 $\mathcal{O}(n^2)$，因为这里没有像 FFT 一样的算法。第三，不能保证在谱域内的滤波器在顶点域上是局部化的；假设使用 $k = O(n)$ 个归一化的拉普拉斯矩阵的特征向量，一个谱卷积层需要 $pqk = O(n)$ 个参数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Smooth Spectral CNN.&lt;/strong&gt; 之后，Henaff et al. 认为 smooth 谱滤波器系数可以使得卷积核在空间上局部化，使用了这个形式：&lt;/p&gt;
$$\tag{3}
\hat{g}\_i = \sum^r\_{j=1} \alpha\_i \beta\_j (\Lambda\_i)
$$&lt;p&gt;其中 $\beta_1(\lambda), \dots, \beta_r(\lambda)$ 是一些固定的插值核，$\mathbb{\alpha} = (\alpha_1, \dots, \alpha_r)$ 是插值系数。矩阵形式中，滤波器写为 $\text{diag}(\hat{G}) = \mathbf{B\alpha}$，其中 $\bf{B} = (b_{ij}) = (\beta_j (\lambda_i))$ 是一个 $k \times r$ 的矩阵。这样一个参数化可以使参数保持在 $n$ 个。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Chebyshev Spectral CNN (ChebNet).&lt;/strong&gt; 为了减轻计算图傅里叶变换的代价，Defferrard et al 使用了切比雪夫多项式来表示卷积核：&lt;/p&gt;
$$\tag{4}
g\_\alpha(\Delta) = \sum^{r-1}\_{j=0} \alpha\_j T\_j(\tilde{\Delta}) = \sum^{r-1}\_{j=0} \alpha\_j \Phi T\_j (\tilde{\Lambda}) \Phi^T,
$$&lt;p&gt;其中 $\tilde{\Delta} = 2 \lambda^{-1}_n \Delta - \bf I$ 是 rescaled 拉普拉斯矩阵，它的特征值 $\tilde{\Lambda} = 2 \lambda^{-1}_n \Lambda - \bf I$ 在区间 $[-1, 1]$ 内，$\alpha$ 是 $r$ 维的滤波器中的多项式系数，&lt;/p&gt;
$$\tag{5}
T\_j(\lambda) = 2 \lambda T\_{j-1}(\lambda) - T\_{j-2} (\lambda),
$$&lt;p&gt;表示 $j$ 阶切比雪夫多项式，$T_1(\lambda) = \lambda$，$T_0(\lambda) = 1$。&lt;/p&gt;
&lt;p&gt;这样的方法有几个优点。首先，它不需要计算拉普拉斯矩阵的特征向量。由于切比雪夫多项式的递归定义，计算滤波器 $g_\alpha(\Lambda) \bf f$ 要使用拉普拉斯矩阵 $r$ 次，会导致一个 $\mathcal{O}(rn)$ 的操作。其次，因为拉普拉斯矩阵是一个局部操作，只影响顶点的一阶邻居，它的 $(r-1)$次幂影响 $r$阶邻居，得到的滤波器是局部化的。&lt;/p&gt;</description></item><item><title>Layer Normalization</title><link>https://davidham3.github.io/blog/p/layer-normalization/</link><pubDate>Mon, 03 Dec 2018 15:17:12 +0000</pubDate><guid>https://davidham3.github.io/blog/p/layer-normalization/</guid><description>&lt;p&gt;Layer Normalization，之前看到一篇论文用了这个LN层，看一下这个怎么实现。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1607.06450.pdf" target="_blank" rel="noopener"
&gt;Layer Normalization&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;训练神经网络很费时，一个减少训练时间的方法是对神经元的激活值归一化。最近的一项技术称为小批量归一化，也就是 batch norm，使用一个神经元的输入的分布，在一个批量的样本上计算均值和方差，然后在神经元的每个训练样例上做归一化。这个能极大地缩短训练时间。但是，batch norm 的效果和 batch size 有关，而且还不知道怎么应用在 RNN 上。我们使用一个训练样例，将 BN 转置，计算一个层上面所有神经元的输入的均值和方差来归一化。就像 BN 一样，我们在归一化后激活之前给每个神经元它自己的可适应的bias和gain。不像 BN 的地方是，LN 在训练和测试的时候都有，通过在每个时间步上做归一化的统计，LN 也能应用在 RNN 上。LN 在稳定 RNN 隐藏状态的动态性上面很有效。经验表明，LN 与之前的技术对比能有效地减少训练时间。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 Introduction
&lt;/h1&gt;&lt;p&gt;很多深度神经网络要训练好多天。BN 除了提升了收敛速度，从批量统计量得到的随机性在训练的时候还会作为一个正则项。&lt;/p&gt;
&lt;p&gt;尽管 BN 简单，但是它需要输入统计量之和的平均值。在定长的 FNN 中，把每个层的 BN 存起来就行。但是，RNN 的循环单元的输入通常随序列长度而变化，所以将 BN 应用在 RNN 上面，不同时间步需要不同的统计量。此外，BN 不能应用在在线学习等任务上，或是非常大的分布式模型上，因为 minibatch 会很小。&lt;/p&gt;
&lt;h1 id="2-background"&gt;2 Background
&lt;/h1&gt;&lt;p&gt;前向神经网络是从输入模式 $\rm{x}$ 映射到输出向量 $y$ 的非线性变换。在深度前向神经网络中的第 $l$ 个隐藏层，$a^l$ 表示这层神经元的输入。汇总后的输入通过一个线性映射计算如下：&lt;/p&gt;
$$\tag{1}
a^l\_i = {w^l\_i}^\text{T} h^l\\
h^{l+1}\_i = f(a^l\_i + b^l\_i)
$$&lt;p&gt;其中 $f(\cdot)$ 是激活函数，$w^l_i$ 和 $b^l_i$ 分别是第 $l$ 个隐藏层的权重和偏置参数。参数通过基于梯度的学习方法得到。&lt;/p&gt;
&lt;p&gt;深度学习的一个挑战是：某一层权重的梯度和上一层的输出高度相关，尤其是当这些输出以一种高度相关的方式变化的时候。BN 提出来是减少这种不希望的 covariate shift 现象。这种方法在输入样例在每个隐藏单元的输入上做计算。详细来说，对于第 $l$ 层的第 $i$ 个输入，BN 根据他们在数据中的分布，将输入缩放了：&lt;/p&gt;
$$\tag{2}
\bar{a}^l\_i = \frac{g^l\_i}{\sigma^l\_i}(a^l\_i - \mu^l\_i)\\
\mu^l\_i = \mathbb{E}\_{\mathrm{x} \sim P(\mathrm{x})}[a^l\_i]\\
\sigma^l\_i = \sqrt{\mathbb{E}\_{\mathrm{x} \sim P(\mathrm{x})}[(a^l\_i - \mu^l\_i)^2]}
$$&lt;p&gt;其中 $\bar{a}^l_i$ 是第 $l$ 层第 $i$ 个输入的归一化结果，$g_i$ 是在非线性激活函数之前的一个增益参数，对归一化激活值进行缩放。注意，期望是在所有训练数据上的。事实上计算式2中的期望是不实际的，因为这需要用当前的参数，前向传播过所有的训练集。实际中是用当前的 mini-batch 来估计 $\mu$ 和 $\sigma$。这就给 batch size 增加了限制，而且很难应用到 RNN 上。&lt;/p&gt;
&lt;h1 id="3-layer-normalization"&gt;3 Layer normalization
&lt;/h1&gt;&lt;p&gt;层归一化用来克服批量归一化的一些缺点。&lt;/p&gt;
&lt;p&gt;一个层输出的变换倾向于导致下一层的输入之间有着关联度很高的变化，尤其是使用 ReLU 激活后，这些输出的变化很多。这表明 covariate shift 问题可以通过固定每层的输入的均值和方差解决。因此，在同一层中所有隐藏单元的层归一化统计量如下：&lt;/p&gt;
$$\tag{3}
\mu^l = \frac{1}{H} \sum^H\_{i = 1}a^l\_i\\
\sigma^l = \sqrt{\frac{1}{H} \sum^H\_{i=1} (a^l\_i - \mu^l)^2}
$$&lt;p&gt;其中 $H$ 表示层内的隐藏单元数。式2和式3的区别是在层归一化之下，层内所有隐藏单元共享相同的归一化项 $\mu$ 和 $\sigma$，但是不同的样本有着不同的归一化项。不像 BN，层归一化不会有 batch size 的限制，而且可以使用在 batch size 设为1的时候的在线学习上。&lt;/p&gt;
&lt;h2 id="31-layer-normalized-recurrent-neural-networks"&gt;3.1 Layer normalized recurrent neural networks
&lt;/h2&gt;&lt;p&gt;最近的序列到序列模型 [Sutskever et al., 2014] 利用了紧致的 RNN 来解决 NLP 中的序列预测问题。在 NLP 任务中不同的训练样例长度不一致是很常见的。RNN 在每个时间步使用的参数都是相同的。但是在使用 BN 来处理 RNN 时，我们需要计算并存储序列中每个时间步的统计量。如果一个测试的序列比任何训练的序列都长，那就会出问题了。层归一化不会有这样的问题，因为它的归一化项只依赖于当前时间步层的输入。它在所有的时间步上也有一组共享的 gain 和 bias 参数。&lt;/p&gt;
&lt;p&gt;在标准的 RNN 中，循环层的输入通过当前的输入 $\mathrm{x}^t$ 和前一层的隐藏状态 $\mathrm{h}^{t-1}$，得到 $\mathrm{a}^t = W_{hh}h^{t-1} + W_{xh} \mathrm{x}^t$。层归一化后的循环层会将它的激活值使用像式3一样的归一化项缩放到：&lt;/p&gt;
$$\tag{4}
\mathrm{h}^t = f[\frac{\mathrm{g}}{\sigma^t} \odot (\mathrm{a}^t - \mu^t) + b]\\
\mu^t = \frac{1}{H} \sum^H\_{i=1}a^t\_i\\
\sigma^t = \sqrt{\frac{1}{H} \sum^H\_{i=1}(a^t\_i - \mu^t)^2}
$$&lt;p&gt;其中 $W_{hh}$ 是循环隐藏到隐藏的权重，$W_{xh}$ 是输入到隐藏的权重，$\odot$ 是element-wise multiplication。$\rm b$ 和 $\rm g$是和 $\mathrm{h}^t$ 同维度的 bias 和 gain 参数。&lt;/p&gt;
&lt;p&gt;在标准的 RNN 中，每个时间步的循环单元的输入的数量级倾向于增大或减小，导致梯度的爆炸或消失问题。在一个层归一化的 RNN 里，归一化项使它对一个层的输入的缩放不发生变化，使得隐藏到隐藏动态性更稳定。&lt;/p&gt;</description></item><item><title>Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</title><link>https://davidham3.github.io/blog/p/deeper-insights-into-graph-convolutional-networks-for-semi-supervised-learning/</link><pubDate>Wed, 31 Oct 2018 21:58:41 +0000</pubDate><guid>https://davidham3.github.io/blog/p/deeper-insights-into-graph-convolutional-networks-for-semi-supervised-learning/</guid><description>&lt;p&gt;AAAI 2018。这篇论文很有趣，讲的是 GCN 堆得过多了之后，效果会变差的问题。作者分析了一下为什么会变差，主要是因为 GCN 的本质实际上是对每个结点的邻居特征和自身特征做线性组合，权重和邻接矩阵相关，所以对于顶点分类问题来说，如果堆得层数多了，就会让一个结点的特征聚合越来越多邻居的特征，让大家都变得相似，从而使得类间的相似度增大，自然分类效果就差了。作者提出了两个方法解决这个问题，算训练上的 trick 吧。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1801.07606" target="_blank" rel="noopener"
&gt;Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;机器学习中很多有趣的问题正在用深度学习工具来重新审视。对于基于图的半监督学习问题，最新的一个重要进展就是图卷积神经网络 (GCNs)，这个模型可以很好的将顶点局部特征和图的拓扑结构整合进卷积层内。尽管 GCN 模型和其他的 state-of-the-art 方法相比效果更好，但是它的机理目前还不是很清楚，而且需要很多的标记数据用于验证以及模型选择。&lt;/p&gt;
&lt;p&gt;在这篇论文中，我们深入了 GCN 模型，解决了它的底层限制。首先，我们发现 GCN 模型的图卷积实际上是一个拉普拉斯平滑的特殊形式，这是 GCN 工作的关键原因，但是这也会给很多卷积层带来潜在的危害。其次，为了克服 GCN 层数少的限制，我们提出了协同训练和自训练方法来训练 GCNs。我们的方法显著地提升了 GCNs 在标记样本少的情况下的学习，并且让他们避免了使用额外的标记用来验证。大量的实验证明了我们的理论和方案。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 Introduction
&lt;/h1&gt;&lt;p&gt;深度学习中的突破使得人工智能和机器学习中正在发生范式变化。一方面，很多老问题通过深度神经网络重新审视，很多原来看起来在任务中无法完成的巨大进步现在也在发生着，如机器翻译和计算机视觉。另一方面，像几何深度学习 (Bronstein et al. 2017) 这样的技术正在发展，可能会将深度神经模型泛化到新的或非传统的领域。&lt;/p&gt;
&lt;p&gt;众所周知，深度学习模型一般需要大量的标记数据，在很多标记训练数据代价很大的场景就无法满足这样的要求。为了减少用于训练的数据的数量，最近的研究开始关注 few-shot learning (Lake, Salakhutdinov, and Tenenbaum 2015; Rezende et al. 2016)——从每个类只有很少的样本中学习一个分类模型。和 few-shot learning 相近的是半监督学习，其中有大量的未标记样本可以用来和很少量的标记样本一起用于训练。&lt;/p&gt;
&lt;p&gt;很多研究者已经证实了如果使用恰当，在训练中利用未标记样本可以显著地提升学习的精度 (Zhu and Goldberg 2009)。关键问题是最大化未标记样本的结构和特征信息的有效利用。由于强力的特征抽取能力和深度学习近些年的成功案例，已经有很多人使用基于神经网络的方法处理半监督学习，包括 ladder network (Rasmus et al. 2015), 半监督嵌入 (Weston et al. 2008)，planetoid (Yang, Cohen, and Salakhutdinov 2016)，图卷积网络 (Kipf and Welling 2017)。&lt;/p&gt;
&lt;p&gt;最近发展的图卷积神经网络 (GCNNs) (Defferrard, Bresson, and Vandergheynst 2016) 是一个将欧氏空间中使用的卷积神经网络 (CNNs) 泛化到对图结构数据建模的成功尝试。在他们的初期工作 (Kipf and Welling 2017)，Kifp and Welling 提出了一个 GCNNs 的简化类型，称为图卷积网络 (GCNs)，应用于半监督分类。GCN 模型很自然地将图结构数据的连接模式和特征属性集成起来，而且比很多 state-of-the-art 方法在 benchmarks 上好很多。尽管如此，它也有很多其他基于神经网络的模型遇到的问题。用于半监督学习的 GCN 模型的工作机理还不清楚，而且训练 GCNs 仍然需要大量的标记样本用于调参和模型选择，这就和半监督学习的理念相违背。&lt;/p&gt;
&lt;p&gt;在这篇论文中，我们弄清楚了用于半监督学习的 GCN 模型。特别地，我们发现 GCN 模型中的图卷积是拉普拉斯平滑的一种特殊形式，这个平滑可以混合一个顶点和它周围顶点的特征。这个平滑操作使得同一类簇内顶点的特征相似，因此使分类任务变得简单，这使为什么 GCNs 表现的这么好的关键原因。然而，这也会带来 over-smoothing 的问题。如果 GCN 有很多卷积层后变深了，那么输出的特征可能会变得过度平滑，且来自不同类簇的顶点可能变得无法区分。这种混合在小的数据集，且只有很少的卷积层上发生的很快，就像图2展示的那样。而且，给 GCN 模型增加更多的层也会使它变得难以训练。&lt;/p&gt;
&lt;p&gt;然而，一个浅层的 GCN 模型，像 Kipf &amp;amp; Welling 2017 使用的两层 GCN 有它自身的限制。除此以外它还需要很多额外的标记用来验证，它也会遇到卷积核局部性等问题。当只有少数标记的时候，一个浅层的 GCN 模型不能有效的将标记传播到整个图上。如图1所示，GCNs 的表现会随着训练集的减少急速下降，甚至有500个额外标记用来验证。&lt;/p&gt;
&lt;p&gt;为了克服限制并理解 GCN 模型的全部潜能，我们提出了一种协同训练方法和一个自训练方法来训练 GCNs。通过使用随机游走模型来协同训练一个 GCN，随机游走模型可以补充 GCN 模型在获取整个图拓扑结构上的能力。通过自训练一个 GCN，我们可以挖掘它的特征提取能力来克服它的局部特性。融合协同训练和自训练方法可以从本质上提升 GCN 模型在半监督学习上只有少量标记的效果，而且使它不用使用额外的标记样本用来验证。如图1所示，我们的方法比 GCNs 好了一大截。&lt;/p&gt;
&lt;p&gt;总而言之，这篇论文的关键创新有：1) 对半监督学习的 GCN 模型提供了新的视角和新的分析；2) 提出了对半监督学习的 GCN 模型提升的解决方案。&lt;/p&gt;
&lt;h1 id="2-preliminaries-and-related-works"&gt;2 Preliminaries and Related Works
&lt;/h1&gt;&lt;p&gt;首先，我们定义一些符号。图表示为 $\mathcal{G} = (\mathcal{V}, \mathcal{E})$，其中 $\mathcal{V}$ 是顶点集，$\vert \mathcal{V} \vert = n$，$\mathcal{E}$ 是边集。在这篇论文中，我们考虑的是无向图。$A = [a_{ij}] \in \mathbb{R}^{n \times n}$ 是邻接矩阵，且为非负的。$D = \mathrm{diag}(d_1, d_2, &amp;hellip;, d_n)$ 表示度矩阵，$d_i = \sum_j a_{ij}$ 是顶点 $i$ 的度。图拉普拉斯矩阵 (Chung 1997) 定义为 $L := D - A$，归一化的图拉普拉斯矩阵的两个版本分别定义为：$L_{sym} := D^{-\frac{1}{2}} L D^{-\frac{1}{2}}$ 和 $L_{rw} := D^{-1}L$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Graph-Based Semi-Supervised Learning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这篇论文中我们考虑的问题是图上的半监督分类任务。给定一个图 $\mathcal{G} = (\mathcal{V}, \mathcal{E}, X)$，其中 $X = \mathrm{[x_1, x_2, &amp;hellip;, x_n]^T} \in R^{n \times c}$ 是特征矩阵，$\mathrm{x}_i \in R^c$ 是顶点 $i$ 的 $c$ 维特征向量。假设给定了一组顶点 $\mathcal{V}_l$ 的标记，目标是预测其余顶点 $\mathcal{V}_u$ 的标记。&lt;/p&gt;
&lt;p&gt;基于图的半监督学习在过去的二十年成为了一个流行的研究领域。通过挖掘图或数据的流形结构，是可以通过少量标记进行学习的。很多基于图的半监督学习方法形成了类簇假设 (cluster assumption) (Chapelle and Zien 2005)，假设了一个图上临近的顶点倾向于有共同的标记。顺着这条路线的研究包括 min-cuts (Blum and Chawla 2001) 和 randomized min-cuts (Blum et al. 2004)，spectral graph transducer (Joachims 2003)，label propagation (Zhu, Ghahramani, and Lafferty 2003) and its variants (Zhou et al. 2004; Bengio, Delalleau, and Le Roux 2006)，modified adsorption (Talukdar and Crammer 2009)，还有 iterative classification algorithm (Sen et al. 2008)。&lt;/p&gt;
&lt;p&gt;但是图只表示数据的结构信息。在很多应用，数据的样本是以包含信息的特征向量表示，而不是在图中表现。比如，在引文网络中，文档之间的引用链接描述了引用关系，但是文档是由 bag-of-words 向量表示的，这些向量描述的内容是文档的内容。很多半监督学习方法寻求对图结构和数据的特征属性共同建模。一个常见的想法是使用一些正则项对一个监督的学习器进行正则化。比如，manifold regularization (LapSVM) (Belkin, Niyogi, and Sindhwani 2006) 使用一个拉普拉斯正则项对 SVM 进行正则化。深度半监督嵌入 (Weston et al. 2008) 使用一个基于嵌入的正则项对深度神经网络进行正则化。Planetoid (Yang, Cohen, and Salakhutdinov 2016) 也通过共同地对类标记和样本的上下文预测对神经网络进行正则化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Graph Convolutional Networks&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;图卷积神经网络 (GCNNs) 将传统的卷积神经网络泛化到图域中。主要有两类 GCNNs (Bronstein et al. 2017): spatial GCNNs 和 spectral GCNNs。空间 GCNNs 将卷积看作是 &amp;ldquo;patch operator&amp;rdquo;，对每个顶点使用它的邻居信息构建新的特征向量。谱 GCNNs 通过对图信号 $\bf{s} \in \mathcal{R}^n$ 在谱域上进行分解，然后使用一个在谱成分上的谱卷积核 $g_\theta$ (是 $L_{sym}$ 的特征值的一个函数) (Bruna et al. 2014; Sandryhaila and Moura 2013; Shuman et al. 2013)。然而这个模型需要计算出拉普拉斯矩阵的特征向量，这对于大尺度的图来说是不太实际的。一种缓解这个问题的方法是通过将谱卷积核 $g_\theta$ 通过切比雪夫多项式趋近到 $K^{th}$ 阶 (Hammond, Vandergheynst, and Gribonval 2011)。在 (Defferrard, Bresson, and Vandergheynst 2016)，Defferrard et al. 使用这个构建了 $K$ 阶 ChebNet，卷积定义为：&lt;/p&gt;
$$\tag{1}
g\_\theta \star \mathbf{s} \approx \sum^K\_{k=0} \theta'\_k T\_k (L\_{sym}) \mathbf{s},
$$&lt;p&gt;其中 $\bf{s} \in \mathcal{R}^n$ 是图上的信号，$g_\theta$是谱滤波器，$\star$ 是卷积操作，$T_k$ 是切比雪夫多项式，$\theta&amp;rsquo; \in \mathcal{R}^K$ 是切比雪夫系数向量。通过这种趋近，ChebNet 域谱无关。&lt;/p&gt;
&lt;p&gt;在 (Kipf and Welling 2017) 中，Kipf and Welling 将上面的模型通过让 $K = 1$ 进行了简化，将 $L_{sym}$ 的最大特征值趋近为2.在这种形式中，卷积变成：&lt;/p&gt;
$$\tag{2}
g\_\theta \star \mathbf{s} = \theta(I + D^{-\frac{1}{2}} A D^{-\frac{1}{2}}) \mathbf{s},
$$&lt;p&gt;其中 $\theta$ 是切比雪夫系数。然后对卷积矩阵使用一种正则化的技巧：&lt;/p&gt;
$$\tag{3}
I + D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \rightarrow \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}},
$$&lt;p&gt;其中 $\tilde{A} = A + I$，$\tilde{D} = \sum_j \tilde{A}_{ij}$.&lt;/p&gt;
&lt;p&gt;将卷积泛化到带有 $c$ 个通道的图信号上，也就是 $X \in \mathcal{R}^{n \times c}$ (每个顶点是一个 $c$ 维特征向量)，使用 $f$ 谱卷积核，简化后的模型的传播规则是：&lt;/p&gt;
$$\tag{4}
H^{(l + 1)} = \sigma(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} \Theta^{(l)}),
$$&lt;p&gt;其中，$H^{(l)}$ 是第 $l$ 层的激活值矩阵，$H^{(0)} = X$，$\Theta^{(l)} \in \mathcal{R}^{c \times f}$ 是第 $l$ 层可训练的权重矩阵，$\sigma$ 是激活函数，比如 $ReLU(\cdot) = max(0, \cdot)$。&lt;/p&gt;
&lt;p&gt;这个简化的模型称为图卷积网络 (GCNs)，是我们这篇论文关注的重点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Semi-Supervised Classification with GCNs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在 Kipf and Welling 2017 中，GCN 模型以一种优雅的方式做半监督分类任务。模型是一个两层 GCN，在输出时使用一个 softmax：&lt;/p&gt;
$$\tag{5}
Z = \mathrm{softmax}(\hat{A} ReLU (\hat{A} X \Theta^{(0)}) \Theta^{(1)} ),
$$&lt;p&gt;其中 $\hat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$，$\mathrm{softmax}(x_i) = \frac{1}{\mathcal{Z}} exp(x_i)$，$\mathcal{Z} = \sum_i exp(x_i)$。损失函数是所有标记样本上的交叉熵：&lt;/p&gt;
$$\tag{6}
\mathcal{L} := - \sum\_{i \in \mathcal{V}\_l} \sum^F\_{f=1} Y\_{if} \mathrm{ln} Z\_{if},
$$&lt;p&gt;其中 $\mathcal{V}_l$ 是标记顶点的下标，$F$ 是输出特征的维数，等价于类别数。$Y \in \mathcal{R}^{\vert \mathcal{V}_l \vert \times F}$ 是标记矩阵。权重参数 $\Theta^{(0)}$ 和 $\Theta^{(1)}$ 可以通过梯度下降训练。&lt;/p&gt;
&lt;p&gt;GCN 模型在卷积中自然地融合了图的结构和顶点的特征，未标记的顶点的特征和临近的标记顶点的混合在一起，然后通过多个层在网络上传播。GCNs 在 Kipf &amp;amp; Welling 2017 中比很多 state-of-the-art 方法都好很多，比如在引文网络上。&lt;/p&gt;
&lt;h1 id="3-analysis"&gt;3 Analysis
&lt;/h1&gt;&lt;p&gt;尽管它的性能很好，但是用于半监督学习的 GCN 模型的机理还没有弄明白。在这部分我们会走近 GCN 模型，分析它为什么好使，并指出它的限制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why GCNs Work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们将 GCN 和最简单的全连接神经网络 (FCN) 进行比较，传播规则是：&lt;/p&gt;
$$\tag{7}
H^{(l + 1)} = \sigma(H^{(l)} \Theta^{(l)}).
$$&lt;p&gt;GCN 和 FCN 之间的唯一一个区别是图卷积矩阵 $\hat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$ (式5)用在特征矩阵 $X$ 的左边。我们在 Cora 数据集上，每类 20 个 标签，做了半监督分类的测试。如表1所示。即便是只有一层的 GCN 也比一层的 FCN 好很多。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deeper-insights-into-graph-convolutional-networks-for-semi-supervised-learning/Table1.JPG"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Laplacian Smoothing.&lt;/strong&gt; 考虑一个一层的 GCN。实际有两步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;从矩阵 $X$ 通过一个图卷积得到新的特征矩阵 $Y$：&lt;/li&gt;
&lt;/ol&gt;
$$\tag{8}
Y = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}X.
$$&lt;ol start="2"&gt;
&lt;li&gt;将新的特征矩阵 $Y$ 放到一个全连接层。很明显，图卷积是性能提升的关键。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们来自己的检查一下图卷积。假设我们给图中的每个结点增加一个自连接，新的图的邻接矩阵就是 $\tilde{A} = A + I$。输入特征的每个通道的拉普拉斯平滑 (Taubin 1995) 定义为：&lt;/p&gt;
$$\tag{9}
\hat{\mathrm{y}}\_i = (1 - \gamma) \mathrm{x}\_i + \gamma \sum\_j \frac{\tilde{a}\_{ij}}{d\_i} \mathrm{x}\_j \quad (\text{for} \quad 1 \leq i \leq n),
$$&lt;p&gt;其中 $0 &amp;lt; \gamma &amp;lt; 1$ 是控制当前结点的特征和它的邻居的特征之间的权重。我们可以将拉普拉斯平滑写成矩阵形式：&lt;/p&gt;
$$\tag{10}
\hat{Y} = X - \gamma \tilde{D}^{-1} \tilde{L} X = (I - \gamma \tilde{D}^{-1} \tilde{L})X,
$$&lt;p&gt;其中 $\tilde{L} = \tilde{D} - \tilde{A}$。通过设定 $\gamma = 1$，也就是只使用邻居的特征，可得 $\hat{Y} = \tilde{D}^{-1} \tilde{A} X$，也就是拉普拉斯平滑的标准形式。&lt;/p&gt;
&lt;p&gt;现在如果我们把归一化的拉普拉斯矩阵 $\tilde{D}^{-1} \tilde{L}$ 替换成对阵的归一化拉普拉斯矩阵 $\tilde{D}^{-\frac{1}{2}} \tilde{L} \tilde{D}^{-\frac{1}{2}}$，让 $\gamma = 1$，可得 $\hat{Y} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} X$，这恰好就是式8中的图卷积。我们因此称图卷积是一种特殊形式的拉普拉斯平滑——对称拉普拉斯平滑。注意，平滑仍然会包含顶点特征，因为每个顶点有一个自连接，还有它自己的邻居。&lt;/p&gt;
&lt;p&gt;拉普拉斯平滑计算了顶点的新的特征，也就是顶点自身和邻居的加权平均。因为同一类簇的顶点倾向于连接的更紧密，这使得分类任务变得更简单。因为我们可以从表1看出只使用一次平滑就很有效了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multi-layer Structure.&lt;/strong&gt; 我们可以从表1看出尽管两层的 FCN 比 一层的 FCN 有了些许的提升，两层的 GCN 却比 一层的 GCN 好了很多。这是因为在第一层的激活值上再使用平滑使得同一个类簇中的顶点特征变得更像四了，使分类任务更简单。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When GCNs Fail&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们已经证明了图卷积本质上就是一种拉普拉斯平滑。那 GCN 中应该放多少层呢？当然不是越多越好。GCN 层多了会不好训练。而且重复使用拉普拉斯平滑可能会混合不同类簇中的顶点的特征，使得他们区分不清。我们来举个例子。&lt;/p&gt;
&lt;p&gt;我们在 Zachary 的 karate club dataset (Zachary 1977) 上跑几个层数不同的模型。这个数据集有 34 个结点，两类，78 条边。GCN 的参数像 (Glorot and Bengio 2010) 中的一样随机初始化。隐藏层的维数是 16，输出层的维度是2。每个结点的特征向量是一个 one-hot 向量。每个 GCN 的输出绘制在图2中。我们可以看到图卷积的影响（图2a）。使用两次平滑，分类效果相对较好。再次使用平滑，点就会混合（图2c，2d，2e）。因为这是个小的数据集，两类之间的顶点有很多连接，所以很快就发生了混合。&lt;/p&gt;
&lt;p&gt;接下来，我们会证明重复使用拉普拉斯平滑，顶点的特征以及图的每个连通分量会收敛到相同的值。对于对称的拉普拉斯平滑，他们收敛到的值与顶点度数的二分之一次幂成正比。&lt;/p&gt;
&lt;p&gt;假设图 $\mathcal{G}$ 有 $k$ 个连通分量 $\lbrace C_i\rbrace ^k_{i=1}$，对于第 $i$ 个连通分量的指示向量表示为 $\mathbf{1}^{(i)} \in \mathbb{R}^n$。这个向量表示顶点是否在分量 $C_i$中，即：&lt;/p&gt;
$$\tag{11}
\mathbf{1}^{(i)}\_j = \begin{cases}
1, v\_j \in C\_i,\\
0, v\_j \notin C\_i
\end{cases}
$$&lt;p&gt;**Theorem 1. ** 如果一个图没有二分的连通分量，那么对于任意 $\mathrm{w} \in \mathbb{R}^n$，$\alpha \in (0, 1]$，&lt;/p&gt;
$$
\lim\_{m \rightarrow + \infty} (I - \alpha L\_{rw})^m \mathrm{w} = [\mathbf{1}^{(1)}, \mathbf{1}^{(2)}, ..., \mathbf{1}^{(k)}]\theta\_1,
$$$$
\lim\_{m \rightarrow + \infty} (I - \alpha L\_{sym})^m \mathrm{w} = D^{-\frac{1}{2}}[\mathbf{1}^{(1)}, \mathbf{1}^{(2)}, ..., \mathbf{1}^{(k)}]\theta\_2,
$$&lt;p&gt;其中 $\theta_1 \in \mathbb{R}^k, \theta_2 \in \mathbb{R}^k$，也就是他们分别收敛到 $\lbrace \mathbf{1}^{(i)}\rbrace ^k_{i=1}$ 和 $\lbrace D^{-\frac{1}{2}} \mathbf{1}^{(i)} \rbrace ^k_{i=1}$。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; $L_{rw}$ 和 $L_{sym}$ 有相同的 $n$ 个特征值，不同的特征向量 (Von Luxbury 2007)。如果一个图没有二分的连通分量，那么特征值就会在 $[0, 2)$ 区间内 (Chung 1997)。后面就没看懂了。。。&lt;/p&gt;
&lt;h1 id="4-solutions"&gt;4. Solutions
&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Co-Train a GCN with a Random Walk Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;使用一个 partially absorbing random walks (Wu et al. 2012) 来捕获网络的全局结构。方法就是计算归一化的吸收概率矩阵 $P = (L + \alpha \Lambda)^{-1}$，$P_{i, j}$ 是从顶点 $i$ 出发被吸收到顶点 $j$ 的概率，表示 $i$ 和 $j$ 有多大的可能性属于同一类。然后我们对每类 $k$，计算可信向量 $\mathbf{p} = \sum_{j \in S_k} P_{:, j}$，其中 $\mathbf{p} \in \mathbb{R}^n$，$p_i$ 是顶点 $i$ 属于类 $k$ 的概率。最后，找到 $t$ 个最可信的顶点把他们加到训练集的类 $k$ 中。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deeper-insights-into-graph-convolutional-networks-for-semi-supervised-learning/Alg1.JPG"
loading="lazy"
alt="Alg1"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GCN Self-Training&lt;/strong&gt;
另一种方法就是先训练一个 GCN，然后使用这个 GCN 去预测，根据预测结果的 $\text{softmax}$ 分数选择可信的样本，加入到训练集中。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/deeper-insights-into-graph-convolutional-networks-for-semi-supervised-learning/Alg2.JPG"
loading="lazy"
alt="Alg2"
&gt;&lt;/p&gt;</description></item><item><title>Dynamic Bike Reposition: A Spatio-Temporal Reinforcement Learning Approach</title><link>https://davidham3.github.io/blog/p/dynamic-bike-reposition-a-spatio-temporal-reinforcement-learning-approach/</link><pubDate>Mon, 08 Oct 2018 21:28:18 +0000</pubDate><guid>https://davidham3.github.io/blog/p/dynamic-bike-reposition-a-spatio-temporal-reinforcement-learning-approach/</guid><description>&lt;p&gt;KDD 2018.强化学习处理共享单车调度问题。&lt;/p&gt;
&lt;h1 id="abstract"&gt;ABSTRACT
&lt;/h1&gt;&lt;p&gt;共享单车系统在很多城市都开始使用了，尽管拥挤和空的站点都会导致客户的流失。当前，运营者试图在系统运行中不断地在站点间调度车辆。然而，如何在一个长时间的范围内有效地调度自行车使得乘客的流失最少还没有得到解决。我们提出了一个基于时空强化学习方法的自行车调度模型解决这个问题。首先，一个互相依赖且内部平衡的聚类算法用来对站点聚类。类簇有两点属性，每个类簇类内平衡而且类间独立。因为在很大的系统中有很多三轮车调整自行车，聚类对于减少问题的复杂度来说很重要。其次，我们给每个类簇分配了多辆三轮车，对类内自行车进行调度。我们设计了一个时空强化学习模型对每个类簇进行策略的学习，目标是在长时间范围内降低用户的流失。为了学习每个模型，我们设计了一个深度神经网络来估计它的最优长期价值函数，最优策略可以从这个函数中轻松地推导出来。除了将模型定义成多智能体的方式，我们还通过两个时空剪枝规则减少了训练的复杂度。其三，我们基于两个预测其设计了一个系统模拟器来预测训练和评估调度模型。在Citi Bike的真实数据集上的实验验证了我们的模型的有效性。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 INTRODUCTION
&lt;/h1&gt;&lt;p&gt;共享单车系统给公民提供了便利的出行方式。用户可以在一个随机的站点租或返还自行车，通过刷卡，生成一条单车使用记录。然而，因为在一个城市内的单车使用是非常不平衡的，系统中经常出现没有车的空站点以及缺少可用停车位的拥挤站点，导致乘客的流失。当前，系统运营者使用 &lt;em&gt;dynamic bike reposition&lt;/em&gt; 来处理这个问题，也就是，在系统运行中使用三轮车不断的在站点间调整车辆。然而，如何在长时间范围内调整车辆使得顾客流失最少还是一个问题。实时监测并不是一个好的方案，因为在观测到不平衡后对车辆重新分配太晚了。仅仅基于单车使用预测来调度只会导致一个贪心且短视的策略，在长时间来看不会是最优的。我们总结了解决这个问题的三个挑战。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;A bike-sharing system is complex and dynamic.&lt;/strong&gt;&lt;/em&gt; 系统中经常有几十辆三轮车在几百个站点间调度车辆。在这么一个大的系统内协同调度很复杂，更不用说系统是在运行中保持动态的情况了。难以预测系统的动态性有三点原因：1) 图1. A 展示出了一个月每个小时的租车需求。可以看到，每天的租用模式变化得很剧烈，受多个复杂的因素影响，比如天气、事件以及站点间的相关性。2) 很多移动看起来是随机的。图1. B的第一张图表示出了历史通勤，也就是2016年4月到10月每个工作日的早上这段期间，平均至少发生一次的移动，只占了18%。我们用图1. C的例子解释了这种现象：从A到B，站点间的移动有12种可能，用户一般会根据哪个站点有可用的车辆或车位，选择随机的一条路线，使12条路线中的一条变成可能的频繁移动路线。3) 影响车辆使用的外部因素非常不平衡，比如，晴天时长要比雨天时长多很多。因此，在每个条件下训练一个学习器不能保证在次要条件下的准确性。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/dynamic-bike-reposition-a-spatio-temporal-reinforcement-learning-approach/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;A single bike reposition has long-term effect.&lt;/strong&gt;&lt;/em&gt; 一个简单的调度是好是坏不是那么简单就能判断的。我们将用图2的两个例子详细描述一下，红圈表示没有可用车位的站点，绿圈表示一个空的站点；带有一个数字和一个时间标记的实线箭头表示在那个时段会有多少辆车从起点租用并返还到目的地；虚线箭头表示了一个三轮车的调度是如何进行的；$t_0 &amp;lt; t_1 &amp;lt; t_2$。首先，一个简单的调度会影响系统内的车辆使用很长的一段时间。如图2. A) 所示，如果一个三轮车到了空站点 $s_1$，在 $t_0$ 时段放了5辆车，在 $s_1$ 的可用车辆就变成了$5$，在 $t_1$ 时段 $s_1$ 可以服务 $5$ 位即将到来的租车者；这5个租车者骑车到 $s_2$ 在 $t_1 &amp;lt; t_2$ 还车，4位到来的租车者在 $t_2$ 时段 $s_2$ 车站想租车的也可以使用那些还回来的车。因此，一个调度可以服务的额外用户的数量很难估计。其次，当前的调度会影响以下情况。如图2. B) 所示，如果一个三轮车到站点 $s_1$ 在 $t_0$ 时段带走了 9 辆自行车，那里的可用车位就变成了 9，因此9个用户就可以在 $t_1$ 时段把他们的自行车还到 $s_1$。然而，因为 $s_1$ 离 $s_3$ 太远，在完成picking up之后，这辆三轮车不能在 $t_2$ 时段之前将5辆自行车运送到空站点 $s_3$，来服务5位即将到来的租客。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/dynamic-bike-reposition-a-spatio-temporal-reinforcement-learning-approach/Fig2.JPG"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Uncertainties in partical reposition.&lt;/strong&gt;&lt;/em&gt; 在实际的调度过程中有很多不确定因素。尽管我们可以预测系统的动态，我们不能保证预测与实际完全一样，因为模型会有错误以及随机噪声。此外，完成一次调度的时间也是不同的，比如，从 $s_1$ 运送车辆到 $s_2$ 今天可能需要10分钟，明天可能就要15分钟，尽管方法可能一样。这可能是由于变化的外部因素导致，比如，恶劣的天气状况以及交通的拥挤程度，或是随机噪声。因为动态调度是在系统运行的时候工作的，时间很重要，这也可以从上面两个例子看出来。这些不确定因素，还有长期影响，使得优化模型非常复杂，甚至是无法工作。&lt;/p&gt;
&lt;p&gt;我们提出了一个基于时空强化学习的动态调度模型来解决这三个挑战。我们的贡献可以归纳为4点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们提出了一个两步聚类算法，称为 Inter-Independent Inner-Balance算法，简称 IIIB。这个算法首先在系统中迭代地将独立的站点聚类生成小的功能区域，保证每个区域更稳定的租车需求以及车辆移动模式。其次，这个算法根据区域间的移动，将这些区域聚类成组，保证每个类簇是类内平衡且类间独立的。将整个系统分为各个类簇，极大地减少了问题的复杂程度。&lt;/li&gt;
&lt;li&gt;我们基于两个预测器生成了一个系统模拟器。一个是O-Model，通过一个基于相似度的KNN模型预测每个区域的租车需求，考虑复杂的影响因素，解决了不平衡样本的问题。另一个模型是I-Model，通过一个基于车辆移动的推断方法，预测每个区域的还车需求。&lt;/li&gt;
&lt;li&gt;我们提出了一个时空强化学习模型，STRL，为每个类簇学习一个最优的类内调度策略。STRL的状态通过捕获系统动态性以及实时的不确定性仔细地设计出来。因为状态以及行动空间很大，我们设计了一个深度神经网络为每个STRL估计最优的长期价值函数，通过这个可以推断出它的最优调度策略。除了将模型定义成多智能体的方式，我们还通过两个时空剪枝规则减少了训练的复杂度。&lt;/li&gt;
&lt;li&gt;我们在Citi Biki 2016年4月到10月的数据集上证明了我们的模型比baselines更有效。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="2-overview"&gt;2 OVERVIEW
&lt;/h1&gt;&lt;p&gt;这部分定义了符号以及相关术语&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/dynamic-bike-reposition-a-spatio-temporal-reinforcement-learning-approach/Table1.JPG"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;
&lt;h2 id="21-preliminary"&gt;2.1 Preliminary
&lt;/h2&gt;&lt;p&gt;&lt;em&gt;Definition 1&lt;/em&gt; Transition. 一个移动 $f_{ij} = (s_i, s_j, \tau_i, \tau_j)$ 是一辆自行车的使用记录，描述了一辆自行车从地点 $S_i$ 在时刻 $\tau_i$ 被租用以及在地点 $s_j$ 时刻 $\tau_j$ 返还。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Definition 2&lt;/em&gt; Demand. 时段 $t$ 的地点 $s_i$ 的租用需求 $o_{i,t}$ 是想在 $s_i$ 时段 $t$ 内租用自行车的顾客的个数，包括成功以及失败的。时段 $t$ 内地点 $s_i$ 的返还需求 $r_{i,t}$的定义类似。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Definition 3&lt;/em&gt; Episode. 一个时段 $E$ 是一天的一个长时段，在这个时段中，我们想最小化总的客户流失。我们在3.1.2中详细地定义了我们问题中的 Episodes，保证了一些约束，而不是随机选取的。&lt;/p&gt;
&lt;h2 id="22-framework"&gt;2.2 Framework
&lt;/h2&gt;&lt;p&gt;如图3所示，我们的模型包括了一个离线学习过程以及一个在线调度过程。这个学习过程有三个部分，分别是IIIB聚类算法，系统模拟器生成过程以及每个类簇的STRL模型。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;IIIB Clustering Algorithm.&lt;/strong&gt;&lt;/em&gt; 为了解决第一个问题，也就是一个系统很大且很复杂，我们提出了一个两步IIIB聚类算法。首先对那些相近且有相似移动模式的站点聚类，在系统中生成小的功能区域。然后基于他们区域间的迁移规律，将区域聚类成组。给每个类簇分配多辆三轮车完成类簇内区域间的调度，而不是类间的自行车调度。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Simulator Generation.&lt;/strong&gt;&lt;/em&gt; 为了训练和评估调度模型，我们基于两个预测器生成了一个系统模拟器，也就是分别预测每个区域的租车需求和还车需求的O-Model和I-Model。对于一个指定的时段，比如周六早上7点到7点半，我们先根据历史的天气统计结果生成一个可能的天气状况，比如晴天，然后O-Model预测每个区域周六早上7点到7点半晴天的租车需求。基于这些预测，每个区域的租车需求由泊松过程模拟。每次一辆自行车被租用，I-Model就会估计它的目的地区域以及到达时间，并且追踪它。每个区域的还车事件通过连续地检查自行车是否到达那里来生成。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;STRL Model.&lt;/strong&gt;&lt;/em&gt; 我们提出了一个STRL模型，对每个类簇学习一个最优的类内调度方案。我们这个基于强化学习的模型是多智能体形式。每次一个三轮车完成它最后的调度，它会不等待其他调度的完成，紧接着开始一个由方案生成的新的调度。新的调度基于当前状态生成，这个调度会很仔细地定义，来捕获系统动态性和实时的不确定性。一个状态包含多个因素，如，每个区域当前自行车和车库的可用性；实时预测的租车和还车需求；三轮车的状态，包含它自身的以及其他的；当前的时间，等等。我们设计了一个深度神经网络为每个STRL估计最优的长期价值函数，通过这个我们能推导出最优的调度策略。网络在系统模拟器迭代地训练出来，在图3中用灰色高亮了出来。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/dynamic-bike-reposition-a-spatio-temporal-reinforcement-learning-approach/Fig3.JPG"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Online Reposition.&lt;/strong&gt;&lt;/em&gt; 在学习过程之后，我们在每个类簇上会获得一个神经网络。在在线过程中，当一个三轮车需要一个新的调度时，我们先确定它在哪个类簇中，通过O模型和I模型生成它的当前状态。然后，对应的网络给当前状态中每个可能的调度估计最优的长期价值。选择最大价值的调度并返回。&lt;/p&gt;
&lt;h1 id="3-methodology"&gt;3 METHODOLOGY
&lt;/h1&gt;&lt;h2 id="31-iiib-clustering-algorithm"&gt;3.1 IIIB Clustering Algorithm
&lt;/h2&gt;&lt;h3 id="311-region-generation"&gt;3.1.1 Region Generation
&lt;/h3&gt;&lt;p&gt;如图1. C)所示的例子，站点间的随机迁移使得车站间的单车调度不是那么有意义，因为我们只需要保证在 $s_1$ 或 $s_2$ 或 $s_3$ 有可用的自行车，在 $s_4$ 或 $s_5$ 或 $s_6$ 或 $s_7$ 有可用的车位。考虑每个站点的车辆和车位的可用性，从 $A$ 到 $B$ 的用户可以选择在哪里租车并且还车。受到这个现象的启发，我们对这个区域周围的几个车站聚类，对目标区域周围的车站进行聚类，生成两个小的区域，也就是 $s_1$，$s_2$ 和 $s_3$ 形成了一个区域，$s_4$，$s_5$，$s_6$ 和 $s_7$ 形成了另一个区域。然后，我们只需要保证每个区域车辆和车位的可用性即可。我们认为一个区域的租车需求比一个单独的车站更稳定，更规律；而且，两个区域间的迁移比两个站点间的迁移更频繁。&lt;/p&gt;
&lt;p&gt;为了正式地阐述这个想法，我们基于两个约束在一个系统中生成了一些区域。1) 一个区域的站点应该和其他的站点相近，保证这个区域内顾客的方便。2) 一个区域内的站点应该有相似的OD区域，使得区域间的迁移更专一且更频繁。生成这些的区域的方法是一个迭代的方法，称为二部聚类算法[2]，这个算法会基于车站的位置和迁移模式进行聚类。&lt;/p&gt;
&lt;p&gt;基于获得的这些区域，我们分析了 Citi Bike 中历史的车辆使用数据，证实了上述的两个优势。如图1. A)底部所示，一个区域的租车需求更稳定，更规律，因此更容易地精确预测。随机迁移的问题也可用得到解决。图1. B)右图展示了2016年4月到10月工作日的早上区域间的通勤，占了56%。可以看到，得到的区域间迁移模式更简单，使得还车需求预测更简单且更精确。这里获得的区域可用看作是城市中小的功能区，比如居民区周围的车站很可能组成一个区域，而在工作区周围的可能形成另一个。我们对这些区域聚类成组，而不是在整个系统中的这些屈居间直接调度，而且因为两个原因，我们只在类簇内开展调度。1) 聚类可用进一步减少问题的复杂度。2) 司机一般只对城市内的一个区域比较熟悉。&lt;/p&gt;
&lt;h3 id="312-iiib-clustering-insight"&gt;3.1.2 IIIB Clustering Insight
&lt;/h3&gt;&lt;p&gt;获得到的类簇应该有两个性质，每个类簇的内部平衡以及类簇间的相互依赖。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Inner-Balance.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</title><link>https://davidham3.github.io/blog/p/convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting/</link><pubDate>Thu, 27 Sep 2018 15:11:55 +0000</pubDate><guid>https://davidham3.github.io/blog/p/convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting/</guid><description>&lt;p&gt;NIPS 2015. 将 FC-LSTM 中的全连接换成了卷积，也就是将普通的权重与矩阵相乘，换成了卷积核对输入和隐藏状态的卷积，为了能捕获空间信息，将输入变成了4维的矩阵，后两维表示空间信息。两个数据集：Moving-MNIST 和 雷达云图数据集。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1506.04214" target="_blank" rel="noopener"
&gt;Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;降雨量预测的目标是预测未来一个局部区域再相对短的一个时间段的降雨密度。之前几乎没有研究从机器学习角度研究这个重要而且很有挑战的问题。在这篇论文中，我们将降雨量预测问题定义成一个时空序列预测问题，输入和预测都是时空序列。通过扩展 &lt;em&gt;fully connected LSTM&lt;/em&gt; (FC-LSTM)，在输入到隐藏与隐藏到隐藏的变换都加入卷积结构，我们提出了 &lt;em&gt;convolutional LSTM&lt;/em&gt; (ConvLSTM)，用它做了一个端到端的模型来预测降雨量。实验表明我们的 ConvLSTM 网络比 FC-LSTM 以及最先进的 ROVER 算法在降水量预测上能更好地捕获时空关系。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1. Introduction
&lt;/h1&gt;&lt;h1 id="2-preliminaries"&gt;2. Preliminaries
&lt;/h1&gt;&lt;h2 id="21-forumulation-of-precipitation-nowcasting-problem"&gt;2.1 Forumulation of Precipitation Nowcasting Problem
&lt;/h2&gt;&lt;p&gt;降水量预测的目标是使用之前的雷达声波序列预测一个区域（如香港、纽约、东京）未来定长（时间）的雷达图。在实际应用中，雷达图通常从气候雷达每6到10分钟获得一次，然后预测未来1到6小时，也就是说，预测6到60帧。从机器学习的角度来看，这个问题可以看作是时空序列预测问题。&lt;/p&gt;
&lt;p&gt;假设我们在一个空间区域观测了一个动态系统，由一个 $M \times N$ 的网格组成，$M$ 行 $N$ 列。在网格的每个单元格内部随着时间的变化，有 $P$ 个测量值。因此，观测值在任意时刻可以表示成一个张量 $\mathcal{X} \in \mathbf{R}^{P \times M \times N}$，其中 $\mathbf{R}$ 表示观测到的特征的定义域。如果我们周期性的记录观测值，我们可以得到一个序列 $\hat{\mathcal{X}_1}, \hat{\mathcal{X}_2}, &amp;hellip;, \hat{\mathcal{X}_t}$。这个时空序列预测问题是给定前 $J$ 个观测值，预测未来长度为 $K$ 的序列：&lt;/p&gt;
$$\tag{1}
\tilde{\mathcal{X}}\_{t+1}, ..., \tilde{\mathcal{X}}\_{t+K} = \mathop{\arg \max}\_{\mathcal{X}\_{t+1}, ...\mathcal{X}\_{t+K}} p(\mathcal{X}\_{t+1}, ..., \mathcal{X}\_{t+K} \mid \hat{\mathcal{X}}\_{t-J+1}, \hat{\mathcal{X}}\_{t-J+2}, ..., \hat{\mathcal{X}}\_t)
$$&lt;p&gt;对于降雨量预测，每个时间戳的观测值是一个2D雷达地图。如果我们将地图分到平铺且不重合的部分，将每个部分内的像素看作是它的观测值（图1），预测问题就会自然地变成一个时空序列预测问题。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;我们注意到我们的时空序列预测问题与一步时间序列预测问题不同，因为我们问题的预测目标是一个包含时间和空间结构的序列。尽管长度为$K$的序列中的自由变量的数量可以达到$O(M^KN^KP^K)$，实际上我们可以挖掘可能的预测值的空间结构，减小维度，使问题变得容易处理。&lt;/p&gt;
&lt;h2 id="22-long-short-term-memory-for-sequence-modeling"&gt;2.2 Long Short-Term Memory for Sequence Modeling
&lt;/h2&gt;&lt;p&gt;这篇论文，我们使用 FC-LSTM 的公式[11]&lt;/p&gt;
$$\tag{2}
\begin{aligned}
i\_t &amp;= \sigma ( W\_{xi} x\_t + W\_{hi} h\_{t-1} + W\_{ci} \circ c\_{t-1} + b\_i) \\
f\_t &amp;= \sigma ( W\_{xf} x\_t + W\_{hf} h\_{t-1} + W\_{cf} \circ c\_{t-1} + b\_f) \\
c\_t &amp;= f\_t \circ c\_{t-1} + i\_t \circ \mathrm{tanh}(W\_{xc} x\_t + W\_{hc} h\_{t-1} + b\_c) \\
o\_t &amp;= \sigma ( W\_{xo} x\_t + W\_{ho} h\_{t-1} + W\_{co} \circ c\_t + b\_o ) \\
h\_t &amp;= o\_t \circ \mathrm{tanh}(c\_t)
\end{aligned}
$$&lt;p&gt;多个 LSTM 可以堆叠，对复杂的结构在时间上拼接。&lt;/p&gt;
&lt;h1 id="3-the-model"&gt;3 The Model
&lt;/h1&gt;&lt;p&gt;尽管 FC-LSTM 层已经在时间关系上表现的很有效了，但是在空间数据上有很多的冗余。为了解决这个问题，我们提出了 FC-LSTM 的扩展，在输入到隐藏，以及隐藏到隐藏的变换上有了卷积的结构。通过堆叠多个 ConvLSTM 层，形成一个 encoding-forecasting 结构，我们可以构建一个不仅可以处理降雨量预测问题，还可以处理更一般的时空序列预测问题的模型。&lt;/p&gt;
&lt;h2 id="31-convolutional-lstm"&gt;3.1 Convolutional LSTM
&lt;/h2&gt;&lt;p&gt;FC-LSTM 的主要问题是处理时空数据的时候，它的全连接层没有对空间信息进行编码。为了解决这个问题，我们的设计中的一个特征就是，所有的输入 $\mathcal{X}_1, &amp;hellip;, \mathcal{X}_t$，细胞状态 $\mathcal{C}_1, &amp;hellip;, \mathcal{C}_t$，隐藏状态 $\mathcal{H}_1, &amp;hellip; \mathcal{H}_t$，还有门 $i_t, f_t, o_t$ 都是三维的张量，而且后两维都是空间维（行和列）。为了更好的理解输入和状态，我们可以把它们想象成在空间网格站立的向量。ConvLSTM 通过输入和局部邻居的上一个状态决定了一个特定细胞的未来状态。通过在输入到隐藏，隐藏到隐藏中使用一个卷积操作器就可以轻松的实现（图2）。ConvLSTM的重要的公式如（3）所示（下面的公式），$\ast$表示卷积操作，$\circ$表示Hadamard积：&lt;/p&gt;
$$\tag{3}
\begin{aligned}
i\_t &amp;= \sigma( W\_{xi} \ast \mathcal{X}\_t + W\_{hi} \ast \mathcal{H}\_{t-1} + W\_{ci} \circ \mathcal{C}\_{t-1} + b\_i )\\
f\_t &amp;= \sigma( W\_{xf} \ast \mathcal{X}\_t + W\_{hf} \ast \mathcal{H}\_{t-1} + W\_{cf} \circ \mathcal{C}\_{t-1} + b\_f )\\
\mathcal{C}\_t &amp;= f\_t \circ \mathcal{C}\_{t-1} + i\_t \circ \mathrm{tanh}(W\_{xc} \ast \mathcal{X}\_t + W\_{hc} \ast \mathcal{H}\_{t-1} + b\_c)\\
o\_t &amp;= \sigma( W\_{xo} \ast \mathcal{X}\_t + W\_{ho} \ast \mathcal{H}\_{t-1} + W\_{co} \circ \mathcal{C}\_t + b\_o )\\
\mathcal{H}\_t &amp;= o\_t \circ \mathrm{tanh}(\mathcal{C}\_t)
\end{aligned}
$$&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting/Fig2.JPG"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;p&gt;如果我们将状态看作是移动物体的隐藏表示，带有一个更大的变换卷积核的 ConvLSTM 应该能捕获更快的移动，而小的核能捕获慢的移动。同时，如果我们用[16]的角度来看，由式2表示的传统的 FC-LSTM 的输入、细胞输出以及隐藏状态可以看作是一个后两维都是1的三维张量。这样的话，FC-LSTM实际上是所有特征都站立在一个单独细胞上 ConvLSTM 的一个特例。&lt;/p&gt;
&lt;p&gt;为了确保状态和输入有相同个数的行和列，需要在卷积操作之前加入 padding 操作。这里隐藏状态在边界点的 padding 可以看作是使用 &lt;em&gt;state of the outside world&lt;/em&gt; 来计算。通常，在第一个输入来之前，我们将 LSTM 的所有状态初始化为0，对应未来的 “total ignorance”。相似地，如果我们在隐藏状态上用 zero-padding，我们实际上将 &lt;em&gt;state of the outside world&lt;/em&gt; 设定为0，而且假设没有关于外部的先验知识。通过在状态上加 padding，我们可以不同地对待边界点，很多时候这都是有用的。举个例子，假设我们的系统正在观测一个被墙环绕的移动的球。尽管我们不能看到这些墙，但是我们可以通过观察球的一次次反弹推测它们的存在，如果边界点像内部的点一样有相同的状态变化动态性，那这就几乎不可能了。&lt;/p&gt;
&lt;h2 id="32-encoding-forecasting-structure"&gt;3.2 Encoding-Forecasting Structure
&lt;/h2&gt;&lt;p&gt;就像 FC-LSTM，ConvLSTM 可以使用块对复杂的结构建模。对于我们的时空序列预测问题，我们使用图3这样的结构，由两个网络组成，一个编码网络，一个预测网络。就像[21]，预测网络的初始状态和细胞输出从编码网络的最后一个状态复制过来。两个网络都通过堆叠多个 ConvLSTM 层构成。因为我们的预测目标与输入的维度相同，我们将预测网络所有的状态拼接，放到一个 $1 \times 1$ 的卷积层中，生成最后的预测结果。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting/Fig3.JPG"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;我们使用像[23]一样的观点解释这个结构。编码 LSTM 将整个输入序列压缩到一个隐藏状态的张量中，预测 LSTM 解压了这个状态，给出了最后的预测：&lt;/p&gt;
$$\tag{4}
\begin{aligned}
\tilde{\mathcal{X}\_{t+1}}, ..., \tilde{\mathcal{X}\_{t+K}} &amp;= \mathop{\arg \max}\_{\mathcal{X}\_{t+1}, ..., \mathcal{X}\_{t+K}} p(\mathcal{X}\_{t+1}, ..., \mathcal{X}\_{t+K} \mid \hat{\mathcal{X}}\_{t-J+1}, \hat{\mathcal{X}}\_{t-J+2}, ..., \hat{\mathcal{X}}\_t) \\
&amp;\approx \mathop{\arg \max}\_{\mathcal{X}\_{t+1}, ..., \mathcal{X}\_{t+K}} p(\mathcal{X}\_{t+1}, ..., \mathcal{X}\_{t+K} \mid f\_{encoding} (\hat{\mathcal{X}}\_{t-J+1}, \hat{\mathcal{X}}\_{t-J+2}, ..., \hat{\mathcal{X}}\_t)) \\
&amp;\approx g\_{forecasting}(f\_{encoding}(\hat{\mathcal{X}}\_{t-J+1}, \hat{\mathcal{X}}\_{t-J+2}, ..., \hat{\mathcal{X}}\_t))
\end{aligned}
$$&lt;p&gt;这个结构与[21]中的 LSTM 未来预测模型相似，除了我们的输入和输出元素都是三维的张量，保留了所有的空间信息。因为网络堆叠了多个 ConvLSTM 层，它由很强的表示能力可以在复杂的动态系统中给出预测，如降雨量预测问题。&lt;/p&gt;
&lt;h1 id="4-experiments"&gt;4. Experiments
&lt;/h1&gt;&lt;p&gt;我们将我们的模型 ConvLSTM 与 FC-LSTM 在一个人工生成的 Moving-MNIST 数据集上做了对比，对我们的模型进行一个初步的了解。我们使用了不同的层数以及不同的卷积核大小，也研究了一些 &amp;ldquo;out-of-domain&amp;rdquo; 的情况，如[21]。为了验证我们的模型在更有挑战的降雨量预测问题上的有效性，我们构建了一个新的雷达声波图，在几个降雨量预测的指标上，比较了我们的模型和当前最先进的 ROVER 算法。结果显示这两个数据集上：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ConvLSTM 比 FC-LSTM 在处理时空关系时更好&lt;/li&gt;
&lt;li&gt;隐藏状态到隐藏状态的卷积核的尺寸大于1对于捕获时空运动模式来说很重要&lt;/li&gt;
&lt;li&gt;深、且参数少的模型能生成更好的结果&lt;/li&gt;
&lt;li&gt;ConvLSTM 比 ROVER 在降雨量预测上表现的更好。&lt;/li&gt;
&lt;/ol&gt;</description></item><item><title>Large-Scale Learnable Graph Convolutional Networks</title><link>https://davidham3.github.io/blog/p/large-scale-learnable-graph-convolutional-networks/</link><pubDate>Mon, 17 Sep 2018 15:22:43 +0000</pubDate><guid>https://davidham3.github.io/blog/p/large-scale-learnable-graph-convolutional-networks/</guid><description>&lt;p&gt;KDD 2018.将图结构数据变换到网格状数据中，使用传统的一维卷积进行卷积。变换的方式是：针对每个特征的大小，对邻居结点进行排序，取这个特征前k大的数作为它邻居这列特征的k个值。如果邻居不够，那就用0来补。这样就能得到该顶点的邻居信息，组成一个矩阵，然后使用一维卷积。但是作者没说为什么非要取最大的k个数。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1808.03965?context=stat.ML" target="_blank" rel="noopener"
&gt;Large-Scale Learnable Graph Convolutional Networks&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="摘要"&gt;摘要
&lt;/h1&gt;&lt;p&gt;卷积神经网络在网格数据上取得了很大的成功，但是在学习像图这样的数据的时候就面临着很多的挑战。CNN中，可学习的局部滤波器可以自动地捕获高层次的特征。滤波器的计算需要感受野内有固定数量的单元。然而，在图结构中，邻居单元的数量不固定，而且邻居也不有序，所以阻碍了卷积的操作。我们提出了可学习图卷积层(learnable graph convolutional layer LGCL)来解决这些挑战。基于值的排序，LGCL为每个特征自动地选择固定数量的邻居结点，以此将图结构数据变换到1维的网格结构中，然后就可以在图上使用常规的卷积操作了。为了能让模型在大尺度的图上训练，我们提出了一个子图训练方法来减少过多的内存和计算资源的开销。在顶点分类任务上，不论是transductive 还是 inductive，表现得都更好一些。我们的结果展示出了我们的子图训练方法比前人的方法更高效。&lt;/p&gt;
&lt;h1 id="3-methods"&gt;3. methods
&lt;/h1&gt;&lt;h2 id="31-challenges-of-applying-convolutional-operations-on-graph-data"&gt;3.1 Challenges of Applying Convolutional Operations on Graph Data
&lt;/h2&gt;&lt;p&gt;为了让传统的卷积操作可以应用在图上，需要解决两个图结构数据和网格数据的差异。首先，顶点的邻居数量通常会变化。其次，我们不能对邻居顶点进行排序，因为他们没有可供排序的信息。举个例子，社交网络中，每个人都可以看作是一个顶点，边表示人与人之间的关系。显然，每个顶点的邻居顶点数量是不同的，因为人们可以有不同数量的朋友。而且，如果没有额外的信息，很难对他们进行排序。&lt;/p&gt;
&lt;p&gt;网格数据可以看作是一种特殊的图结构数据，每个顶点有固定数量的邻居。因为卷积操作是直接应用在图像这样的网格数据上。为了看清楚固定邻居数量以及排序信息的重要性，我们举个例子，有一个$3 \times 3$的卷积核，扫描一张图像。我们将这张图片考虑成一个特殊的图，每个像素是一个顶点。在扫描的过程中，计算包括了中心结点和周围8个邻居结点的计算。这8个顶点在这个特殊的图中通过边连接到中心结点。与此同时，我们使用他们和中心结点的相对位置对他们排序，这对于卷积操作很重要，因为在扫描的过程中，滤波器的权重和图中的顶点要一一对应。举个例子，在上面的例子中，$3 \times 3$的卷积核，左上角的权重应该总是对应中心节点左上方的邻居结点。没有这样的排序信息，卷积的输出结果就不再是确定的。从刚才的讨论中可以看到传统卷积在图结构数据上应用的挑战。为了解决这两个挑战，我们提出了一个方法将图结构数据变换到网格数据内。&lt;/p&gt;
&lt;h2 id="32-learnable-graph-convolutional-layers"&gt;3.2 learnable Graph Convolutional Layers
&lt;/h2&gt;&lt;p&gt;为了让传统卷积可以在图上可用，我们提出了LGCL。LGCL的layer-wise传播规则写为：&lt;/p&gt;
$$\tag{3}
\tilde{X}\_l = g(X\_l, A, k),\\
X\_{l+1} = c(\tilde{X}\_l)
$$&lt;p&gt;其中，$A$是邻接矩阵，$g(\cdot)$使用了$k$-largest Node Selection，将图结构数据映射到网格结构，$c(\cdot)$表示1维常规的CNN，将顶点信息聚合，为每个顶点输出了一个新的特征向量。我们会在下面分开讨论$g(\cdot)$和$c(\cdot)$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;$k$-largest Node Selection&lt;/strong&gt;. 我们提出了一个新的方法称为$k$-largest Node Selection，将图结构映射到网格数据上，其中$k$是LGCL的超参数。在这个操作之后，每个顶点的邻居信息聚合，表示成一个有$(k+1)$个位置的1维的网格状。变换后的数据会输入到CNN中来生成新的特征向量。&lt;/p&gt;
&lt;p&gt;假设有行向量$x^1_l, x^2_l, &amp;hellip;, x^N_l$的$X_l \in \mathbb{R}^{N \times C}$，表示$N$个顶点的图，每个顶点有$C$个特征。邻接矩阵$A \in \mathbb{N}^{N \times N}$，$k$为定值。顶点$i$的特征向量是$x^i_l$，它有$n$个邻居。通过在$A$中的一个简单查找，我们可以获得这些邻居结点的下标，$i_1, i_2, &amp;hellip;, i_n$。对它们对应的特征向量$x^{i_1}_l, x^{i_2}_l, &amp;hellip;, x^{i_n}_l$进行拼接，得到$M^i_l \in \mathbb{R}^{n \times C}$。假设$n \geq k$，就没有泛化上的损失。如果$n &amp;lt; k$，我们可以使用全为0的列，给$M^i_l$加padding。$k$-largest node selection是在$M^i_l$上做的：也就是，对于每列，我们排出$n$个值，然后选最大的$k$个数。我们就可以得到一个$k \times C$的输出矩阵。因为$M^i_l$表示特征，这个操作等价于为每个特征选择$k$个最大值。通过在第一行插入$x^i_l$，输出变为$\tilde{M}^i_l \in \mathbb{R}^{(k+1) \times C}$。如图2左部分。通过对每个顶点重复这个操作，$g(\cdot)$将$X_l$变为$\tilde{X}_l \in \mathbb{R}^{N \times (k + 1) \times C}$。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/large-scale-learnable-graph-convolutional-networks/Fig2.JPG"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;p&gt;注意，如果将$N$，$(k+1)$，$C$分别看作是batch size，spatial size，通道数，那么$\tilde{X}_l$可以看作是1维网格状的结构。因此，$k$个最大顶点选择函数$g(\cdot)$成功地将图结构变换为网格结构。这个操作充分利用了实数的自然顺序信息，使得每个顶点有固定数量的有序邻居。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1-D Convolutional Neural Networks&lt;/strong&gt;. 就像3.1节讨论的，传统的卷积操作可以直接应用到网格状的数据上。$\tilde{X}_l \in \mathbb{R}^{N \times (k + 1) \times C}$是1维的数据，我们部署一个一维CNN模型$c(\cdot)$。LGCL基本的功能是聚合邻居信息，为每个顶点更新特征。后续的话，它需要$X_{l + 1} \in \mathbb{R}^{N \times D}$，其中$D$是更新后的特征空间的维度。一维CNN $c(\cdot)$ 使用$\tilde{X}_l \in \mathbb{R}^{N \times (k + 1) \times C}$作为输入，输出一个$N \times D$的矩阵，或是$N \times 1 \times D$的矩阵。$c(\cdot)$可以将空间维度从$(k+1)$减小到$1$。&lt;/p&gt;
&lt;p&gt;注意，$N$看作是batch size，与$c(\cdot)$的设计无关。结果就是，我们只聚焦于一个样本，也就是图中的一个顶点。对于顶点$i$，变换得到的输出是$\tilde{M}^i_l \in \mathbb{R}^{(k + 1) \times C}$，是$c(\cdot)$的输入。由于任何一个卷积核大于1且没有padding的卷积都会减少空间的大小，最简单的$c(\cdot)$只有一个卷积核大小为$(k+1)$的卷积，没有padding。输入和输出的通道数分别为$C$和$D$。同时，可以部署任意一个多层CNN，得到最后的输出的维度是$1 \times D$。图2右侧展示了一个两层CNN的例子。再对所有的$N$个顶点使用一次$c(\cdot)$，输出$X_{l+1} \in \mathbb{R}^{N \times D}$。总结一下，我们的LGCL使用$k$最大顶点选择以及传统的一维CNN，将图结构变换到网格数据，实现了对每个顶点进行的特征聚合和特征过滤。&lt;/p&gt;
&lt;h2 id="33-可学习的图卷积网络"&gt;3.3 可学习的图卷积网络
&lt;/h2&gt;&lt;p&gt;越深的网络一般会产生越好的结果。然而，之前在图上的深度模型，如GCN，只有两层。尽管随着深度的增加，它们的性能有有所下降[Kipf &amp;amp; Welling 2017]，我们的LGCL可以构造的很深，构造出图顶点分类的可学习的图卷积网络。我们基于densely connected convolutional networks(DCNNs)，构造了LGCNs，前者获得了ImageNet分类任务最好的成绩。&lt;/p&gt;
&lt;p&gt;在LGCN中，我们先用一个图嵌入层来生成顶点的低维表示，因为原始输入一般都是高维特征，比如Cora数据集。第一层的图嵌入层本质上就是一个线性变换，表示为：&lt;/p&gt;
$$\tag{4}
X\_1 = X\_0 W\_0
$$&lt;p&gt;其中，$X_0 \in \mathbb{R}^{N \times C_0}$表示高维的输入，$W_0 \in \mathbb{R}^{C_0 \times C_1}$将特征空间从$C_0$映射到了$C_1$。结果就是，$X_1 \in \mathbb{R}^{N \times C_1}$和$C_1 &amp;lt; C_0$。或者，使用一个GCN层来做图嵌入。如第二部分描述的，GCN层中的参数数量等价于传统的图嵌入层中参数的数量。&lt;/p&gt;
&lt;p&gt;在图嵌入层后，我们堆叠多个LGCL，多少个取决于数据的复杂程度。因为每个LGCL只能聚合一阶邻居的信息，也就是直接相连的邻居顶点，堆叠LGCL可以从一个更大的顶点集中获得信息，这也是传统CNN的功能。为了提升模型的性能，帮助训练过程，我们使用skip connections来拼接LGCL的输入和输出。最后，在softmax激活前使用一个全连接层。&lt;/p&gt;
&lt;p&gt;就像LGCN的设计理念，$k$以及堆叠的LGCL的数量是最重要的超参数。顶点的平均度是选择$k$的一个重要参数。LGCL的数量应该依赖任务的复杂度，比如类别的个数，图的顶点数等。越复杂的模型需要越深的模型。&lt;/p&gt;
&lt;h2 id="34-sub-graph-training-on-large-scale-data"&gt;3.4 Sub-Graph Training on Large-Scale Data
&lt;/h2&gt;&lt;p&gt;大部分图上的深度学习模型都有另一个限制。在训练的时候，输入的是所有顶点的特征向量以及整个图的邻接矩阵。图的尺寸大的时候这个矩阵就会变大。这些方法在小尺度的图上表现的还可以。但是对于大尺度的图，这些方法一般都会导致内存和计算资源极大的开销，限制了这些模型的一些应用。&lt;/p&gt;
&lt;p&gt;其他类型的数据集也有相似的问题，比如网格数据。举个例子，图像分割上的深度模型通常使用随机切片的方式来处理大的图片。受到这种策略的启发，我们随机的将图“切分”，使用得到的小图进行训练。然而，尽管一张图片的一个矩形部分很自然地包含了像素的邻居信息。如何处理图中顶点的不规则连接还是一个问题。&lt;/p&gt;
&lt;p&gt;我们提出了子图选择算法来解决大尺度图上计算资源的问题，如算法1所示。给定一个图，我们先采样出一些初始顶点。从它们开始，我们使用广度优先搜索算法，迭代地将邻接顶点扩充到子图内。经过一定次数的迭代后，初始顶点的高阶邻居顶点就会被加进去。注意，我们在算法1中使用一个简单的参数$N_m$。实际上在每个迭代中，我们将$N_m$设置为了不同的值。图4给出了子图选择过程的一个例子。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/large-scale-learnable-graph-convolutional-networks/Alg1.JPG"
loading="lazy"
alt="Algorithm1"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/large-scale-learnable-graph-convolutional-networks/Fig4.JPG"
loading="lazy"
alt="Figure4"
&gt;&lt;/p&gt;
&lt;p&gt;这样随机的切分子图，我们可以在大尺度的图上训练深层模型。此外，我们可以充分利用mini-batch训练方法来加速学习过程。在每轮训练中，我们可以使用子图训练方法采样多个子图，然后把它们放到batch中。对应的特征向量和邻接矩阵组成了网络的输入。&lt;/p&gt;
&lt;h1 id="4-experimental-studies"&gt;4. Experimental studies
&lt;/h1&gt;&lt;p&gt;代码：&lt;a class="link" href="https://github.com/divelab/lgcn/" target="_blank" rel="noopener"
&gt;https://github.com/divelab/lgcn/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="42-experimental-setup"&gt;4.2 Experimental Setup
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Transduction Learning.&lt;/strong&gt; 在transductive learning 任务中，我们像图3一样部署LGCN模型。因为transductive learning数据集使用高维的词袋表示作为顶点的特征向量，输入通过一个图嵌入层来降维。我们这里使用GCN层作为图嵌入层。&lt;/p&gt;</description></item><item><title>Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks</title><link>https://davidham3.github.io/blog/p/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/</link><pubDate>Sat, 11 Aug 2018 10:14:13 +0000</pubDate><guid>https://davidham3.github.io/blog/p/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/</guid><description>&lt;p&gt;NIPS 2015. 在训练seq2seq的时候，比如像机器翻译，训练的时候，每个输出y，它所依据的前一个词，都是正确的。但是在预测的时候，输出的这个词依照的上一个词，是模型输出的词，无法保证是正确的，这就会造成模型的输入和预测的分布不一致，可能会造成错误的累积。本文提出了scheduled sampling来处理这个问题。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1506.03099" target="_blank" rel="noopener"
&gt;Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;循环神经网络可以训练成给定一些输入，输出一些token的模型，比如最近的机器翻译和图像描述。现在训练这些的方法包括给定当前状态和之前的token，最大化序列中每个token的似然。在推理阶段，未知的token会被模型生成的token替代。这种在训练和推断之间的差异，会在生成序列时产生快速积累的误差。我们提出了一个递进学习策略(curriculum learning strategy)轻微地将训练过程进行一些改变，从之前的完全使用一个真实的token进行引导的策略，变成了基本使用生成的token来引导的策略。在几个序列预测的实验上表明我们的方法有很大的提升。此外，它成功的在MSCOCO图像描述2015任务上获得冠军。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 Introduction
&lt;/h1&gt;&lt;p&gt;循环神经网络可以用于处理序列，要么输入，要么输出，或者都可以。尽管他们很难在长期依赖的数据上训练，一些如LSTM的版本可以更好的适应这种问题。事实是，最近在一些序列预测问题上，包括机器翻译，contextual parsing，图像描述甚至视频描述上，这些模型表现的很好。&lt;/p&gt;
&lt;p&gt;在这篇论文中，我们考虑生成可变大小的token的序列的问题，比如机器翻译，目标是给定源语言翻译成目标语言。我们也考虑当输入不是一个序列的问题，如图像描述问题，目标是对给定的图像生成一个文本描述。&lt;/p&gt;
&lt;p&gt;在这两种情况，循环神经网络一般都是通过给定的输入，最大化生成目标序列的似然。实际上，这个是通过给定当前模型的状态和之前的目标token，最大化每个目标token的似然，这使得模型可以在目标token上学习一种语言模型。然而，在推断过程中，真的&lt;em&gt;previous&lt;/em&gt;目标token是无法获取的，这就使得模型需要用它自己生成的token，导致模型在训练和预测时会有差异。通过使用beam search启发式的生成几个目标序列可以缓解这种差异，但是对于连续的状态空间模型，如RNN，不存在动态规划的方法，所以即便是使用beam search，考虑的序列的数量仍然会很小。&lt;/p&gt;
&lt;p&gt;主要问题是，在生成序列时越早出现错误，会导致将这个错误输入进模型，然后会扩大模型的误差，因为模型会将它在训练时未见过的错误考虑到状态空间内。&lt;/p&gt;
&lt;p&gt;我们提出了一个递进学习方法，在对序列预测任务上使用RNN的训练和推测时构建了桥梁。我们提出，改变训练过程，为了逐渐地使模型处理它的错误，使它在推断时也可以进行。这样，模型在训练时会探索更多的情况，因此在推断时会更鲁棒的纠正它的错误，因为它在训练时就学习过这个。我们会展示这个方法在几个序列预测问题上的结果。&lt;/p&gt;
&lt;h1 id="2-proposed-approach"&gt;2 Proposed Approach
&lt;/h1&gt;&lt;p&gt;我们考虑一个监督学习任务，训练集是给定的 $N$ 个样本的输入输出对，$\lbrace X^i, Y^i \rbrace^N_{i=1}$，$X^i$ 是输入，要么静态（图像），要么动态（序列），输出 $Y^i$ 是一个可变数量的token的序列 $y^i_1, y^i_2, &amp;hellip;, y^i_{T_i}$，token属于一个已知的词典。&lt;/p&gt;
&lt;h2 id="21-model"&gt;2.1 Model
&lt;/h2&gt;&lt;p&gt;给定一个输入/输出对儿 $(X, Y)$，log 概率 $P(Y \mid X)$ 可由下式计算：&lt;/p&gt;
$$\tag{1}
\begin{aligned}
\mathrm{log} P(Y \mid X) &amp;= \mathrm{log} P(y^T\_1 \mid X) \\
&amp;= \sum^T\_{t = 1} \mathrm{log} P(y\_t \mid y^{t-1}\_1, X)
\end{aligned}
$$&lt;p&gt;其中，$Y$ 是长度为 $T$ 的序列，$y_1, y_2, &amp;hellip;, y_T$。在前面的等式中，后面的项通过一个参数为 $\theta$ 的循环神经网络，通过一个状态向量 $h_t$估计得到，也就是通过前一个输出 $y_{t-1}$ 和前一个状态 $h_{t-1}$估计得到：&lt;/p&gt;
$$\tag{2}
\mathrm{log} P(y\_t \mid y^{t-1}\_1, X; \theta) = \mathrm{log} P(y\_t \mid h\_t; \theta)
$$&lt;p&gt;其中，$h_t$ 通过如下的一个循环神经网络计算得到：&lt;/p&gt;
$$\tag{3}
h\_t = \begin{cases}
f(X; \theta) \ \ \mathrm{if} t = 1\\
f(h\_{t-1}, y\_{t-1}; \theta) \mathrm{otherwise}.
\end{cases}
$$&lt;p&gt;$P(y_t \mid h_t; \theta)$ 经常通过状态向量 $h_t$ 的一个线性变换，变换到一个 vector of scores 实现，这个向量是输出字典的每个token的分数，然后用一个 softmax 确保分数适当的归一化。$f(h, y)$通常是一个非线性函数，这个函数融合了之前的状态和之前的输出来生成当前的状态。&lt;/p&gt;
&lt;p&gt;这就意味着模型专注于给定模型当前状态，学习预测下一个输出。因此，模型会以最普通的形式表示序列的概率分布——不像条件随机场以及其他的模型，在给定隐变量状态后，假设不同时间步的输出相互独立。模型的容量只会被循环层和前向传播层的表示容量限制。LSTM，因为他们能学习长范围的结构，所以对这种问题来说非常适合，也就可以学习序列上的rich distributions。&lt;/p&gt;
&lt;p&gt;为了学习边长序列，一个特殊的token，&lt;EOS&gt;，表示序列的结束被添加进字典和模型中。在训练的过程中，&lt;EOS&gt;会拼接在每个序列的结尾处。在推理的时候，模型会生成tokens直到它生成了&lt;EOS&gt;。&lt;/p&gt;
&lt;h2 id="22-training"&gt;2.2 Training
&lt;/h2&gt;&lt;p&gt;训练循环神经网络来解决这样的问题通常通过mini-batch随机梯度下降求解，通过给定输入数据 $X^i$，为所有的训练对儿 $(X^i, Y^i)$最大化生成正确的目标序列 $Y^i$ 的似然，找到一组参数$\theta^*$。&lt;/p&gt;
$$\tag{4}
\theta^* = \mathop{\arg \max\_\theta} \sum\_{(X^i, Y^i)} \mathrm{log} P(Y^i \mid X^i; \theta)
$$&lt;h2 id="23-inference"&gt;2.3 Inference
&lt;/h2&gt;&lt;p&gt;在推理的过程中，模型可以在给定 $X$ 的情况下通过一次生成一个token，生成整个序列 $y^T_1$。生成一个 &lt;EOS&gt; 后，它标志着序列的结束。对于这个过程，在时间 $t$，模型为了生成 $y_t$，需要从最后一个时间戳讲输出的token $y_{t-1}$ 作为输入。因为我们没法知道真正的上一个token是什么，我们可以要么选择模型给出的最可能的那个，要么根据这个来抽样。&lt;/p&gt;
&lt;p&gt;给定 $X$ 搜索最大概率的序列 $Y$ 非常费时，因为序列的长度是组合地上升的。我们使用一个beam search来生成 $k$ 个最好的序列。我们通过维护 $m$ 个最优候选序列组成的一个堆来做这个。每次通过给每个候选序列扩充一个token并把它增加到堆内，都能得到一个新的候选序列。在这步的结尾，堆重新地剪枝到只有 $m$ 个候选序列。beam searching在没有新的序列增加的时候就会截断，然后返回 $k$ 个最优的序列。&lt;/p&gt;
&lt;p&gt;尽管beam search通常用于基于HMM这样的模型的离散状态，这些模型可以使用动态规划，但是对于像RNN这样的连续状态模型就很难了，因为没有办法再连续空间内factor the followed state paths，因此在beam search解码的时候，可以控制的候选序列的实际数量是很小的。&lt;/p&gt;
&lt;p&gt;在所有的情况里，如果在时间 $t-1$ 有一个错误生成，那么模型就会在一个和训练分布不同的状态空间中，而且它会在这个空间中不知所措。更糟的是，这会导致模型在决策的时候导致不好的决策的累计——a classic problem in sequential Gibbs sampling type appraoches to sampling, where future samples can have no influence on the past.&lt;/p&gt;
&lt;h2 id="24-bridging-the-gap-with-scheduled-sampling"&gt;2.4 Bridging the Gap with Scheduled Sampling
&lt;/h2&gt;&lt;p&gt;在预测token $y_t$ 时，训练和推断的主要差别是我们是否使用前一个真实的token $y_{t-1}$，还是使用一个从模型得到的估计值 $\hat{y}_{t-1}$。&lt;/p&gt;
&lt;p&gt;我们在这里提出了一个采样机制，会在训练的时候，随机地选择 $y_{t-1}$ 或 $\hat{y}_{t-1}$。假设我们使用mini-batch随机梯度下降，对于训练算法的第 $i$ 个mini-batch中预测 $y_t \in Y$ 的每个token，我们提出用抛硬币的方法，设使用真实token的概率为$\epsilon_i$，使用它估计的token的概率为$(1 - \epsilon_i)$。模型的估计值可以根据模型的概率分布$P(y_{t-1} \mid h_{t-1})$ 来采样获得，或是取 $\mathop{\arg \max_s} P(y_{t-1} = s \mid h_{t-1})$。这个过程由图1所示。&lt;/p&gt;
&lt;div align=center&gt;![Figure1](/blog/images/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/Fig1.jpg)
&lt;p&gt;当 $\epsilon_i = 1$ 时，模型就像之前一样训练，但是当 $\epsilon_i = 0$ 时，模型就会和推断时一样训练。我们这里提出了一个递进学习策略，在训练的开始接断，从模型可能生成的token中进行采样，因为此时模型还没有训练好，这可能会使模型的收敛速度变慢，所以这里选择较多的真实token会帮助训练；另一方面，在训练快结束的时候，$\epsilon_i$ 应该更倾向于从模型的生成结果中采样，因为这个对应了推测的场景，这时我们会期望模型已经有足够好的能力来处理这个问题，并且采样出有效的tokens。&lt;/p&gt;
&lt;p&gt;因此我们提出使用一个规则来减少 $\epsilon_i$ 来作为 $i$ 的函数，就像现在很多随机梯度下降算法那样降低学习率一样。这样的规则如图2所示：&lt;/p&gt;
&lt;p&gt;· Linear decay: $\epsilon_i = \mathrm{max} (\epsilon, k - ci)$，其中 $0 \leq \epsilon &amp;lt; 1$ 是给模型的真值最小的数量，$k$ 和 $c$ 提供了衰减的截距和斜率，这些依赖于收敛速度。&lt;/p&gt;
&lt;p&gt;· Exponential decay: $\epsilon_i = k^i$，其中 $k &amp;lt; 1$ 是一个依赖于期望收敛速度的常量。&lt;/p&gt;
&lt;p&gt;· Inverse sigmoid decay: $\epsilon_i = k/(k + \mathrm{exp}(i/k))$，其中 $k \geq 1$ 依赖于期望收敛速度。&lt;/p&gt;
&lt;p&gt;我们的方法命名为 &lt;em&gt;Scheduled Sampling&lt;/em&gt;。需要注意的是在模型训练时从它的输出采样到前一个token $\hat{y}_{t-1}$时，我们可以在时间 $t \rightarrow T$ 内进行梯度的反向传播。在实验中我们没有尝试，会在未来的工作中尝试。&lt;/p&gt;
&lt;div align=center&gt;![Figure2](/blog/images/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/Fig2.jpg)</description></item><item><title>The Emerging Field of Signal Processing on Graphs</title><link>https://davidham3.github.io/blog/p/the-emerging-field-of-signal-processing-on-graphs/</link><pubDate>Fri, 03 Aug 2018 11:06:12 +0000</pubDate><guid>https://davidham3.github.io/blog/p/the-emerging-field-of-signal-processing-on-graphs/</guid><description>&lt;p&gt;IEEE Signal Processing Magazine 2013, 原文链接：&lt;a class="link" href="https://arxiv.org/abs/1211.0053" target="_blank" rel="noopener"
&gt;The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and Other Irregular Domains&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;社交、能源、运输、传感器、神经网络、高维数据很多都很自然地依赖于带权图的顶点。新兴的图信号处理领域融合了代数、谱图理论与计算谐波分析来处理图上的信号。在这篇教程中，我们列出了这个领域的主要挑战，讨论了定义图谱域的不同方法，点明了融合图数据域中不规则的结构在处理图信号时的重要性。然后回顾了将基础的操作，如filtering, translation, modulation, dilation, downsampling等技术泛化到图上的方法，对已经提出的高效地从图中的高维数据提取信息的localized, multisacle transforms进行了总结。最后对一些问题以及未来的扩展做了一些讨论。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1. Introduction
&lt;/h1&gt;&lt;p&gt;图是很多数据的表示形式，在描述很多应用的几何结构时很有用，如社交、能源、运输、传感器、神经网络等。图中每条边的权重，经常表示为两个顶点之间的相似度。连接性和边权重要么由问题的物理性质指明，要么从数据中推断出来。举个例子，边权重可能与网络中两个顶点之间的距离成反比。这些图的数据可以看作是一个样本的有限集合，每个顶点一个样本。我们称这些样本为一个图信号。一个图信号的例子如图1所示。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/the-emerging-field-of-signal-processing-on-graphs/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;在运输网络中，我们关心分析描述疾病传播的传染病数据，描述用户迁移的人口数据，或是描述货物仓库的后勤数据。现在，在大脑图像中，推断大脑皮层上独特的功能区结构上的连接性变为可能，这种连接可以表示为一个带权图，顶点代表了功能区。因此，noisy fMRI图像可以看作是带权图上的信号。带权图一般用来表示统计学习问题中数据点之间的相似性，如计算机视觉和文本分类问题。事实上，很多研究图数据分析的论文是从统计学习社区中发表出来的，因为基于图的方法在半监督学习问题中变得非常流行，这些问题的目标是用一些标记样本对未知的样本进行分类。在图像处理，对图像的像素构造非局部和半局部连接的图的这种，基于图的filtering methods突然流行了起来，这些方法不仅基于像素间的物理相似性，还有要处理的图像的nosiy versions。这些方法经常能更好地识别并考虑图像的边和材质。&lt;/p&gt;
&lt;p&gt;这些应用中常见的数据处理任务有filtering, denoising, inpainting, compressing graph signals。如何在不规则的域中处理他们，比如在任意结构的图上面？对数据的存储，通信，分析最有效的从高维数据中提取信息的方法是什么，统计与可视化？传统的信号处理的操作或算法可以使用吗？在图上的信号处理领域还有一些这样的问题。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A. The Main Challenges of Signal Processing on Graphs&lt;/em&gt;&lt;br&gt;
小波，时频，曲波和其他局部变化来稀疏地表示不同类别的高维数据，如欧氏空间中的音频和图像信号，这种表示能力在之前提到的信号处理任务中取得了很多的成功。&lt;/p&gt;
&lt;p&gt;$N$个顶点的图信号和一个传统的$N$个样本的离散时域信号可以看作是$\mathbb{R}^N$中的向量。然而，传统信号处理方法应用到图数据上的一个主要障碍是用离散时域信号的处理方式处理图信号时忽略了不规整的数据域内的关键依赖。此外，传统信号处理技术中的很多很简单的基础概念在图信号中变得很有挑战性：
·为了让一个模拟信号$f(t)$向右移动3，我们只要简单的改变变量，考虑$f(t-3)$即可。然而，把一个图信号向右移动3的意义就不是很清晰了。改变变量的方法不会有效因为$f(\circ - 3)$没有意义。一个朴素的方法是将顶点从$1$标到$N$，定义$f(\circ - 3) := f(\mathrm{mod}(\circ - 3, N))$，但是如果这个变换依赖于顶点的顺序的话，这个方法就不是很有用了。不可避免的是，带权图是不规则的结构，这种结构缺少一种变换的平移不变性的性质。
·通过乘以一个复杂的指数项在实数线上对信号建模对应了傅里叶域中的变换。然而，图问题中的模拟谱是离散且不规则的，因此没有好的方法定义一种对应图谱域中的变换。
·举个例子，我们凭直觉每隔一个数据点删除一个数据点，对离散时域信号做下采样。但是在图1中的图信号中这意味着什么？带权图中的“每隔一个顶点”没有明确的含义。
·甚至我们做一个固定的下采样，为了在图上做一个多分辨率，我们需要一个生成粗糙版本的图的方法，这个方法可以捕获原始图中嵌入的结构属性。&lt;/p&gt;
&lt;p&gt;此外，处理数据域的不规则性，图结构在之前提到的应用中，可以表示很多顶点的特征。为了能很好地对数据的尺度进行缩放，对于图信号的处理技术应该使用局部操作，通过对每个顶点，计算顶点的邻居，或是和它很近的顶点的信息得到。&lt;/p&gt;
&lt;p&gt;因此，图信号处理的主要挑战是：1. 有些任务中图没有直接给出，需要决定如何构建可以捕获数据几何结构的带权图；2. 将图结构整合到局部变换操作中；3. 同时利用这些年来信号处理在欧氏空间发展出的理论成果；4. 研究局部变换的高效实现，从高维的图结构数据或其他不规则数据域中提取信息。&lt;/p&gt;
&lt;p&gt;为了解决这些问题，新兴的图信号处理领域将代数和谱图理论的概念与计算谐波分析融合了起来。这是在代数图理论和谱图理论中的扩展；但是，早于十年前的研究主要是聚焦于分析图，而不是分析图的信号。&lt;/p&gt;
&lt;h1 id="2-the-graph-spectral-domains"&gt;2. The Graph Spectral Domains
&lt;/h1&gt;&lt;p&gt;谱图理论是聚焦于构建、分析、操作图的，不是图上的信号。在构建扩展图、图的可视化、谱聚类、着色问题、还有许多如化学、物理、计算科学领域的问题上都很有效。&lt;/p&gt;
&lt;p&gt;图信号处理领域，谱图理论被用作一个定义频谱和图傅里叶变换的基的扩展的工具。这部分我们会回顾一些谱图理论基本的定义与符号，研究它如何使得从传统的傅里叶分析扩展出很多重要的数学理论到图论上。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A. Weighted Graphs and Graph Signals&lt;/em&gt;&lt;br&gt;
我们分析无向、连通图$\mathcal{G} = \lbrace \mathcal{V}, \mathcal{E}, \mathbf{W} \rbrace$上的信号。边$e = (i, j)$连接了顶点$i$和$j$，$W_{i,j}$表示边的权重，否则$w_{i,j} = 0$。如果$\mathcal{G}$有$M$个连通分量，我们可以将信号分为$M$份，然后将每份看作是一个子图进行处理。&lt;/p&gt;
&lt;p&gt;当边的权重没有给出的时候，一种常用的方法是使用一个带阈值的高斯核权重函数：
&lt;/p&gt;
$$\tag{1}
W\_{i,j} = \begin{cases}
\exp{(-\frac{[dist(i,j)]^2}{2\theta^2})} \quad &amp;\text{if } dist(i, j) \leq \kappa \\
0 \quad &amp;\text{otherwise}
\end{cases},
$$&lt;p&gt;
参数是$\theta$和$\kappa$。式1中，$dist(i, j)$表示顶点$i$和$j$之间的物理距离，或是两个顶点的特征向量的欧氏空间中的距离，后者在半监督学习任务中很常用。另一个常用的方法是基于物理距离或特征空间距离，将顶点与它的$k$最近邻顶点相连。其他构建图的方法，见第四章，14。&lt;/p&gt;
&lt;p&gt;一个定义在图的顶点上的信号或函数$f: \mathcal{V} \rightarrow \mathbb{R}$可能表示成一个向量$\mathbf{f} \in \mathbb{R}^N$，第$i$个分量表示顶点集$\mathcal{V}$中的第$i$个顶点。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;B. The Non-Normalized Graph Laplacian&lt;/em&gt;
非归一化的拉普拉斯矩阵，也称为组合拉普拉斯矩阵(combinatorial graph Laplacian)，定义为$\bf L := D - W$，$\bf{D}$是对角矩阵，对角线上的第$i$个元素等于与顶点$i$相关的边的权重之和。拉普拉斯矩阵是一个差操作，因为对于任意一个信号$\mathbf{f} \in \mathbb{R}^N$，它满足：
&lt;/p&gt;
$$
(\mathbf{L}f)(i) = \sum\_{j \in \mathcal{N}\_i} W\_{i,j}[f(i) - f(j)],
$$&lt;p&gt;
邻居$\mathcal{N}_i$是与顶点$i$通过一条边相连的顶点集合。我们用$\mathcal{N}(i, k)$表示通过$k$步或小于$k$步连接到顶点$i$的顶点集合。
因为图的拉普拉斯矩阵$L$是实对称矩阵，它的特征向量相互正交，我们表示为$\lbrace \mathbf{u}_l \rbrace_{l=0,1,&amp;hellip;,N-1}$。这些特征向量对应非负的特征值$\lbrace \lambda_l\rbrace_{l=0,1,&amp;hellip;,N-1}$，满足$L \mathbf{u}_l = \lambda_l \mathbf{u}_l$，$l = 0,1,&amp;hellip;,N-1$。零作为特征值,其多重性等于图的连通分量数，因为我们考虑的是连通图，我们假设拉普拉斯矩阵的特征值的顺序为：$0 = \lambda_0 &amp;lt; \lambda_1 \leq \lambda_2 &amp;hellip; \leq \lambda_{N-1} := \lambda_{\text{max}}$。我们将整个谱表示为$\sigma(L) = \lbrace \lambda_0, \lambda_1, &amp;hellip;, \lambda_{N-1}\rbrace$。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;C. A Graph Fourier Transform and Notion of Frequency&lt;/em&gt;
传统的傅里叶变换
&lt;/p&gt;
$$
\hat{f}(\xi) := \langle f, e^{2\pi i \xi t} \rangle = \int\_\mathbb{R} f(t) e^{-2\pi i \xi t}dt
$$&lt;p&gt;
是函数$f$根据复指数的扩展，是一维拉普拉斯算子的特征函数：
&lt;/p&gt;
$$\tag{2}
-\Delta(e^{2\pi i \xi t}) = -\frac{\partial^2}{\partial t^2} e^{2\pi i \xi t} = (2 \pi \xi)^2 e^{2\pi i \xi t}.
$$&lt;p&gt;类比这个，我们可以定义任何一个在图$\mathcal{G}$的顶点上的函数$\mathbf{f} \in \mathbb{R}^N$的图傅里叶变换$\hat{\mathbf{f}}$，根据图拉普拉斯矩阵的特征向量对$\mathbf{f}$的扩展：
&lt;/p&gt;
$$\tag{3}
\hat{f}(\lambda\_l) := \langle \mathbf{f}, \mathbf{u}\_l \rangle = \sum^N\_{i = 1} f(i) u^*\_l (i).
$$&lt;p&gt;
逆图傅里叶变换为：
&lt;/p&gt;
$$\tag{4}
f(i) = \sum^{N - 1}\_{l = 0} \hat{f}(\lambda\_l) u\_l(i).
$$&lt;p&gt;传统的傅里叶分析中，式2中的特征值$\lbrace (2 \pi \xi )^2 \rbrace_{\xi \in \mathbb{R}}$对频率有特殊性：对于$\xi$接近0（低频），对应的复指数特征函数是平滑的，震荡慢的函数，而$\xi$远离0（高频）的对应的复指数特征函数震荡的很快。在图任务中，图拉普拉斯矩阵的特征值和特征向量在频率上提供了相似的特点。对于连通图，拉普拉斯矩阵的对应特征值为0的特征向量$\mathbf{u}_0$是不变的，且每个顶点的值为$\frac{1}{\sqrt{N}}$。图拉普拉斯矩阵的特征向量中对应低频的$\lambda_l$在图上变化的慢；也就是，如果两个顶点通过一条权重很大的边连接，这些地方的特征向量的值就会变得比较相似。对应大的特征值的特征向量在图上变化的更快，且边的权重越高，这些顶点上的值越不相似。图2给出了不同的随机的sensor网络的拉普拉斯矩阵的特征向量，图3展示了每个特征向量zero crossing的数量$\vert Z_\mathcal{G}(\cdot) \vert$。一个信号$\bf{f}$在图$\mathcal{G}$的zero crossing的集合定义为：
&lt;/p&gt;
$$
Z\_\mathcal{G}(\mathbf{f}) := \lbrace e = (i, j) \in \Large\varepsilon \normalsize : f(i)f(j) &lt; 0 \rbrace;
$$&lt;p&gt;
也就是，连接一个正信号和一个负信号的边的集合。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/the-emerging-field-of-signal-processing-on-graphs/Fig2.JPG"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/the-emerging-field-of-signal-processing-on-graphs/Fig3.JPG"
loading="lazy"
alt="Figure3"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;D. Graph Signal Representations in Two Domains&lt;/em&gt;&lt;br&gt;
图傅里叶变换(3)和它的逆(4)给了我们一种方式在两个不同的域中等价的表示一个信号：顶点域和图谱域。尽管我们经常从顶点域的一个信号$\bf{g}$开始，直接在图谱域中定义一个信号$\hat{\bf{g}}$可能仍然是有用的。我们称这样的信号为&lt;em&gt;核(kernels)&lt;/em&gt;。图4a和图4b中，一个这样的核，一个heat kernel，分别展示了在两个域中的效果。类比传统的模拟情况，图4中展示的一个平缓的信号图傅里叶系数衰减的很快。这样的信号是&lt;em&gt;可压缩的(compressible)&lt;/em&gt;，因为可以通过调整一些图傅里叶系数来趋近他们。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/the-emerging-field-of-signal-processing-on-graphs/Fig4.JPG"
loading="lazy"
alt="Figure4"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;E. Discrete Calculus and Signal Smoothness with Respect to the Intrinsic Structure of the Graph&lt;/em&gt;&lt;br&gt;
分析信号时，需要强调一点是，属性（如smoothness）与数据域的内在结构相对应，在我们讨论的环境中，就是带权图。尽管微分几何提供了方法将潜在流形的几何结构整合进可微分流形上连续信号的分析中，*离散微积分(discrete calculus)*提供了一组可以在有限离散空间中操作的多变量微积分的定义与可微分操作器。&lt;/p&gt;
&lt;p&gt;为了增加smoothness对应图的内在结构的问题，我们简单的提一些离散可微分操作。一个信号$\bf{f}$在顶点$i$，对于边$e = (i, j)$的&lt;em&gt;边导数&lt;/em&gt;(edge derivative)定义为：
&lt;/p&gt;
$$
\left. \frac{\partial \mathbf{f}}{\partial e} \right|\_i := \sqrt{W\_{i,j}}[f(j) - f(i)],
$$&lt;p&gt;
顶点$i$处$\bf{f}$的图梯度是：
&lt;/p&gt;
$$
\nabla\_i \mathbf{f} := [\lbrace \left. \frac{\partial f}{\partial b} \right|\_i \rbrace\_{e \in \varepsilon \ \text{s.t.} \ e=(i,j) \ \text{for some} \ j \in \mathcal{V}}].
$$&lt;p&gt;顶点$i$的&lt;em&gt;local variation&lt;/em&gt;
&lt;/p&gt;
$$
\begin{aligned}
\Vert \nabla\_i \mathbf{f} \Vert\_2 : &amp; = [\sum\_{e \in \varepsilon \ \text{s.t.} \ e =(i, j) \ \text{for some} \ j \in \mathcal{V}} (\left. \frac{\partial \mathbf{f}}{\partial e} \right|\_i)^2]^{\frac{1}{2}} \\
&amp; = [\sum\_{j \in \mathcal{N}\_i} W\_{i, j} [f(j) - f(i)]^2]^{\frac{1}{2}}
\end{aligned}
$$&lt;p&gt;
可以度量顶点$i$周围的$\bf{f}$的local smootheness，当顶点$i$和它的邻居$j$的$\bf{f}$有相近的值时这个值较小。&lt;/p&gt;
&lt;p&gt;对于global smoothness，$\bf{f}$的&lt;em&gt;discrete p-Dirichlet form&lt;/em&gt;定义为：
&lt;/p&gt;
$$\tag{5}
S\_p(\mathbf{f}) := \frac{1}{p} \sum\_{i \in V} \Vert \nabla\_i \mathbf{f} \Vert^p\_2 = \frac{1}{p} \sum\_{i \in V}\LARGE[ \normalsize \sum\_{j \in \mathcal{N}\_i} W\_{i,j} [f(j) - f(i)]^2 \LARGE]^{\normalsize \frac{p}{2}}.
$$&lt;p&gt;当$p=1$时，$S_1(\mathbf{f})$是信号对图的&lt;em&gt;total variation&lt;/em&gt;。当$p = 2$时：
&lt;/p&gt;
$$\tag{6}
\begin{aligned}
S\_2(\mathbf{f}) &amp;= \frac{1}{2}\sum\_{i \in V} \sum\_{j \in \mathcal{N}\_i} W\_{i,j} [f(j) - f(i)]^2 \\
&amp;= \sum\_{(i,j) \in \varepsilon} W\_{i,j} [f(j) - f(i)]^2 = \mathbf{f^TLf}.
\end{aligned}
$$&lt;p&gt;$S_2(\mathbf{f})$被称为图拉普拉斯矩阵的二次型，semi-norm $\bf \Vert f \Vert_L$定义为：
&lt;/p&gt;
$$
\Vert \mathbf{f} \Vert\_\mathbf{L} := \Vert \mathbf{L}^{\frac{1}{2}} \mathbf{f} \Vert\_2 = \sqrt{\mathbf{f^TLf}} = \sqrt{S\_2(\mathbf{f})}.
$$&lt;p&gt;注意式6，二次型$S_2(\mathbf{f})$等于0当且仅当$\bf{f}$在所有顶点上都为常数（which is why $\Vert \mathbf{f} \Vert_L$ is only a semi-form），而且，更一般地，当信号$\bf{f}$在那些通过大权重的边连接的邻居顶点上有相似值时，$S_2(\mathbf{f})$的值较小；也就是当它平滑的时候。&lt;/p&gt;
&lt;p&gt;回到拉普拉斯矩阵的特征值和特征向量上，Courant-Fischer Theorem指出，他们也可以通过Rayleigh quotient定义为：
&lt;/p&gt;
$$\tag{7}
\lambda\_0 = \min\_{ \mathbf{f} \in \mathbb{R}^N, \Vert \mathbf{f} \Vert\_2 = 1} \lbrace \mathbf{f^TLf} \rbrace,
$$&lt;p&gt;
&lt;/p&gt;
$$\tag{8}
\text{and} \ \lambda\_l = \min\_{ \mathbf{f} \in \mathbb{R}^N, \Vert \mathbf{f} \Vert\_2 = 1, \mathbf{f} \perp span\lbrace \mathbf{u}\_0, ..., \mathbf{u}\_{l-1} \rbrace} \lbrace \mathbf{f^TLf} \rbrace, \ l = 1, 2, ..., N-1.
$$&lt;p&gt;
其中，特征向量$\mathbf{u}_l$是第$l$个问题的最小化问题的解。从式6和式7中，我们可以再次看出为什么$\mathbf{u}_0$对于连通图来说是常数。式8解释了为什么拉普拉斯矩阵中对应小的特征值的特征向量更平滑，也提供了另一个对为什么拉普拉斯矩阵的谱反映了频率的解释。&lt;/p&gt;
&lt;p&gt;总结一下，图的连通性编码进了拉普拉斯矩阵，拉普拉斯矩阵通常用于定义图傅里叶变换（通过特征向量），平滑性的不同表示。Example 1展示了smoothness和一个图信号的谱内容是如何依赖于图的。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/the-emerging-field-of-signal-processing-on-graphs/Example1.JPG"
loading="lazy"
alt="“Example 1”"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;F. Other Graph Matrices&lt;/em&gt;
图拉普拉斯矩阵的基$\lbrace \mathbf{u}_l \rbrace_{l = 0, 1, &amp;hellip;, N - 1}$只是在正向(3)和逆向(4)图傅里叶变换中使用的一组可能的基。第二个常用的normalize每个权重$W_{i,j}$的方法是乘以$\frac{1}{\sqrt{d_i d_j}}$。这样可以对图的拉普拉斯矩阵归一化，定义为$\bf\tilde{L} := D^{-\frac{1}{2}} L D^{-\frac{1}{2}}$，等价于：
&lt;/p&gt;
$$
(\tilde{L}f)(i) = \frac{1}{\sqrt{d\_i}} \sum\_{j \in \mathcal{N}\_i} W\_{i,j} \LARGE[\normalsize \frac{f(i)}{\sqrt{d\_i}} - \frac{f(j)}{\sqrt{d\_j}} \LARGE].
$$&lt;p&gt;连通图$\mathcal{G}$的归一化的拉普拉斯矩阵的特征值$\lbrace \tilde{\lambda}_l \rbrace_{l=0,1,&amp;hellip;,N-1}$满足：
&lt;/p&gt;
$$
0 = \tilde{\lambda}\_0 &lt; \tilde{\lambda}\_1 \leq ... \leq \tilde{\lambda}\_{\text{max}} \leq 2,
$$&lt;p&gt;
当且仅当$\mathcal{G}$是二分图时，$\tilde{\lambda}_{\text{max}} = 2$。我们将归一化的拉普拉斯矩阵表示为$\lbrace \mathbf{\tilde{u}}_l \rbrace_{l = 0,1,&amp;hellip;N-1}$。图3b中，$\tilde{L}$的谱和频率也有关系，对应大的特征值的特征向量一般有着更多的zero crossing。然而，不像$\mathbf{u}_0$，归一化的拉普拉斯矩阵中对应特征值为0的$\tilde{\mathbf{u}}_0$不是一个常向量。&lt;/p&gt;
&lt;p&gt;归一化和非归一化的拉普拉斯矩阵都是&lt;em&gt;generalized graph Laplacians&lt;/em&gt;的例子，也称为&lt;em&gt;discrete Schrödinger operators&lt;/em&gt;。一个图$\mathcal{G}$的泛化拉普拉斯矩阵是任意的对阵矩阵，如果这个矩阵中有边连接顶点$i$和顶点$j$，那么这个矩阵的$(i, j)$是负的，如果$i \not = j$，而且$i$与$j$不相连，那么为$0$，如果$i = j$，那么有可能是任何值。&lt;/p&gt;
&lt;p&gt;第三个常用的矩阵，经常在图信号的降维技术中使用，是&lt;em&gt;random walk matrix&lt;/em&gt;，$\bf{P := D^{-1}W}$。每个值$P_{i,j}$表示在图$\mathcal{G}$上从顶点$i$到顶点$j$通过一步马尔可夫随机游走的概率。对于连通的、非周期的图，$\mathbf{P}^t$在$t$趋近于无穷时，收敛至平稳分布。与随机游走矩阵密切相关的是非对称拉普拉斯矩阵，定义为 $\mathbf{L}_a := \mathbf{I}_N - \mathbf{P}$，其中$\mathbf{I}_N$表示$N \times N$的单位阵。注意$\mathbf{L}_a$有着和$\tilde{\mathbf{L}}$同样的特征值集合，如果$\tilde{\mathbf{u}}_l$是对应$\tilde{L}$的特征值$\tilde{\lambda}_l$的特征向量，则$\bf{D}^{-\frac{1}{2}} \tilde{\mathbf{u}}_l$是对应$\mathbf{L}_a$的特征值$\tilde{\lambda}_l$的特征向量。&lt;/p&gt;
&lt;p&gt;正如下一节要讨论的，归一化和非归一化的拉普拉斯矩阵都能用于filtering。没有明确的规定要求什么时候必须使用归一化的，什么时候使用非归一化的拉普拉斯矩阵的特征向量，什么时候使用其他的基。归一化的拉普拉斯矩阵有很好的性质，它的谱总时在$[0, 2]$区间内，而且对于二分图，spectral folding phenomenon可以研究。然而，非归一化的拉普拉斯矩阵中对应特征值为0的特征向量是常向量，这在从传统filtering理论扩展关于信号的DC components上是一个有用的性质。&lt;/p&gt;
&lt;h1 id="3-generalized-operators-for-signals-on-graphs"&gt;3. Generalized Operators For Signals on Graphs
&lt;/h1&gt;&lt;p&gt;在这部分，我们会回顾不同的方式来泛化基本操作到图上，如filtering, translation, modulation, dilation, downsampling。这些泛化的操作是第四部分要讨论的localized, multiscale transforms的基础。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A. Filtering&lt;/em&gt;
第一个泛化的操作是filtering。我们从扩展频率滤波的概念到图上开始，然后讨论顶点域上的局部滤波。&lt;/p&gt;
&lt;h2 id="1-frequency-filtering"&gt;1. Frequency Filtering:
&lt;/h2&gt;&lt;p&gt;在传统的信号处理中，频率滤波是将输入信号表示成一个复指数的线性组合，扩大或缩小一些复指数贡献的过程
&lt;/p&gt;
$$\tag{9}
\hat{f}\_{out}(\xi) = \hat{f}\_{in}(\xi) \hat{h}(\xi),
$$&lt;p&gt;
其中，$\hat{h}(\cdot)$是滤波器的传递函数。取式9的逆傅里叶变换，傅里叶域中的乘法对应了时域中的卷积：
&lt;/p&gt;
$$\tag{10}
f\_{out}(t) = \int\_\mathbb{R} \hat{f}\_{in}(\xi) \hat{h}(\xi)e^{2 \pi i \xi t} d\xi
$$&lt;p&gt;
&lt;/p&gt;
$$\tag{11}
=\intop\_\mathbb{R} f\_{in}(\tau) h(t-\tau)d\tau =: (f\_in * h)(t).
$$&lt;p&gt;
一旦我们fix一个图谱表示，我们的图傅里叶变换的概念，我们可以直接将式9泛化到定义频率滤波上，或&lt;em&gt;图谱滤波&lt;/em&gt;(graph spectral filtering)上：
&lt;/p&gt;
$$\tag{12}
\hat{f}\_{out}(\lambda\_l) = \hat{f}\_{in}(\lambda\_l) \hat{h}(\lambda\_l),
$$&lt;p&gt;
或者，等价的，取逆图傅里叶变换，
&lt;/p&gt;
$$\tag{13}
f\_{out}(i) = \sum^{N-1}\_{l=0} \hat{f}\_{in}(\lambda\_l) \hat{h}(\lambda\_l) u\_l(i).
$$&lt;p&gt;接用matrix functions[38]理论中的符号，我们可以将式12和式13写成$\mathbf{f}_{out} = \hat{h}(\mathbf{L})\mathbf{f}_{in}$，其中
&lt;/p&gt;
$$\tag{14}
\hat{h}(\mathbf{L}) := \mathbf{U} \begin{bmatrix}
\hat{h}(\lambda\_0) &amp; &amp; 0 \\
&amp; \ddots &amp; \\
0 &amp; &amp;\hat{h}(\lambda\_{N-1})
\end{bmatrix}\mathbf{U^T}
$$&lt;p&gt;基础的图谱滤波可以用来实现连续滤波技术的离散版，如高斯平滑，双边滤波，total variation filtering，anisotropic diffusion，non-local means filtering。特别地，这些滤波器中的很多成为了解决variational problems的方法，对ill-posed inverse problems进行正则化，这些问题如denoising，inpainting，super-resolution。举个例子，离散正则框架：
&lt;/p&gt;
$$\tag{15}
\min\_\mathbf{f}\lbrace \Vert \mathbf{f} - \mathbf{y} \Vert^2\_2 + \gamma S\_p(\mathbf{f}) \rbrace,
$$&lt;p&gt;
其中，$S_p(\mathbf{f})$是式5的p-Dirichlet form。在Example 2中，我们举了个式15的$p = 2$时处理图像去噪的问题的例子。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/the-emerging-field-of-signal-processing-on-graphs/Example2.JPG"
loading="lazy"
alt="Example2"
&gt;&lt;/p&gt;
&lt;h2 id="2-filtering-in-the-vertex-domain"&gt;2. Filtering in the Vertex Domain:
&lt;/h2&gt;&lt;p&gt;在顶点域中filter一个信号，只要简单的将顶点$i$的输出$f_{out}(i)$写成一个顶点$i$的$K-hop$局部邻居上输入信号各分量的线性组合：
&lt;/p&gt;
$$\tag{18}
f\_{out}(i) = b\_{i, i} f\_{in}(i) + \sum\_{j \in \mathcal{N}(i, K)} b\_{i,j} f\_{in}(j),
$$&lt;p&gt;
$\lbrace b_{i,j} \rbrace_{i,j \in \mathcal{V}}$是常数。式18只说明了顶点域上的滤波是一个局部的线性变换。&lt;/p&gt;
&lt;p&gt;我们现在简单地将图谱域上的滤波关联到了顶点域的滤波上。当式12中的频率滤波是$K$阶多项式$\hat{h}(\lambda_l) = \sum^K_{k=0} a_k \lambda^k_l$时，其中$\lbrace a_k\rbrace_{k=0,1,&amp;hellip;K}$是常数，我们也可以将式12在顶点域中解释。由式13，我们得到：
&lt;/p&gt;
$$\tag{19}
\begin{aligned}
f\_{out}(i) &amp; = \sum^{N-1}\_{l=0} \hat{f}\_{in}(\lambda\_l) \hat{h}(\lambda\_l) u\_l(i) \\
&amp; = \sum^N\_{j=1} f\_{in}(j) \sum^K\_{k=0} a\_k \sum^{N-1}\_{l=0} \lambda^k\_l u^*\_l(j) u\_l(i) \\
&amp; = \sum^N\_{j=1} f\_{in}(j) \sum^K\_{k=0} a\_k(\mathbf{L}^k)\_{i,j}.
\end{aligned}
$$&lt;p&gt;然而，在顶点$i$到顶点$j$之间的最短路径距离$d_\mathcal{G}(i,j)$大于$k$时，$(\mathbf{L}^k)_{i,j} = 0$。因此，我们可以将式19写成式18，常数定义为：
&lt;/p&gt;
$$
b\_{i,j} := \sum^K\_{k=d\_\mathcal{G}(i,j)} a\_k (\mathbf{L}^k)\_{i,j}.
$$&lt;p&gt;
所以当频率滤波是一个$K$阶多项式时，顶点$i$上频率滤波后的信号，$f_{out}(i)$，是顶点$i$的$K-hop$邻居上的输入信号的线性组合。这个性质在关联一个卷积核的平滑性与顶点域中滤波后信号的局部化之间很有用。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;B. Convolution&lt;/em&gt;
我们不能直接将卷积的定义（11）泛化到图上，因为$h(t - \tau)$。然而，一种定义图上的卷积的方式是替换式10中的复指数为拉普拉斯矩阵的特征向量：
&lt;/p&gt;
$$\tag{20}
(f * h)(i) := \sum^{N-1}\_{l=0} \hat{f}(\lambda\_l) \hat{h}(\lambda\_l) u\_l(i),
$$&lt;p&gt;
这个使得在顶点域上的卷积等价于在图谱域的乘法。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;C. Translation&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting</title><link>https://davidham3.github.io/blog/p/diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting/</link><pubDate>Tue, 31 Jul 2018 14:37:10 +0000</pubDate><guid>https://davidham3.github.io/blog/p/diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting/</guid><description>&lt;p&gt;ICLR 2018，DCRNN，模型借鉴了&lt;a class="link" href="https://davidham3.github.io/blog/2018/07/23/structured-sequence-modeling-with-graph-convolutional-recurrent-networks/" target="_blank" rel="noopener"
&gt;Structured Sequence Modeling With Graph Convolutional Recurrent Networks (ICLR 2017 reject)&lt;/a&gt;里面的DCRNN，将该模型应用于了交通预测上。而且后者的论文使用的卷积是Defferrard提出的图卷积，这篇论文中使用的是扩散卷积，这种扩散卷积使用的是随机游走，与&lt;a class="link" href="https://davidham3.github.io/blog/2018/07/19/diffusion-convolutional-neural-networks/" target="_blank" rel="noopener"
&gt;Diffusion-Convolutional Neural Networks (NIPS 2016)&lt;/a&gt;的扩散卷积还不一样。构造出来的DCRNN使用了&lt;a class="link" href="https://davidham3.github.io/blog/2018/07/23/structured-sequence-modeling-with-graph-convolutional-recurrent-networks/" target="_blank" rel="noopener"
&gt;Structured Sequence Modeling With Graph Convolutional Recurrent Networks (ICLR 2017 reject)&lt;/a&gt;两种形式中的模型2，即使用扩散卷积学习出空间表示后，放入GRU中进行时间上的建模。原文链接：&lt;a class="link" href="http://arxiv.org/abs/1707.01926" target="_blank" rel="noopener"
&gt;Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="摘要"&gt;摘要
&lt;/h1&gt;&lt;p&gt;交通预测的挑战：1. 对路网复杂的空间依赖关系， 2. 路况变换与非线性的时间动态性， 3. 长期预测的困难性。我们提出了在有向图上对交通流以扩散形式进行建模的方法，介绍了 &lt;em&gt;Diffusion Convolutional Recurrent Neural Network&lt;/em&gt; (DCRNN)，用于交通预测的深度学习框架，同时集成了交通流中的空间与时间依赖。DCRNN 使用图上的双向随机游走捕获了空间依赖，使用编码解码框架以及 scheduled sampling 捕获时间依赖。我们在两个真实的交通数据集上评估了模型，比 state-of-the-art 强了12%-15%。&lt;/p&gt;
&lt;h1 id="1-引言"&gt;1 引言
&lt;/h1&gt;&lt;p&gt;对一个在动态系统中运行的学习系统来说，时空预测是一个很关键的任务。自动驾驶、电网优化、供应链管理等都是它的应用。我们研究了一个重要的任务：路网上的交通预测，这是智能交通系统中的核心部分。目标是给定历史车速与路网数据，预测未来的车速。&lt;/p&gt;
&lt;p&gt;任务有挑战性的原因是复杂的时空依赖关系以及长期预测的上的难度。一方面，交通数据序列表现出了强烈的时间动态性(temporal dynamics)。反复的事件如高峰期或交通事故导致了数据的非平稳性，使得长期预测很困难。另一方面，路网上的监测器包含了复杂但是唯一的空间联系(spatial correlations)。图1展示了一个例子。路1和路2是相关联的，但是路1和路3没有关联。尽管路1和路3在欧氏空间中很近，但是他们表现出了不同的形式。此外，未来的车速更容易受到下游交通的影响，而非上游。这就意味着交通上的空间结构不是欧氏空间的，而是有向的。&lt;/p&gt;
&lt;div align="center"&gt;![Figure1](/blog/images/diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting/Fig1.JPG)
&lt;p&gt;交通预测已经研究了几十年，有两个主要类别：知识驱动的方法和数据驱动的方法。在运输和操作研究中，知识驱动的方法经常使用排队论，模拟交通中的用户行为(Cascetta, 2013)。时间序列社区中，数据驱动的方法如 Auto-Regressive Integrated Moving Average(ARIMA) 模型，Kalman filtering 还是很流行的(Liu et al., 2011; Lippi et al., 2013)。然而，简单的时间序列模型通常依赖平稳假设，这经常与实际交通数据不符。最近开始在交通预测上应用深度学习模型 (Lv et al., 2015; Yu et al., 2017b) ，但是没有考虑空间结构。Wu &amp;amp; Tan 2016和Ma et al. 2017 使用 CNN 对空间关系进行建模，但是在欧氏空间中的。Bruna et al. 2014，Defferrard et al. 2016 研究了图卷积，但是只能处理无向图。&lt;/p&gt;
&lt;p&gt;我们使用一个有向图来表示 pair-wise spatial correlations。图的顶点是sensors，边是权重，通过路网上 sensor 之间的距离得到。我们使用扩散卷积 (diffusion convolution) 操作来捕获空间依赖关系，以扩散性是对交通流的动态性建模。提出了 &lt;em&gt;Diffusion Convolutional Recurrent Neural Network&lt;/em&gt; (DCRNN)，整合了 &lt;em&gt;diffusion convolution&lt;/em&gt; 和 &lt;em&gt;sequence to sequence&lt;/em&gt; 架构以及 &lt;em&gt;scheduled sampling&lt;/em&gt; 技术。在真实数据集上衡量模型时，DCRNN 比state-of-the-art好很多。
· 我们研究了交通预测问题，在有向图上对交通的空间依赖以扩散形式建模。提出了 &lt;em&gt;diffusion convolution&lt;/em&gt;，有着直观的解释以及高效的计算。
· 我们提出了 &lt;em&gt;Diffusion Convolutional Recurrent Neural Network&lt;/em&gt; (DCRNN)，使用 &lt;em&gt;diffusion convolution&lt;/em&gt;，&lt;em&gt;sequence to sequence&lt;/em&gt;，&lt;em&gt;scheduled sampling&lt;/em&gt; 同时对时间和空间依赖关系进行捕获的方法。DCRNN 不限于运输领域，可以应用到其他的时空预测问题上。
· 做了很多实验，效果很好。&lt;/p&gt;
&lt;h1 id="2-methodology"&gt;2 Methodology
&lt;/h1&gt;&lt;h2 id="21-traffic-forecasting-problem"&gt;2.1 Traffic Forecasting Problem
&lt;/h2&gt;&lt;p&gt;$N$ 个sensors。检测器网络表示成带权有向图 $\mathcal{G} = (\mathcal{V}, \mathcal{E}, \boldsymbol{W})$，$\mathcal{V}$ 是顶点集，$\vert \mathcal{V} \vert = N$，$\mathcal{E}$ 是边集，$\boldsymbol{W} \in \mathbb{R}^{N \times N}$ 是带权邻接矩阵，表示顶点相似性（如路网距离的一个函数）。图信号矩阵$\boldsymbol{X} \in \mathbb{R}^{N \times P}$，$P$ 是每个顶点的特征数。$\boldsymbol{X}^{(t)}$ 表示时间 $t$ 观测到的图信号，交通预测问题目的是学习一个函数 $h(\cdot)$，将 $T&amp;rsquo;$ 个历史的图信号映射到未来的 $T$ 个图信号上，给定图 $\mathcal{G}$:
&lt;/p&gt;
$$[\boldsymbol{X}^{(t-T'+1)}, ..., \boldsymbol{X}^{(t)}; \mathcal{G}] \xrightarrow{h(\cdot)} [\boldsymbol{X}^{(t+1)}, ..., \boldsymbol{X}^{(t+T)}]$$&lt;h2 id="22-spatial-dependency-modeling"&gt;2.2 Spatial Dependency Modeling
&lt;/h2&gt;&lt;p&gt;扩散形式以 $\mathcal{G}$ 上的随机游走来刻画，重启概率 $\alpha \in [0, 1]$，状态转移矩阵 $\boldsymbol{D}^{-1}_O \boldsymbol{W}$。这里，$\boldsymbol{D}_{\boldsymbol{O}} = \mathrm{diag}(\boldsymbol{W1})$ 是出度的对角矩阵，$\mathbf{1} \in \mathbb{R}^N$ 表示所有都为1的向量。多个时间步之后，Markov process 会收敛到平稳分布 $\mathcal{P} \in \mathbb{R}^{N \times N}$上，第 $i$ 行 $\mathcal{P}_{i,:} \in \mathbb{R}^N$ 表示从顶点 $v_i \in \mathcal{V}$ 扩散的可能性，也就是对顶点 $v_i$ 的 proximity。下面的引理是平稳分布的闭式解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 2.1&lt;/strong&gt; (Teng et al., 2016) 扩散过程的平稳分布可以表示为图上的无限随机游走的带权组合，可以通过以下式子计算：
&lt;/p&gt;
$$\tag{1}
\mathcal{P} = \sum^\infty\_{k=0} \alpha(1 - \alpha)^k (\boldsymbol{D}^{-1}\_O \boldsymbol{W})^k
$$&lt;p&gt;
其中 $k$ 是diffusion step。实际上，我们使用有限的 $K$ 阶扩散过程，给每一步分配一个可训练的权重。我们也融入反向扩散过程，因为双向扩散可以让模型更灵活地去捕获上游和下游交通带来的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Diffusion Convolution&lt;/strong&gt; 图信号 $\boldsymbol{X} \in \mathbb{R}^{N \times P}$ 和滤波器 $f_\theta$ 的扩散卷积操作的结果是：
&lt;/p&gt;
$$\tag{2}
\boldsymbol{X}\_{:,p} \star\_{\mathcal{G}} f\_\theta = \sum^{K-1}\_{k=0} (\theta\_{k,1} (\boldsymbol{D}^{-1}\_O \boldsymbol{W})^k + \theta\_{k,2}(\boldsymbol{D}^{-1}\_I \boldsymbol{W}^T)^k) \boldsymbol{X}\_{:,p} \ \ \ \ \mathrm{for} \ \ p \in \lbrace 1, ..., P \rbrace
$$&lt;p&gt;
其中 $\theta \in \mathbb{R}^{K \times 2}$ 表示卷积核参数，$\boldsymbol{D}^{-1}_O \boldsymbol{W}$ 和 $\boldsymbol{D}^{-1}_I \boldsymbol{W}^T$ 表示扩散过程和反向扩散的转移概率矩阵。一般，计算卷积是很耗时的。然而，如果 $\mathcal{G}$ 是稀疏的，式2可以通过递归的复杂度为 $O(K)$ 的sparse-dense矩阵乘法高效的计算，总时间复杂度为 $O(K \vert \mathcal{E} \vert) \ll O(N^2)$。附录B有详细的描述。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Diffusion Convolutional Layer&lt;/strong&gt; 式2定义的卷积操作，我们可以构建一个扩散卷积层，将 $P$ 维特征映射到 $Q$ 维输出上。将参数表示为 $\mathbf{\Theta} \in \mathbb{R}^{Q \times P \times K \times 2} = [ \boldsymbol{\theta} ]_{q, p}$，其中 $\mathbf{\Theta}_{q,p,:,:} \in \mathbb{R}^{K \times 2}$ 是第 $p$ 个输入和 $q$ 个输出的参数。扩散卷积层为：
&lt;/p&gt;
$$\tag{3}
\boldsymbol{H}\_{:,q} = \boldsymbol{a}(\sum^P\_{p=1} \boldsymbol{X}\_{:,p} \star\_{\mathcal{G}} f\_{\mathbf{\Theta}\_{q,p,:,:}}) \ \ \ \ \mathrm{for} \ q \in \lbrace 1, ..., Q \rbrace
$$&lt;p&gt;
其中，$\boldsymbol{X} \in \mathbb{R}^{N \times P}$ 是输入，$\boldsymbol{H} \in \mathbb{R}^{N \times Q}$ 是输出，$\lbrace f_{\mathbf{\Theta}_{q,p,:,:}} \rbrace $ 是滤波器，$a$ 是激活函数。扩散卷积层学习图结构数据的表示，我们可以使用基于随机梯度的方法训练它。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relation with Spectral Graph Convolution:&lt;/strong&gt; 扩散卷积是定义在有向和无向图上的。当使用在无向图上时，我们发现很多现存的图结构卷积操作，包括流行的普图卷积，ChebNet，可以看作是一个扩散卷积的特例。令 $\boldsymbol{D}$ 表示度矩阵，$\boldsymbol{L} = \boldsymbol{D}^{-\frac{1}{2}}(\boldsymbol{D} - \boldsymbol{W}) \boldsymbol{D}^{-\frac{1}{2}}$ 是图归一化的拉普拉斯矩阵，接下来的Proposition解释了连接。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proposition 2.2.&lt;/strong&gt; 谱图卷积的定义：
&lt;/p&gt;
$$
\boldsymbol{X}\_{:,p} \star\_{\mathcal{G}} f\_\boldsymbol{\theta} = \Phi F(\boldsymbol{\theta}) \Phi^T \boldsymbol{X}\_{:,p}
$$&lt;p&gt;
特征值分解 $\boldsymbol{L} = \Phi \Lambda \Phi^T$，当图 $\mathcal{G}$是无向图时，$F(\boldsymbol{\theta}) = \sum^{K-1}_0 \theta_k \Lambda^k$，等价于图的扩散卷积。
证明见后记C。&lt;/p&gt;
&lt;h2 id="23-temporal-dynamics-modeling"&gt;2.3 Temporal Dynamics Modeling
&lt;/h2&gt;&lt;p&gt;我们利用 RNN 对时间依赖建模。我们使用 GRU，简单有效的 RNN 变体。我们将 GRU 中的矩阵乘法换成了扩散卷积，得到了我们的扩散卷积门控循环单元 &lt;em&gt;Diffusion Convolutional Gated Recurrent Unit(DCGRU)&lt;/em&gt;.
&lt;/p&gt;
$$
\boldsymbol{r}^{(t)} = \sigma(\mathbf{\Theta}\_r \star\_\mathcal{G} [\boldsymbol{X}^{(t)}, \boldsymbol{H}^{(t-1)}] + \boldsymbol{b}\_r) \\
\boldsymbol{u}^{(t)} = \sigma( \mathbf{\Theta}\_u \star\_\mathcal{G} [\boldsymbol{X}, \boldsymbol{H}^{(t-1)}] + \boldsymbol{b}\_u) \\
\boldsymbol{C}^{(t)} = \mathrm{tanh}(\mathbf{\Theta}\_C \star\_\mathcal{G} [\boldsymbol{X}^{(t)}, (\boldsymbol{r}^{(t)} \odot \boldsymbol{H}^{(t-1)})] + \boldsymbol{b}\_c) \\
\boldsymbol{H}^{(t)} = \boldsymbol{u}^{(t)} \odot \boldsymbol{H}^{(t-1)} + (1 - \boldsymbol{u}^{(t)}) \odot \boldsymbol{C}^{(t)}
$$&lt;p&gt;
其中 $\boldsymbol{X}^{(t)}, \boldsymbol{H}^{(t)}$ 表示时间 $t$ 的输入和输出，$\boldsymbol{r}^{(t)}, \boldsymbol{u}^{(t)}$ 表示时间 $t$ 的reset gate和 update gate。$\star_\mathcal{G}$ 表示式2中定义的混合卷积，$\mathbf{\Theta}_r, \mathbf{\Theta}_u, \mathbf{\Theta}_C$ 表示对应的滤波器的参数。类似 GRU，DCGRU 可以用来构建循环神经网络层，使用 BPTT 训练。&lt;/p&gt;
&lt;div align="center"&gt;![Figure2](/blog/images/diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting/Fig2.JPG)
&lt;p&gt;在多步预测中，我们使用 &lt;em&gt;Sequence to Sequence&lt;/em&gt; 架构。编码解码器都是 DCGRU。训练时，我们把历史的时间序列放到编码器，使用最终状态初始化解码器。解码器生成预测结果。测试时，ground truth 替换成模型本身生成的预测结果。训练和测试输入的分布的差异会导致性能的下降。为了减轻这个问题的影响，我们使用了 &lt;em&gt;scheduled sampling&lt;/em&gt; (Bengio et al., 2015)，在训练的第 $i$ 轮时，模型的输入要么是概率为 $\epsilon_i$ 的 ground truth，要么是概率为 $1 - \epsilon_i$ 的预测结果。在训练阶段，$\epsilon_i$ 逐渐的减小为0，使得模型可以学习到测试集的分布。&lt;/p&gt;
&lt;p&gt;图2展示了 DCRNN 的架构。整个网络通过 BPTT 循环生成目标时间序列的最大似然得到。DCRNN 可以捕获时空依赖关系，应用到多种时空预测问题上。&lt;/p&gt;
&lt;h1 id="3-related-work"&gt;3 Related Work
&lt;/h1&gt;&lt;p&gt;运输领域和运筹学中交通预测是传统问题，主要依赖于排队论和仿真(Drew, 1968)。数据驱动的交通预测方法最近受到了很多的关注，详情可以看近些年的 paper (Vlahogianni et al., 2014)。然而，现存的机器学习模型要么有着很强的假设（如 auto-regressive model ）要么不能考虑非线性的时间依赖（如 latent space model Yu et al. 2016; Deng et al. 2016）。深度学习模型为解决时间序列预测问题提供了新的方法。举个例子，在 Yu et al. 2017b; Laptev et al. 2017 的工作中，作者使用深度循环神经网络研究时间序列预测问题。卷积神经网络已经被应用到交通预测上。Zhang et al. 2016; 2017 将路网转换成了 2D 网格，使用传统的 CNN 预测人流。Cheng et al. 2017 提出了DeepTransport，通过对每条路收集上下游邻居路段对空间依赖建模，在这些邻居上分别使用卷积操作。&lt;/p&gt;
&lt;p&gt;最近，CNN 基于谱图理论已经泛化到任意的图结构上。图卷积神经网络由 Bruna et al. 2014 首次提出，在深度神经网络和谱图理论之间建立了桥梁。Defferrard et al. 2016 提出了 ChebNet，使用快速局部卷积滤波器提升了 GCN。Kipf &amp;amp; Welling 2017 简化了 ChebNet，在半监督分类任务上获得了 state-of-the-art 的表现。Seo et al. 2016 融合了 ChebNet 和 RNN 用于结构序列建模。Yu et al. 2017a 对检测器网络以无向图的形式，使用 Chebnet 和卷积序列模型 (Gehring et al. 2017) 进行建模做预测。这些提及的基于谱的理论的限制之一是，他们需要图是无向的，来计算有意义的谱分解。从谱域到顶点域，Atwood &amp;amp; Towsley 2016 提出了扩散卷积神经网络 (DCNN)，以图结构中每个顶点的扩散过程定义了卷积。Hechtlinger et al. 2017 提出了 GraphCNN 对每个顶点的 $p$ 个最近邻邻居进行卷积，将卷积泛化到图上。然而，这些方法没有考虑时间的动态性，主要处理的是静态图。&lt;/p&gt;
&lt;p&gt;我们的方法不同于这些方法，因为问题的设定不一样，而且图卷积的公式不同。我们将 sensor network 建立成一个带权有向图，比网格和无向图更真实。此外，我们提出的卷积操作使用双向图随机游走来定义，集成了序列到序列模型以及 scheduled sampling ，对长时间的时间依赖建模。&lt;/p&gt;
&lt;h1 id="4-experiments"&gt;4 Experiments
&lt;/h1&gt;&lt;p&gt;我们在两个数据集上做了实验：（1）&lt;strong&gt;METR-LA&lt;/strong&gt; 这个交通数据集包含了洛杉矶高速公路线圈收集的数据 (Jagadish et al., 2014)。我们选择了207个检测器，收集了从2012年3月1日到2012年6月30日4个月的数据用于实验。（2）&lt;strong&gt;PEMS_BAY&lt;/strong&gt; 这个交通数据集由 California Transportation Agencies(CalTrans)Performance Measurement System (PeMS) 收集。我们选了 Bay Area 的325个检测器，收集了从2017年1月1日到2017年5月31日6个月的数据用于实验。两个数据集监测器的分布如图8所示。&lt;/p&gt;
&lt;p&gt;这两个数据集，我们将车速聚合到了5分钟的窗口内，使用了 Z-Score normalization。70%的数用于训练，20%用于测试，10%用于验证。为了构建检测器网络，我们计算了任意两个 sensor 的距离，使用了 thresholded Gaussian kernel 来构建邻接矩阵(Shuman et al., 2013)。$W_{ij} = \exp{(-\frac{\mathrm{dist}(v_i, v_j)^2}{\sigma^2})} \ \text{if} \ \text{dist}(v_i, v_j) \leq \mathcal{\kappa}, \mathrm{otherwise} \ 0$，其中 $W_{ij}$ 表示了检测器 $v_i$ 和 $v_j$ 之间的权重，$\mathrm{dist}(v_i, v_j)$ 表示检测器 $v_i$ 到 $v_j$ 之间的距离。$\sigma$ 表示距离的标准差，$\kappa$ 表示阈值。&lt;/p&gt;
&lt;div align="center"&gt;![Figure8](/blog/images/diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting/Fig8.JPG)
&lt;div align="center"&gt;![Table](/blog/images/diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting/Table1.JPG)
&lt;h2 id="41-experimental-settings"&gt;4.1 Experimental Settings
&lt;/h2&gt;&lt;p&gt;Baselines 1. $\rm{HA}$：历史均值，将交通流建模成周期性过程，使用之前的周期的加权平均作为预测。2. $\mathrm{ARIMA}_{kal}$：Auto-Regressive Integrated Moving Average model with Kalman filter，广泛地应用于时间序列预测上。3. $\rm{VAR}$: Vector Auto-Regression(Hamilton, 1994)。4. $\rm{SVR}$：Support Vector Regression，使用线性支持向量机用于回归任务。5. Feed forward Neural network (FNN)：前向传播神经网络，两个隐藏层，L2正则化。6. Recurrent Neural Network with fully connected LSTM hidden units (FC-LSTM)(Sutskever et al., 2014).&lt;/p&gt;
&lt;p&gt;所有的神经网络方法都是用 Tensorflow 实现，使用 Adam 优化器，学习率衰减。使用 Tree-structured Parzen Estimator(TPE)(Bergstra et al., 2011) 在验证集上选择最好的超参数。DCRNN 的详细参数设置和 baselines 的超参数设置见附录E。&lt;/p&gt;
&lt;h2 id="42-traffic-forecasting-performance-comparison"&gt;4.2 Traffic Forecasting Performance Comparison
&lt;/h2&gt;&lt;p&gt;表1展示了不同的方法在15分钟，30分钟，1小时在两个数据集上预测的对比。这些方法在三种常用的 metrics 上进行了评估，包括1. MAE， 2. MAPE（Mean Absolute Percentage Error）， 3. RMSE。这些 metrics 中的缺失值被排除出去。这些公式在后记E.2。我们观察到这两个数据集上有以下现象：1. RNN-based methods，包括FC-LSTM和DCRNN，一般比其他的方法表现得好，这强调对时间依赖的建模的重要性。2. DCRNN在所有的 forecasting horizons 中的所有 metrics 上都获得了最好的表现，这说明对空间依赖建模的有效性。3. 深度学习模型，包括 FNN，FC-LSTM，DCRNN 在长期预测上，倾向于比线性的 baseline 有更好的结果。比如，1小时。这是因为随着 horizon 的增长，时间依赖变得更加非线性。此外，随着历史均值不依赖短期数据，它的表现对于 forecasting horizon 的小增长是不变的。&lt;/p&gt;
&lt;p&gt;需要注意的是，METR-LA（Los Angeles，有很复杂的交通环境）数据比 PEMS-BAY 更有挑战性，所以我们将 METR-LA 的数据作为以下实验的默认数据集。&lt;/p&gt;
&lt;h2 id="43-effect-of-spatial-dependency-modeling"&gt;4.3 Effect of Spatial Dependency Modeling
&lt;/h2&gt;&lt;p&gt;为了继续深入对空间依赖建模的影响，我们对比了 DCRNN 和以下变体： 1. DCRNN-NoConv，这个通过使用单位阵替换扩散卷积（式2）中的转移矩阵，忽略了空间依赖。这就意味着预测只能通过历史值预测。 2. DCRNN-UniConv，扩散卷积中只使用前向随机游走；图3展示了这三个模型使用大体相同数量的参数时的学习曲线。没有扩散卷积，DCRNN-NoConv 有着更大的 validation error。此外，DCRNN获得了最低的 validation error，说明了使用双向随机游走的有效性。这个告诉我们双向随机游走赋予了模型捕获上下游交通影响的能力与灵活性。&lt;/p&gt;
&lt;div align="center"&gt;![Figure3](/blog/images/diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting/Fig3.JPG)
&lt;p&gt;为了研究图的构建方法的影响，我们构建了一个无向图，$\widehat{W}_{ij} = \widehat{W}_{ji} = \max(W_{ij}, W_{ji})$，其中 $\widehat{\boldsymbol{W}}$ 是新的对称权重矩阵。然后我们使用了 DCRNN 的一个变体，表示成 GCRNN，使用 &lt;em&gt;ChebNet&lt;/em&gt; 卷积的序列到序列学习，并用大体相同的参数数量。表2展示了 DCRNN 和 GCRNN 在 METR-LA 数据集上的对比。DCRNN 都比 GCRNN 好。这说明有向图能更好的捕获交通检测器之间的非对称关系。图4展示了不同参数的影响。$K$ 大体对应了卷积核感受野的大小，单元数对应了卷积核数。越大的 $K$ 越能使模型捕获更宽的空间依赖，代价是增加了学习的复杂度。我们观测到随着 $K$ 的增加，验证集上的误差先是快速下降，然后微微上升。改变不同数量的单元也会有相似的情况。&lt;/p&gt;
&lt;div align="center"&gt;![Table2](/blog/images/diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting/Table2.JPG)
&lt;div align="center"&gt;![Figure4](/blog/images/diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting/Fig4.JPG)
&lt;h2 id="44-effect-of-temporal-dependency-modeling"&gt;4.4 Effect of Temporal Dependency Modeling
&lt;/h2&gt;&lt;p&gt;为了衡量时间建模的影响，包括序列到序列框架以及 scheduled sampling 技术，我们设计 DCRNN 的三种变体：1. DCNN：我们拼接历史的观测值为一个固定长度的向量，将它放到堆叠的扩散卷积层中，预测未来的时间序列。我们训练一个模型只预测一步，将之前的预测结果放到模型中作为输入，使用多步前向预测。2. DCRNN-SEQ：使用编码解码序列到序列学习框架做多步预测。3. DCRNN：类似 DCRNN-SEQ ，除了增加了 scheduled sampling。&lt;/p&gt;
&lt;p&gt;图5展示了这四种方法针对 MAE 的对比。我们观察到：1. DCRNN-SEQ 比 DCNN 好很多，符合了对时间建模的重要性。2. DCRNN 达到了最好的效果，随着预测 horizon 的增加，它的先进性变得越来越明显。这主要是因为模型在训练的时候就在处理多步预测时出现的误差，因此会很少的受到误差反向传播的影响。我们也训练了一个总是将输出作为输入扔到模型中的模型。但是它的表现比这三种变体都差，这就强调了 scheduled sampling 的重要性。&lt;/p&gt;
&lt;div align="center"&gt;![Figure5](/blog/images/diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting/Fig5.JPG)
&lt;h2 id="45-模型的解释性"&gt;4.5 模型的解释性
&lt;/h2&gt;&lt;p&gt;为了更好的理解模型，我们对预测结果和学习到的滤波器进行性了可视化。图6展示了预测1小时的效果。我们观察到了以下情况：1. DCRNN 在交通流速度中存在小的震荡时，用均值生成了平滑的预测结果（图6a）。这反映了模型的鲁棒性。2. DCRNN 比 baseline 方法（如FC-LSTM）更倾向于精确的预测出突变。图6b展示了 DCRNN 预测了高峰时段的起始和终止。这是因为 DCRNN 捕获了空间依赖，能够利用邻居检测器速度的变换来精确预测。图7展示了以不同顶点为中心学习到的滤波器的样例。星表示中心，颜色表示权重。我们可以观察到权重更好的在中心周围局部化，而且权重基于路网距离进行扩散。更多的可视化在附录F。&lt;/p&gt;
&lt;div align="center"&gt;![Figure6](/blog/images/diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting/Fig6.JPG)
&lt;div align="center"&gt;![Figure7](/blog/images/diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting/Fig7.JPG)
&lt;h1 id="5-conclusion"&gt;5 Conclusion
&lt;/h1&gt;&lt;p&gt;我们对路网上的交通预测做了时空上的建模，提出了 &lt;em&gt;diffusion convolutional recurrent neural network&lt;/em&gt;，可以捕获时空依赖。特别地，我们使用双向随机游走，对空间依赖建模，使用循环神经网络捕获时间的动态性。还继承了编码解码架构和 scheduled sampling 技术来提升长期预测的性能。在两个真实的数据集上评估了性能，我们的方法比 baselines 好很多。未来的工作，1. 使用提出的网络解决其他的时空预测问题；2. 对不断演化的图结构的时空依赖关系建模。&lt;/p&gt;
&lt;h1 id="appendix"&gt;Appendix
&lt;/h1&gt;&lt;h2 id="b-efficient-calculation-of-equation"&gt;B Efficient Calculation Of Equation
&lt;/h2&gt;&lt;p&gt;式2可以分解成两个有相同时间复杂度的部分，一部分是 $\boldsymbol{D}^{-1}_O \boldsymbol{W}$，另一部分是 $\boldsymbol{D}^{-1}_I \boldsymbol{W}^T$。因此我们只研究第一部分的时间复杂度。&lt;/p&gt;
&lt;p&gt;令 $T_k(x) = (\boldsymbol{D}^{-1}_O \boldsymbol{W})^k \boldsymbol{x}$，式2的第一部分可以重写为：
&lt;/p&gt;
$$\tag{4}
\sum^{K-1}\_{k=0} \theta\_k T\_k (X\_{:,p})
$$&lt;p&gt;
因为 $T_{k+1}(x) = \boldsymbol{D}^{-1}_O \boldsymbol{W} T_k(\boldsymbol{x})$ 和 $\boldsymbol{D}^{-1}_O \boldsymbol{W}$ 是稀疏的，可以很容易看出式4可以通过 $O(K)$ 的递归稀疏-稠密矩阵乘法，每次时间复杂度为 $O(\vert \varepsilon \vert)$ 得到。然后，式2和式4的时间复杂度都为 $O(K\vert \varepsilon \vert)$。对于稠密图，我们可以使用 spectral sparsification(Cheng et al., 2015) 使其稀疏。&lt;/p&gt;
&lt;h2 id="c-relation-with-spectral-graph-convolution"&gt;C Relation With Spectral Graph Convolution
&lt;/h2&gt;&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; 谱图卷积利用归一化的拉普拉斯矩阵 $\boldsymbol{L = D^{-\frac{1}{2}}(D - W)D^{\frac{1}{2}}} = \mathbf{\Phi \Lambda \Phi^T}$。ChebNet 使 $f_\theta$ 参数化为一个 $\Lambda$ 的 $K$ 阶多项式，使用稳定的切比雪夫多项式基计算这个值。
&lt;/p&gt;
$$\tag{5}
\boldsymbol{X}\_{:,p} \star\_\mathcal{G} f\_\theta = \mathbf{\Phi} (\sum^{K-1}\_{k=0} \theta\_k \mathbf{\Lambda}^k) \mathbf{\Phi^T X}\_{:,p} = \sum^{K-1}\_{k=0} \theta\_k \boldsymbol{L}^k \boldsymbol{X}\_{:,p} = \sum^{K-1}\_{k=0} \tilde{\theta}\_k T\_k(\tilde{\boldsymbol{L}})\boldsymbol{X}\_{:,p}
$$&lt;p&gt;
其中 $T_0(x)=1, T_1(x)=x, T_k(x) = xT_{k-1}(x) - T_{k-2}(x)$ 是切比雪夫多项式的基。令 $\lambda_{\mathrm{max}}$ 表示 $\boldsymbol{L}$ 最大的特征值，$\tilde{\boldsymbol{L}} = \frac{2}{\lambda_{\text{max}}} \boldsymbol{L - I}$ 表示将拉普拉斯矩阵的缩放，将特征值从 $[0, \lambda_{\text{max}}]$ 映射到 $[-1, 1]$，因为切比雪夫多项式生成了一个在 $[-1, 1]$ 内正交的基。式5可以看成一个关于 $\tilde{\boldsymbol{L}}$ 的多项式，我们一会儿可以看到，ChebNet 卷积的输出和扩散卷积到常数缩放因子的输出相似。假设 $\lambda_{\text{max}} = 2$，无向图 $\boldsymbol{D}_I = \boldsymbol{D}_O = \boldsymbol{D}$。
&lt;/p&gt;
$$\tag{6}
\tilde{\boldsymbol{L}} = \boldsymbol{D}^{-\frac{1}{2}}(\boldsymbol{D} - \boldsymbol{W}) \boldsymbol{D}^{-\frac{1}{2}} - \boldsymbol{I} = - \boldsymbol{D}^{-\frac{1}{2}} \boldsymbol{W} \boldsymbol{D}^{-\frac{1}{2}} \sim - \boldsymbol{D}^{-1} \boldsymbol{W}
$$&lt;p&gt;
$\tilde{\boldsymbol{L}}$ 和负的随机游走转移矩阵相似，因此式5的输出也和式2直到常数缩放因子的输出相似。&lt;/p&gt;</description></item><item><title>Structured Sequence Modeling With Graph Convolutional Recurrent Networks</title><link>https://davidham3.github.io/blog/p/structured-sequence-modeling-with-graph-convolutional-recurrent-networks/</link><pubDate>Mon, 23 Jul 2018 10:59:15 +0000</pubDate><guid>https://davidham3.github.io/blog/p/structured-sequence-modeling-with-graph-convolutional-recurrent-networks/</guid><description>&lt;p&gt;ICLR 2017(reject)，两个模型，第一个是将数据扔到Defferrard的图卷积里面，然后将输出扔到LSTM里面。第二个模型是将RNN中的矩阵乘法换成了图卷积操作，最后对动态的mnist进行了识别。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1612.07659v1" target="_blank" rel="noopener"
&gt;Structured Sequence Modeling With Graph Convolutional Recurrent Networks&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="摘要"&gt;摘要
&lt;/h1&gt;&lt;p&gt;GCRN(Graph Convolutional Recurrent Network)，一个可以预测结构化序列数据的深度学习模型。GCRN是传统的循环神经网络的在任意的图结构上的一种泛化形式。这样的结构化数据可以表示成视频中的一系列帧，检测器组成的网络监测到的时空监测值，或是用于自然语言建模的词网中的随机游走。我们提出的模型合并了图上的CNN来辨识空间结构，RNN寻找动态模型。我们研究了两种GCRN，对Penn Treebank数据集进行建模。实验显示同时挖掘图的空间与动态信息可以同时提升precision和学习速度。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 Introduction
&lt;/h1&gt;&lt;p&gt;很多工作，Donahue et al. 2015; Karpathy &amp;amp; Fei-Fei 2015; Vinyals et al. 2015，利用CNN和RNN的组合来挖掘时空规律性。这些模型那个处理时间变化的视觉输入来做变长的预测。这些网络架构由视觉特征提取的CNN，和一个在CNN后面，用于序列学习的RNN组成。这样的架构成功地用于视频活动识别，图像注释生成以及视频描述。&lt;/p&gt;
&lt;p&gt;最近，大家开始对时空序列建模时融合CNN和RNN感兴趣。受到语言模型的启发，Ranzato et al. 2014提出了通过发现时空相关性的能表示复杂变形和动作模式的模型。他们的实验表明在通过quantizing the image patches获得到的visual words上，使用RNN建模，可以很好的预测视频的下一帧以及中间帧。他们的表现最好的模型是recursive CNN(rCNN)，对输入和状态同时使用卷积。Shi et al. 2015之后提出了卷积LSTM(convLSTM)，一个使用2D卷积利用输入数据的空间相关性，用于时空序列建模的RNN模型。他们成功的对降雨临近预报的雷达回波图的演化进行了预测。&lt;/p&gt;
&lt;p&gt;很多重要问题中，空间结构不是简单的网格状。气象站就不是网格状。而且空间结构不一定是空间上的，如社交网络或生物网络。最后，Mikolov et al. 2013等人认为，句子可以解释成在词网上的随机游走，使得我们转向了分析图结构的句子建模问题。&lt;/p&gt;
&lt;p&gt;我们的工作利用了近期的模型——Defferrard et al. 2016; Ranzato et al. 2014; Shi et al. 2015——来设计GCRN模型对时间变化的图结构数据建模和预测。核心思想是融合图结构上的CNN和RNN来同时辨识空间结构和动态模式。图1给出了GCRN的架构。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/structured-sequence-modeling-with-graph-convolutional-recurrent-networks/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;h1 id="2-preliminaries"&gt;2 Preliminaries
&lt;/h1&gt;&lt;h2 id="21-structured-sequence-modeling"&gt;2.1 Structured Sequence Modeling
&lt;/h2&gt;&lt;p&gt;序列建模是给定前$J$个观测值，对未来最可能的长度为$K$的序列进行预测：
&lt;/p&gt;
$$\tag{1}
\hat{x}\_{t+1},...,\hat{x}\_{t+K} = \mathop{\mathrm{argmax}}\limits\_{x\_{t+1},...,x\_{t+K}}P(x\_{t+1},...,x\_{t+K} \mid x\_{t-J+1},...,x\_t),
$$&lt;p&gt;
$x_t \in \mathbf{D}$是时间$t$的观测值，$\mathbf{D}$表示观测到的特征的域。原型应用是$n-\mathrm{gram}$模型$(n = J + 1)$，$P(x_{t+1} \mid x_{t-J+1},&amp;hellip;,x_t)$对在句子中给定过去$J$个词时$x_{t+1}$出现的概率进行建模。&lt;/p&gt;
&lt;p&gt;我们感兴趣的是特别的结构化的句子，也就是句子中$x_t$的特征不是相互独立的，而是有着两两相连的关系。这样的关系广义上通过带权图建模。&lt;/p&gt;
&lt;p&gt;$x_t$可以看作是一个图信号，也就是一个定义在无向带权图$\mathcal{G} = ( \mathcal{V}, \Large{\varepsilon}, \normalsize{A )}$，其中$\mathcal{V}$是$\vert \mathcal{V} \vert = n$个顶点的有限集，$\Large{\varepsilon}$是边集，$A \in \mathbb{R}^{n \times n}$是带权邻接矩阵，编码了两个顶点之间的连接权重。定义在图的顶点上的信号$x_t: \mathcal{V} \rightarrow \mathbb{R}^{d_x}$可以当作是一个矩阵$x_t \in \mathbb{R}^{n \times d_x}$，列$i$是$d_x$维向量，表示$x_t$在第$i$个顶点的值。尽管自由变量的数量在长度为$K$的结构化序列中本质上是$\mathcal{O}(n^K{d_x}^K)$，我们仍然试图去挖掘可能的预测结果的空间结构以减少维度，来使这些问题变得容易解决。&lt;/p&gt;
&lt;h2 id="22-long-short-term-memory"&gt;2.2 Long Short-Term Memory
&lt;/h2&gt;&lt;p&gt;防止梯度过快消失，由Hochreiter &amp;amp; Schmidhuber 1997发明的一种RNN，LSTM。这个模型已经被证明在各种序列建模任务中，对长期依赖关系是稳定且强劲的模型(Graves, 2013; Srivastava et al., 2015; Sutskever et al., 2014)。全连接LSTM(FC-LSTM)可以看作是一个多变量版本的LSTM，其中$x_t \in \mathbb{R}^d_x$是输入，$h_t \in [-1, 1]^{d_h}$是细胞状态，$c_t \in \mathbb{R}^{d_h}$是隐藏状态，他们都是向量。我们使用Graves 2013的FC-LSTM：
&lt;/p&gt;
$$\tag{2}
i = \sigma(W\_{xi} x\_t + W\_{hi}h\_{t-1} + w\_{ci} \odot c\_{t-1} + b\_i),\\
f = \sigma(W\_{xf} x\_t + W\_{hf} h\_{t-1} + w\_{cf} \odot c\_{t-1} + b\_f),\\
c\_t = f\_t \odot c\_{t-1} + i\_t \odot \mathrm{tanh}(W\_{xc} x\_t + W\_{hc} h\_{t-1} + b\_c),\\
o = \sigma(W\_{xo} x\_t + W\_{ho} h\_{t-1} + w\_{co} \odot c\_t + b\_o),\\
h\_t = o \odot \mathrm{tanh}(c\_t),
$$&lt;p&gt;
其中$\odot$表示Hadamard product，$\sigma(\cdot)$表示sigmoid function $\sigma(x) = 1/(1+e^{-x})$，$i,f,o \in [0, 1]^{d_h}$是输入门，遗忘门，输出门。权重$W_{x\cdot} \in \mathbb{R}^{d_h \times d_x}$，$W_{h\cdot} \in \mathbb{R}^{d_h \times d_h}$，$w_{c\cdot} \in \mathbb{R}^{d_h}$，偏置$b_i,b_f,b_c,b_o \in \mathbb{R}^{d_h}$是模型参数。这个模型之所以称为全连接是因为$W_{x\cdot}$和$W_{h\cdot}$与$x$和$h$所有分量进行线性组合。由Gers &amp;amp; Schmidhuber 2000引入可选的peephole connections $w_{c\cdot} \odot c_t$，在某些特定任务上可以提升性能。&lt;/p&gt;
&lt;h2 id="23-convolutional-neural-networks-on-graphs"&gt;2.3 Convolutional Neural Networks On Graphs
&lt;/h2&gt;&lt;p&gt;Defferrard et al., 2016选择了谱上的卷积操作：
&lt;/p&gt;
$$\tag{3}
y = g\_\theta \ast\_\mathcal{G} x = g\_\theta (L)x = g\_\theta (U \Lambda U^T)x = U g\_\theta (\Lambda) U^T x \in \mathbb{R}^{n \times d\_x},
$$&lt;p&gt;
对于归一化的拉普拉斯矩阵$L = I_n - D^{-1/2} A D^{-1/2} = U \Lambda U^T \in \mathbb{R}^{n \times n}$来说，$U \in \mathbb{R}^{n \times n}$是矩阵的特征向量，$\Lambda \in \mathbb{R}^{n \times n}$是特征值的对角矩阵。式3的时间复杂度很高，因为$U$的乘法的时间复杂度是$\mathcal{O}(n^2)$。此外，计算$L$的特征值分解对于大的图来说很慢。Defferrard et al., 2016使用切比雪夫多项式：
&lt;/p&gt;
$$\tag{4}
g\_\theta(\Lambda) = \sum^{K-1}\_{k=0} \theta\_k T\_k(\tilde{\Lambda}),
$$&lt;p&gt;
参数$\theta \in \mathbb{R}^K$是切比雪夫系数的向量，$T_k(\tilde{\Lambda}) \in \mathbb{R}^{n \times n}$是切比雪夫多项式的k阶项在$\tilde{\Lambda} = 2\Lambda/\lambda_{max} - I_n$的值。图卷积操作可以写为：
&lt;/p&gt;
$$\tag{5}
y = g\_\theta \ast\_{\mathcal{G}} x = g\_\theta (L) x = \sum^{K-1}\_{k=0} \theta\_k T\_k (\tilde{L})x,
$$&lt;p&gt;
$T_0 = 1$，$T_1 = x$，$T_k(x) = 2xT_{k-1}(x)-T_{k-2}(x)$，时间复杂度是$\mathcal{O}(K \vert \Large{\varepsilon} \normalsize \vert)$，也就是和边数相关。这个图卷积是$K$阶局部化的。&lt;/p&gt;
&lt;h1 id="3-related-works"&gt;3 Related Works
&lt;/h1&gt;&lt;p&gt;Shi et al. 2015提出了针对常规网格结构的序列的模型，可以看作是图是图像网格且顶点有序的特殊情况。他们的模型本质上是FC-LSTM，$W$的乘法替换为卷积核$W$：
&lt;/p&gt;
$$\tag{6}
i = \sigma(W\_{xi} \ast x\_t + W\_{hi} \ast h\_{t-1} + w\_{ci} \odot c\_{t-1} + b\_i),\\
f = \sigma(W\_{xf} \ast x\_t + W\_{hf} \ast h\_{t-1} + w\_{cf} \odot c\_{t-1} + b\_f),\\
c\_t = f\_t \odot c\_{t-1} + i\_t \odot \mathrm{tanh}(W\_{xc} \ast x\_t + W\_{hc} \ast h\_{t-1} + b\_c),\\
o = \sigma(W\_{xo} \ast x\_t + W\_{ho} \ast h\_{t-1} + w\_{co} \odot c\_t + b\_o),\\
h\_t = o \odot \mathrm{tanh}(c\_t),
$$&lt;p&gt;
$\ast$表示一组卷积核的2D卷积。在他们的设定中$x_t \in \mathbb{R}^{n_r \times n_c \times d_x}$是一个动态系统中，时间$t$的$d_x$的观测值，这个动态系统建立在一个表示为$n_r$行$n_c$列的空间区域上。模型有着空间分布的隐藏核细胞状态，大小是$d_h$，由张量$c_t$体现，$h_t \in \mathbb{R}^{n_r \times n_c \times d_h}$。卷积核$W_{h\cdot} \in \mathbb{R}^{m \times m \times d_h \times d_h}$和$W_{x\cdot} \in \mathbb{R}^{m \times m \times d_h \times d_x}$的尺寸$m$决定了参数的数量，与网格大小$n_r \times n_c$无关。更早一点，Ranzato et al. 2014提出了相似的RNN变体，使用卷积层而不是全连接层。时间$t$的隐藏状态：
&lt;/p&gt;
$$\tag{7}
h\_t = \mathrm{tanh}(\sigma(W\_{x2} \ast \sigma(W\_{x1} \ast x\_t)) + \sigma(W\_h \ast h\_{t-1})),
$$&lt;p&gt;
卷积核$W_h \in \mathbb{R}^{d_h \times d_h}$受限到$1 \times 1$的大小。&lt;/p&gt;
&lt;p&gt;观察到自然语言表示出语法性质，自然的将词融入短语中，Tai et al. 2015提出了一个处理树结构的模型，每个LSTM可以获取他们的孩子的状态。他们在semantic relatedness and sentiment classification上获得了state-of-the-art的结果。Liang et al. 2016在之后提出了在图上的变体。他们复杂的网络结构在4个数据集上获得了semantic object parsing的state-of-the-art结果。这些模型中，状态通过一个可训练的权重矩阵的带权加和从邻居上聚集。然而这些权重并不在图上共享，否则需要对顶点排序，就像其他图卷积的空间定义一样。此外，他们的公式受限于当前顶点的一阶邻居，给其他的邻居相同的权重。&lt;/p&gt;
&lt;p&gt;受到如人体动作和物体交互等时空任务的启发，Jain et al 2016提出了一个方法将时空图看作是一个富RNN的混合，本质上是将一个RNN连接到每个顶点与每条边上。同样的，通信受限于直接连接的顶点与边。&lt;/p&gt;
&lt;p&gt;和我们的工作最相关的模型可能是Li et al 2015提出的模型，在program verification上表现出了最好的结果。尽管他们使用Scarselli et al. 2009提出的GNN，以迭代的步骤传播顶点的表示，直到收敛，我们使用的是Defferrard et al. 2016提出的GCN在顶点间扩散信息。尽管他们的动机和我们很不一样，这些模型的关联是使用$K$阶多项式定义的谱滤波器可以实现成一个$K$层的GNN。&lt;/p&gt;
&lt;h1 id="4-proposed-gcrn-models"&gt;4 Proposed GCRN Models
&lt;/h1&gt;&lt;p&gt;我们提出了两种GCRN架构&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model 1.&lt;/strong&gt;
&lt;/p&gt;
$$\tag{8}
x^{\mathrm{CNN}}\_t = \mathrm{CNN}\_\mathcal{G}(x\_t)\\
i = \sigma(W\_{xi} x^{\mathrm{CNN}}\_t + W\_{hi}h\_{t-1} + w\_{ci} \odot c\_{t-1} + b\_i),\\
f = \sigma(W\_{xf} x^{\mathrm{CNN}}\_t + W\_{hf} h\_{t-1} + w\_{cf} \odot c\_{t-1} + b\_f),\\
c\_t = f\_t \odot c\_{t-1} + i\_t \odot \mathrm{tanh}(W\_{xc} x^{\mathrm{CNN}}\_t + W\_{hc} h\_{t-1} + b\_c),\\
o = \sigma(W\_{xo} x^{\mathrm{CNN}}\_t + W\_{ho} h\_{t-1} + w\_{co} \odot c\_t + b\_o),\\
h\_t = o \odot \mathrm{tanh}(c\_t).
$$&lt;p&gt;
我们简单地写成$x^{\mathrm{CNN}}_t = W^{\mathrm{CNN}} \ast_\mathcal{G} x_t$，其中$W^{\mathrm{CNN}} \in \mathbb{R}^{K \times d_x \times d_x}$是切比雪夫系数。Peepholes由$w_{c\cdot} \in \mathbb{R}^{n \times d_h}$控制。这样的架构可能足以捕获数据的分布，通过挖掘局部静止性以及性质的组合性，还有动态属性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model 2.&lt;/strong&gt;
&lt;/p&gt;
$$\tag{9}
i = \sigma(W\_{xi} \ast\_\mathcal{G} x\_t + W\_{hi} \ast\_\mathcal{G} h\_{t-1} + w\_{ci} \odot c\_{t-1} + b\_i),\\
f = \sigma(W\_{xf} \ast\_\mathcal{G} x\_t + W\_{hf} \ast\_\mathcal{G} h\_{t-1} + w\_{cf} \odot c\_{t-1} + b\_f),\\
c\_t = f\_t \odot c\_{t-1} + i\_t \odot \mathrm{tanh}(W\_{xc} \ast\_\mathcal{G} x\_t + W\_{hc} \ast\_\mathcal{G} h\_{t-1} + b\_c),\\
o = \sigma(W\_{xo} \ast\_\mathcal{G} x\_t + W\_{ho} \ast\_\mathcal{G} h\_{t-1} + w\_{co} \odot c\_t + b\_o),\\
h\_t = o \odot \mathrm{tanh}(c\_t),
$$&lt;p&gt;
图卷积核是切比雪夫系数$W_{h\cdot} \in \mathbb{R}^{K \times d_h \times d_h}$，$W_{x\cdot} \in \mathbb{R}^{K \times d_h \times d_x}$决定了参数的数目，与顶点数$n$无关。
这种RNN和CNN的混合，不限于LSTM。普通的RNN $h_t = \mathrm{tanh}(W_x x_t + W_h h_{t-1})$可以写为：
&lt;/p&gt;
$$\tag{10}
h\_t = \mathrm{tanh}(W\_x \ast\_\mathcal{G} x\_t + W\_h \ast\_\mathcal{G} h\_{t-1}),
$$&lt;p&gt;
GRU的版本可以写为：
&lt;/p&gt;
$$\tag{11}
z = \sigma(W\_{xz} \ast\_\mathcal{G} x\_t + W\_{hz} \ast\_\mathcal{G} h\_{t-1}),\\
r = \sigma(W\_{xr} \ast\_\mathcal{G} x\_t + W\_{hr} \ast\_\mathcal{G} h\_{t-1}),\\
\tilde{h} = \mathrm{tanh}(W\_{xh} \ast\_\mathcal{G} x\_t + W\_{hh} \ast\_\mathcal{G} (r \odot h\_{t-1})),\\
h\_t = z \odot h\_{t-1} + (1 - z) \odot \tilde{h}.
$$&lt;h1 id="5-experiments"&gt;5 Experiments
&lt;/h1&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/structured-sequence-modeling-with-graph-convolutional-recurrent-networks/Table1.JPG"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;
&lt;p&gt;数据集是moving-MNIST(Shi et al., 2015)。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/structured-sequence-modeling-with-graph-convolutional-recurrent-networks/Fig3.JPG"
loading="lazy"
alt="“Figure3 &amp; Figure4”"
&gt;&lt;/p&gt;</description></item><item><title>Inductive Representation Learning on Large Graphs</title><link>https://davidham3.github.io/blog/p/inductive-representation-learning-on-large-graphs/</link><pubDate>Thu, 19 Jul 2018 18:53:17 +0000</pubDate><guid>https://davidham3.github.io/blog/p/inductive-representation-learning-on-large-graphs/</guid><description>&lt;p&gt;NIPS 2017。提出的方法叫 GraphSAGE，针对的问题是之前的 NRL 是 transductive，不能泛化到新结点上，而作者提出的 GraphSAGE 是 inductive。主要考虑了如何聚合顶点的邻居信息，对顶点或图进行分类。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1706.02216" target="_blank" rel="noopener"
&gt;Inductive Representation Learning on Large Graphs&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;现存的方法需要图中所有的顶点在训练 embedding 的时候都出现；这些前人的方法本质是 &lt;em&gt;transductive&lt;/em&gt;，不能自然地泛化到未见过的顶点上。我们提出了 GraphSAGE，一个 &lt;em&gt;inductive&lt;/em&gt; 框架，利用顶点特征信息（比如文本属性）来高效地为没有见过的顶点生成 embedding。与其为每个顶点训练单独的 embedding，我们的方法是学习一个函数，这个函数通过从一个顶点的局部邻居采样并聚合顶点特征。我们的算法在三个 inductive 顶点分类数据集上超越了那些很强的 baseline：在 citation 和 Reddit post 数据的演化的信息图中对未见过的顶点分类，在 PPI 多图数据集上可以泛化到完全未见过的图上。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 Introduction
&lt;/h1&gt;&lt;p&gt;顶点嵌入的基本思想是使用降维技术从高维信息中提炼一个顶点的邻居信息，存到低维向量中。这些顶点嵌入之后会作为后续的机器学习系统的输入，解决像顶点分类、聚类、链接预测这样的问题。&lt;/p&gt;
&lt;p&gt;然而，前人的工作专注于对一个固定的图中的顶点进行表示，很多真实的应用需要很快的对未见过的顶点或是全新的图（子图）生成 embedding。这个推断的能力对于高吞吐的机器学习系统来说很重要，这些系统都运作在不断演化的图上，而且时刻都会遇到未见过的顶点（比如 Reddit 上的文章，Youtube 上的用户或视频）。一个生成顶点 embedding 的推断方法也会帮助在拥有同样形式特征的图上进行泛化：举个例子，我们可以从一个有机物得到的 PPI 图上训练一个 embedding 生成器，然后很简单的使用这个模型利用新的有机物上收集的数据生成他们的顶点嵌入。&lt;/p&gt;
&lt;p&gt;对比 transductive 问题，推断顶点嵌入问题很困难，因为泛化未见过的顶点需要将新观测到的子图“对齐”到算法已经优化好的顶点嵌入上。一个推断模型必须学习到可以识别一个顶点邻居的结构性质，这个性质既反映了顶点在图中的局部角色，也反映了它的全局位置。&lt;/p&gt;
&lt;p&gt;很多现存的生成顶点嵌入的方法是继承于 transductive。这些方法的主流是直接使用矩阵分解目标函数对每个顶点的 embedding 进行优化，因为他们在一个固定的单个图上的顶点做预测，所以不能自然地泛化到未见过的数据。这些方法可以被修改然后在 inductive 问题上运行，但是这些修改往往计算复杂度高，在新的预测之前需要额外的梯度下降优化。当然也有一些在图结构上使用卷积神经网络的方法，在 embedding 上表现的很好。迄今为止，GCN 只在固定的图上的 transductive 问题上应用过。我们工作是扩展了 GCN 到无监督推断任务上，同时还提出了一个方法，可以让 GCN 使用可训练的聚合函数（并不是只有简单的卷积）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Present work.&lt;/strong&gt; 我们提出了一个通用的框架，叫 GraphSAGE(SAmple and aggreGatE)，用于学习 inductive node embedding。不像基于矩阵分解的嵌入方法，我们利用了顶点特征（比如文本属性，顶点信息，顶点的度）来学习可以生成未见过的顶点的嵌入的函数。通过在算法中结合顶点信息，我们同时学习了每个顶点邻居的拓扑结构和顶点特征在邻居中的分布。尽管我们的研究更专注于富特征的图（如有文本信息的引文网络，有功能/分子组成的生物数据），我们的方法仍能充分利用所有图展现的结构特征（比如顶点的度）。因此我们的算法可以应用在没有顶点特征的图上。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/inductive-representation-learning-on-large-graphs/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;我们没有对每个顶点都训练一个单独的 embeddding，我们训练了一组 &lt;em&gt;aggregator functions&lt;/em&gt;，这些函数学习如何从一个顶点的局部邻居聚合特征信息（图1）。每个聚合函数从一个顶点的不同搜索深度聚合信息。测试的时候，或是说推断的时候，我们使用我们训练的系统来对完全未见过的顶点，通过使用学习到的聚合函数来生成 embedding。跟随着前人在生成顶点上的工作，我们设计了无监督的损失函数，使得 GraphSAGE 可以在没有任务监督的情况下训练。我们也展示了如何使用监督的方法训练 GraphSAGE。&lt;/p&gt;
&lt;p&gt;我们在三个顶点分类数据集上评估了我们的算法，测试了 GraphSAGE 在未见过的数据上生成有效 embedding 的能力。使用了两个基于 citation 数据和 Reddit post 数据的演化网络（分别预测 paper 和 post 类别），还有一个基于 PPI 的多图泛化实验（预测蛋白质功能）。我们的方法效果很好，跨领域，监督的方法在 F1 值上对比只使用顶点特征的方法平均提高了 51%，而且 GraphSAGE 一直都比 transductive baseline 强很多，尽管 baseline 在未见过的顶点上的运行时间要长 100 倍以上。我们提出的新的聚合结构比受图卷积启发的聚合函数更好（平均提升了 7.4%）。最后我们通过实验证明了我们方法的表达能力，尽管 GraphSAGE 是基于特征的，它却能学习到一个顶点在一个图中的结构信息，（第 5 部分）。&lt;/p&gt;
&lt;h1 id="2-related-work"&gt;2 Related work
&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Factorization-based embedding approaches.&lt;/strong&gt; 最近的 node embedding 方法使用随机游走的统计和矩阵分解的目标函数。这些方法和传统的方法如谱聚类，multi-dimensional scaling，PageRank 关系很近。因为这些嵌入方法对每个顶点直接训练 embedding，本质上是 transductive，而且需要大量的额外训练（如随机梯度下降）使他们能预测新的顶点。此外，这些方法中的大部分方法，目标函数对于 embedding 的正交变换是不变的，意味着嵌入空间不能自然地在图之间泛化，而且在再次训练的时候会 drift。一个值得注意的例外是 Yang et al. 的 Planetoid-I 算法，是一个 inductive 的方法，基于嵌入的半监督学习。然而，Planetoid-I 在推断的时候不使用任何图结构信息，而在训练的时候将图结构作为一种正则化的形式。不同于前面提到的这些方法，我们是利用特征信息训练可以对未见过的顶点生成 embedding 的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Supervised learning over graphs.&lt;/strong&gt; 除了顶点嵌入方法，还有很多在图结构数据上的监督学习方法。包括很多核方法，图的特征向量从多个图的核得到。最近有很多用于图结构的监督学习的神经网络方法。我们的方法从概念上是受到了这些方法的启发。然而，这些方法试图对整个图（或子图）进行分类，我们的工作关注的是如何对单个顶点生成有效的表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Graph convolutional networks.&lt;/strong&gt; 近些年，一些用于图的卷积神经网络被相继提出。这些方法中的大部分不能扩展到大的图上，或者是为了整个图的分类而设计（或是两点都有）。我们的方法与 Kipf et al. 提出的图卷积很相关，GCN 在训练的时候需要整个图的拉普拉斯矩阵。我们算法的一个简单的变体可以看作是 GCN 框架在 inductive setting 上的扩展，我们会在 3.3 说明。&lt;/p&gt;
&lt;h1 id="3-graphsage"&gt;3 GraphSAGE
&lt;/h1&gt;&lt;p&gt;我们的核心思想在于如何从一个顶点的局部邻居聚合特征信息（比如度或近邻顶点的文本特征）。3.1 描述 embedding 的生成算法。3.2 描述随机梯度下降学习参数。&lt;/p&gt;
&lt;h2 id="31-embedding-generation-ie-forward-propagation-algorithm"&gt;3.1 Embedding generation (i.e., forward propagation) algorithm
&lt;/h2&gt;&lt;p&gt;假设已经学习到了 $K$ 个聚合函数（表示为 $AGGERGATE_k, \forall k \in \lbrace 1,&amp;hellip;,K\rbrace$ ）的参数，对顶点的信息聚合，还有一组权重矩阵 $\mathbf{W}^k, \forall k \in \lbrace 1,&amp;hellip;,K\rbrace$，用来在模型的不同层或搜索深度间传播信息。下一节描述参数是怎么训练的。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/inductive-representation-learning-on-large-graphs/Alg1.JPG"
loading="lazy"
alt="Algo1"
&gt;&lt;/p&gt;
&lt;p&gt;算法 1 的思路是在每次迭代，或每一个搜索深度，顶点从他们的局部邻居聚合信息，而且随着这个过程的迭代，顶点会从越来越远的地方获得信息。&lt;/p&gt;
&lt;p&gt;算法 1 描述了在各整个图上生成 embedding 的过程，$\mathcal{G} = \left( \mathcal{V}, \Large{\varepsilon} \right)$，以及所有顶点的特征$X_v, \forall v \in \mathcal{V}$作为输入。在算法 1 最外层循环的每一步如下，$k$ 表示外循环（或搜索深度）的当前步，$\mathbf{h}^k$ 表示当前这步的一个顶点的表示：首先，每个顶点 $v \in \mathcal{V}$聚合了在它在中间邻居的表示，$\lbrace \mathbf{h}^{k-1}_u, \forall u \in \mathcal{N}(u) \rbrace$，聚合到向量$\mathbf{h}^{k-1}_{\mathcal{N}(v)}$中。注意，这个聚合步骤依赖于外循环前一次迭代生成的表示（比如$k - 1$），$k = 0$表示输入的顶点特征。聚合邻居特征向量后，GraphSAGE之后拼接了顶点当前的表示，$\mathbf{h}^{k-1}_v$，核聚合的邻居向量一起，$\mathbf{h}^{k-1}_{\mathcal{N}(v)}$，拼接后的向量输入到了激活函数为$\sigma$的全连接层中，将表示变换为下一步使用的形式（$\mathbf{h}^k_v, \forall v \in \mathcal{V}$）。为了记号的简单，我们将深度为$K$的输出表示记为$\mathbf{z} \equiv \mathbf{h}^K_v, \forall v \in \mathcal{V}$。邻居表示的聚合可以通过多个聚合架构得到（在算法1中表示为$\mathrm{AGGERGATE}$），我们会在3.3讨论不同的架构。&lt;/p&gt;
&lt;p&gt;为了将算法1扩展到minibatch设定上，给定一组输入顶点，我们先采样采出需要的邻居集合（到深度$K$），然后运行内部循环（算法1的第三行），但不是迭代所有的顶点，我们在每个深度只计算必须满足的表示（后记A包括了完整的minibatch伪代码）。&lt;/p&gt;
&lt;h2 id="32-learning-the-parameters-of-graphsage"&gt;3.2 Learning the parameters of GraphSAGE
&lt;/h2&gt;&lt;p&gt;为了在半监督设定下学习一个有效的表示，我们使用基于图的损失函数来输出表示$\mathbf{z}_u, \forall u \in \mathcal{V}$，调整权重矩阵$\mathbf{W}^k, \forall k \in \lbrace 1,&amp;hellip;,K\rbrace$，聚合函数的参数通过随机梯度下降训练。基于图的损失函数倾向于使得相邻的顶点有相似的表示，尽管这会使相互远离的顶点的表示很不一样：
&lt;/p&gt;
$$\tag{1}
J\mathcal{G}(\mathbf{z}\_u) = -\log(\sigma(\mathbf{z}^T\_u \mathbf{z}\_v)) - Q \cdot \mathbb{E}\_{v\_n \sim P\_n(v)} \log(\sigma(-\mathbf{z}^T\_u \mathbf{z}\_{v\_n})),
$$&lt;p&gt;
其中$v$是通过定长随机游走得到的$u$旁边的共现顶点，$\sigma$是sigmoid函数，$P_n$是负采样分布，$Q$定义了负样本的数目。重要的是，不像之前的那些方法，我输入到损失函数的表示$\mathbf{z}_u$是从包含一个顶点局部邻居的特征生成出来的，而不是对每个顶点训练一个独一无二的embedding（通过一个embedding查询表）。&lt;/p&gt;
&lt;p&gt;这个无监督设定模拟了顶点特征提供给后续机器学习应用的情况。在那些表示只在后续的任务中使用的情况下，无监督损失（式1）可以被替换或改良，通过一个以任务为导向的目标函数（比如cross-entropy）。&lt;/p&gt;
&lt;h2 id="33-聚合架构"&gt;3.3 聚合架构
&lt;/h2&gt;&lt;p&gt;不像在$N$维网格（如句子、图像、$3\rm{D}$）上的机器学习，一个顶点的邻居是无序的；因此，算法1中的聚合函数必须在以一个无序的向量上运行。理想上来说，一个聚合函数需要是对称的（也就是对它输入的全排列来说是不变的），而且还要可训练，且保持表示的能力。聚合函数的对称性之确保了我们的神经网络模型可以被训练且可以应用于任意顺序的顶点邻居特征集合上。我们检验了三种聚合函数：
&lt;strong&gt;Mean aggregator.&lt;/strong&gt; 第一个聚合函数是均值聚合，我们简单的取$\lbrace \mathbf{h}^{k-1}_u, \forall v \in \mathcal{N}(v) \rbrace$中的向量的element-wise均值。均值聚合近似等价在transducttive GCN框架[17]中的卷积传播规则。特别地，我们可以通过替换算法1中的4行和5行为以下内容得到GCN的inductive变形：
&lt;/p&gt;
$$\tag{2}
\mathbf{h}^k\_v \leftarrow \sigma(\mathbf{W} \cdot \mathrm{MEAN}(\lbrace \mathbf{h}^{k-1}\_v \rbrace \cup \lbrace \mathbf{h}^{k-1}\_u, \forall u \in \mathcal{N}(v) \rbrace)).
$$&lt;p&gt;
我们称这个修改后的基于均值的聚合器是&lt;em&gt;convolutional&lt;/em&gt;，因为它是一个粗略的，局部化谱卷积的的线性近似[17]。这个卷积聚合器和我们的其他聚合器的重要不同在于它没有算法1中第5行的拼接操作——卷积聚合器没有将顶点前一层的表示$\mathbf{h}^{k-1}_v$和聚合的邻居向量$\mathbf{h}^k_{\mathcal{N}(v)}$拼接起来。拼接操作可以看作一个是在不同的搜索深度或层之间的简单的skip connection的形式，它使得模型获得了巨大的提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LSTM aggregator.&lt;/strong&gt; 我们也检验了一个基于LSTM的复杂的聚合器。对比均值聚合器，LSTM有更强的表达能力。然而，LSTM不是对称的这个需要注意，因为他们处理他们的输入是以一个序列的方式。我们简单地将LSTM应用在一个顶点邻居的随机序列上。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pooling aggregator.&lt;/strong&gt; 我们检验的最后一个聚合器既是对称的，又是可训练的。在这个&lt;em&gt;池化&lt;/em&gt;方法种，每个邻居的向量都是相互独立输入到全连接神经网络中的；随着这种变化，一个element-wise最大池化操作应用在邻居集合上来聚合信息：
&lt;/p&gt;
$$\tag{3}
\mathrm{AGGREGATE}^{pool}\_k = \mathrm{max}(\lbrace \sigma (\mathbf{W}\_{pool} \mathbf{h}^k\_{u\_i} + \mathbf{b}), \forall u\_i \in \mathcal{N}(v) \rbrace),
$$&lt;p&gt;
其中，$\mathrm{max}$表示element-wise最大值操作，$\sigma$是非线性激活函数。原则上，在最大池化使用之前，函数可以是任意的深度多层感知机，但是我们关注的是单个的单层结构。方法是搜到了最近的神经网络架构在学习general point sets上的启发[29]。直觉上来说，多层感知机可以看作是一组函数，这组函数为邻居集合中的每个顶点计算表示。通过对每个计算得到的特征使用最大池化操作，模型有效地捕获了邻居集合的不同方面。注意，原则上，任何对称的向量函数都可以替换$\mathrm{max}$操作器（比如element-wise mean）。我们发现最大池化和均值池化在测试时没有太大的差别，所以使用了最大池化完成了后续的实验。&lt;/p&gt;
&lt;h1 id="4-实验"&gt;4 实验
&lt;/h1&gt;&lt;p&gt;我们在三个benchmark上测试了GraphSAGE：1. 使用Web of Science citation dataset对不同的学术文章进行主题分类。2. 对属于不同社区的Reddit posts进行分类，3. 对多个PPI图进行蛋白质功能分类。在所有的实验中，我们在训练时没有见过的顶点上做预测，对PPI上对完全未见过的图做预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Experimental set-up.&lt;/strong&gt; 为了在inductive benchmark上面将经验结果置于上下文中考虑，我们对比了四个baseline：随即分类器，基于特征的逻辑回归，raw features和DeepWalk embedding拼接的embedding。我们也比较了GraphSAGE的四个变体，分别使用不同的聚合函数（3.3部分）。因为，GraphSAGE的卷积变体是一种扩展形式，是Kipf et al. 半监督GCN的inductive version，我们称这个变体为GraphSAGE-GCN。我们测试了根据式1的损失函数训练的GraphSAGE变体，还有在cross-entropy上训练的监督变体。对于所有的GraphSAGE变体我们使用ReLU作为激活，$K = 2$，邻居采样大小$S_1 = 25$，$S_2 = 10$（详情见4.4节）。&lt;/p&gt;
&lt;p&gt;对于Reddit和citation数据集，我们使用&amp;quot;online&amp;quot;来训练DeepWalk，如Perozzi et al. 提到的那样，我们在做预测前，跑一轮新的SGD来嵌入新的测试顶点（详情见后记）。在多图设定中，我们不能使用DeepWalk，因为通过DeepWalk在不同不相交的图上运行后生成的嵌入空间对其他来说可以是arbitrarily rotated（后记D）。&lt;/p&gt;
&lt;p&gt;所有的模型都是用tf实现的，用Adam优化（除了DeepWalk，使用梯度下降效果更好）。我们设计的实验目标是1. 验证GraphSAGE比其他方法好。 2. 严格对比集中聚合架构，为了严格对比，所有的方法使用相同的实现，如minibatch迭代器，损失函数和邻居采样（如果可以的话）。此外，为了防止对比聚合器时非有意的&amp;quot;hyperparameter hacking&amp;quot;，我们检查了所有GraphSAGE变体的超参数集合（为每个变体根据他们在验证集上的表现选择最好的设定）。可能的超参数集合在早期的验证集上决定，这个验证集是citation和Reddit的子集，后续就丢掉了。后记包含了实现的细节。&lt;/p&gt;</description></item><item><title>Diffusion-Convolutional Neural Networks</title><link>https://davidham3.github.io/blog/p/diffusion-convolutional-neural-networks/</link><pubDate>Thu, 19 Jul 2018 11:17:40 +0000</pubDate><guid>https://davidham3.github.io/blog/p/diffusion-convolutional-neural-networks/</guid><description>&lt;p&gt;NIPS 2016。DCNNs，写的云里雾里的，不是很懂在干什么。。。就知道是融入了转移概率矩阵，和顶点的特征矩阵相乘，算出每个顶点到其他所有顶点的 $j$ 步转移的特征与转移概率的乘积，成为新的顶点表示，称为diffusion-convolutional representation，然后乘以一个卷积核，套一个激活，卷积就定义好了。应用还是在顶点分类与图分类上。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1511.02136" target="_blank" rel="noopener"
&gt;Diffusion-Convolutional Neural Networks&lt;/a&gt;。&lt;/p&gt;
&lt;h1 id="摘要"&gt;摘要
&lt;/h1&gt;&lt;p&gt;我们提出了diffusion-convolutional neural networks(DCNNs)，是图结构数据上的新模型。引入diffusion-convolution操作，可以从图结构数据中得到基于扩散性质的表示，用于顶点分类。DCNN还有几个性质，关于一个图数据的隐含表示，还有多项式时间复杂度的预测以及高效的GPU实现。通过多个真实数据集的实验，DCNN表现出了在关系顶点分类任务上超越概率关系模型以及kernel-on-graph的结果。&lt;/p&gt;
&lt;h1 id="1-引言"&gt;1 引言
&lt;/h1&gt;&lt;p&gt;处理结构化数据很难。一方面是找到正确的方式表示并挖掘数据的结构可以提升预测的性能；另一方面，找到这样的表示很难，增加结构信息到模型中会急剧地增加预测和学习的复杂度。&lt;/p&gt;
&lt;p&gt;我们的工作是为一类结构化数据设计一个灵活的模型，这个模型增强预测能力且避免复杂度的增加。为了完成这个模型，我们通过引入&amp;quot;diffusion-convolution&amp;quot;操作将卷积神经网络扩展到图结构数据上。简单地说，并不是像标准卷积操作一样扫描一个矩形的参数，diffusion-convolution操作通过扩散性的过程扫描图结构输入中每个顶点，构建一个隐含表示。&lt;/p&gt;
&lt;p&gt;我们的受到的启发是：捕获了图扩散性的表示在预测上可以提供比图本身更好的结果。图扩散性可以表达成矩阵的幂序列，提供了一个简单的机制来包含关于实体的上下文信息，这些实体可以在多项式时间复杂度内计算出来，并在GPU上实现。&lt;/p&gt;
&lt;p&gt;我们提出了diffusion-convolutional神经网络(DCNN)，在图数据的各种各样的分类任务上测试了性能。在分类任务中很多技术包含了结构的信息，比如概率关系模型和核方法；DCNN提供了一个补充的方法，在顶点分类上获得了巨大的提升。&lt;/p&gt;
&lt;p&gt;DCNN的优势：
·&lt;strong&gt;精度：&lt;/strong&gt; DCNN比其他方法在顶点分类任务上精度更高，图分类上表现的也不错。
·&lt;strong&gt;灵活性：&lt;/strong&gt; DCNN提供了图数据的灵活表示，使用简单的处理对顶点特征、边特征以及结构信息进行编码。DCNN可以用于很多分类任务，包括顶点分类，边分类，图分类。
·&lt;strong&gt;速度：&lt;/strong&gt; DCNN的预测可以表示成一系列多项式时间复杂度的tensor操作，模型可以在GPU上实现。&lt;/p&gt;
&lt;h1 id="2-模型"&gt;2 模型
&lt;/h1&gt;&lt;p&gt;长度为 $T$ 的一个图的集合 $\mathcal{G} = \lbrace G_t \mid t \in 1&amp;hellip;T \rbrace $。每个图 $G_t = (V_t, E_t)$ 由顶点 $V_t$ 和边 $E_t$ 组成。顶点一起表示为一个 $N_t \times F$ 的特征矩阵 $X_t$，其中 $N_t$ 是 $G_t$ 的顶点数，边 $E_t$ 通过一个 $N_t \times N_t$ 的邻接矩阵 $A_t$ 编码，通过这个我们可以计算出一个度归一化的转移矩阵 $P_t$，这个矩阵给出了从顶点 $i$ 一步转移到 $j$ 的概率。图 $G_t$ 没有限制，有向无向，带权不带权都可以。对于我们的任务来说，要么是顶点、边有标签 $Y$，要么是图有标签 $Y$，不同情况下 $Y$ 的维度不同。&lt;/p&gt;
&lt;p&gt;如果 $T=1$，也就是只有一个图，标签是顶点或边，那么预测标签 $Y$ 就转换为了半监督分类问题了；如果输入中没有边的表示，就变成了标准的监督问题。如果 $T&amp;gt;1$，标签是每个图的标签，那就是监督图分类问题。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/diffusion-convolutional-neural-networks/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;DCNN接受 $\mathcal{G}$ 作为输入，返回一个 $Y$ 的hard prediction或是条件概率分布 $\mathbb{P}(Y \mid X)$。每个实体（顶点、图、或边）被转换为扩散卷积表示，由 $F$ 个特征上 $H$ 步扩散的维度为 $H \times F$ 的实数矩阵定义，每个实体是由 $H \times F$ 的实数矩阵 $W^c$ 和一个非线性可微分函数 $f$ 计算激活定义的。所以对于顶点分类任务，图 $t$ 的扩散卷积(diffusion-convolutional)表示 $Z_t$，是一个 $N_t \times H \times F$ 的tensor，如图1a所示；对于图或边分类任务，$Z_t$ 是一个 $H \times F$ 或 $N_t \times H \times F$ 的矩阵，如图1b和图1c。(原文里面写的是 $M_t \times H \times F$，我觉得是写错了)&lt;/p&gt;
&lt;p&gt;术语&amp;quot;diffusion-convolution&amp;quot;的意思是唤起卷积神经网络的特征的特征：feature learning, parameter tying, invariance。DCNN核心操作是从顶点和他们的特征映射到从那个顶点开始的扩散过程的结果上。不同于标准的CNN，DCNN参数根据搜索的深度而不是他们在网格中的位置而绑定起来。扩散卷积表示对于顶点的index是不变的，而不是他们的位置；换句话说，两个同质的图的扩散卷积激活会是一样的。不像标准的CNN，DCNN没有池化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;顶点分类&lt;/strong&gt; $P^\ast_t$ 是一个 $N_t \times H \times N_t$ 的tensor，包含了 $P_t$ 的幂序列，对顶点 $i$，$j$ 步，图 $t$ 的特征 $k$ 的扩散卷积的激活值 $Z_{tijk}$ 是：
&lt;/p&gt;
$$\tag{1}
Z\_{tijk} = f(W^c\_{jk} \cdot \sum^{N\_t}\_{l=1}P^*\_{tijl}X\_{tlk})
$$&lt;p&gt;激活使用矩阵形式可以写成：
&lt;/p&gt;
$$\tag{2}
Z\_t = f(W^c \odot P^\ast\_t X\_t)
$$&lt;p&gt;其中 $\odot$ 表示element-wise乘法；图1a。模型只有 $O(H \times F)$ 个参数，使得隐扩散卷积表示的参数数量与输入大小无关。&lt;/p&gt;
&lt;p&gt;模型通过一个连接 $Z$ 和 $Y$ 的dense layer完成。对于 $Y$ 的hard prediction，表示为 $\hat{Y}$，可以通过最大的激活值得到，条件概率分布 $\mathbb{P}(Y \mid X)$ 可以通过使用softmax得到：
&lt;/p&gt;
$$\tag{3}
\hat{Y} = \arg\max(f(W^d \odot Z))
$$&lt;p&gt;
&lt;/p&gt;
$$\tag{4}
\mathbb{P}(Y \mid X) = \mathrm{softmax}(f(W^d \odot Z))
$$&lt;p&gt;&lt;strong&gt;图分类&lt;/strong&gt; DCNN可以通过在顶点上取均值激活扩展成图分类
&lt;/p&gt;
$$\tag{5}
Z\_t = f(W^c \odot 1^T\_{N\_t} P^\ast\_t X\_t / N\_t)
$$&lt;p&gt;
其中 $1_{N_t}$ 是一个 $N_t \times 1$ 的向量，如图1b所示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;学习&lt;/strong&gt; DCNN使用随机梯度下降训练。每轮，顶点的index随机的分到几个batches中。每个batch的error通过取taking slices of the graph definition power series，然后正向、反向、梯度上升更新权重。也使用了windowed early stopping，如果validation error大于前几轮的平均值，就stop。&lt;/p&gt;</description></item><item><title>Convolution on Graph: A High-Order and Adaptive Approach</title><link>https://davidham3.github.io/blog/p/convolution-on-graph-a-high-order-and-adaptive-approach/</link><pubDate>Mon, 16 Jul 2018 11:01:00 +0000</pubDate><guid>https://davidham3.github.io/blog/p/convolution-on-graph-a-high-order-and-adaptive-approach/</guid><description>&lt;p&gt;重新定义了卷积的定义，利用$k$阶邻接矩阵，定义考虑$k$阶邻居的卷积，利用邻接矩阵和特征矩阵构建能同时考虑顶点特征和图结构信息的卷积核。在预测顶点、预测图、生成图三个任务上验证了模型的效果。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1706.09916" target="_blank" rel="noopener"
&gt;Graph Convolution: A High-Order and Adaptive Approach&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="摘要"&gt;摘要
&lt;/h1&gt;&lt;p&gt;我们提出了两个新的模块为图结构数据而设计：k阶卷积器和自适应滤波模块。重要的是，我们的框架(HA-GCN)是一种通用框架，可以在顶点和图上适应多种应用，还有图生成模型。我们的实验效果很好。在顶点分类和分子属性预测上超越了state-of-the-art。生成了超过32%的真实分子，在材料设计和药物筛选上很有效。&lt;/p&gt;
&lt;h1 id="引言"&gt;引言
&lt;/h1&gt;&lt;p&gt;图卷积网络通常应用在两个学习任务上：
·以顶点为中心：预测任务与顶点相关。GCN对图中的每个顶点输出一个特征向量，有效地反映顶点属性和邻居结构。举个例子，社交网络中，向量用于顶点分类和链路预测。有时也和顶点表示学习有关。
·以图为中心：预测任务和图相关。举个例子，化学领域中，分子可以被看作是一个图，原子作顶点，化学键是边。根据分子的物理和化学性质，构建图卷积网络对分子进行有意义地编码。这些任务对很多生活中的应用如材料设计、药物筛选很重要。在这种情况下，图卷积通常对图进行编码，使用编码进行图的预测。&lt;/p&gt;
&lt;p&gt;和我们的工作最相关的是Kipf &amp;amp; Welling 2016a，他们的卷积只考虑了一阶邻居。我们的高阶操作器有着到达k阶邻居的高效设计。此外，我们还引入了自适应模块动态地基于局部图的连接和顶点属性调整权重。对比Li et al., 2015，他们将LSTM引入图中，我们的自适应模块可以解释成Xu et al., 2015提出的注意力机制。不像前人设计的模型要么是以顶点为重，要么是以图为中心，我们的HA-GCN框架是个通用的模型。除此以外，我们用HA-GCN构建了针对分子生成的图生成模型，比state-of-the-art的效果提高了很多。&lt;/p&gt;
&lt;p&gt;贡献有两点：
·引入两个模块，构建新的图卷积网络架构HA-GCN
·提出了可以应用到以顶点为中心、图为中心的通用框架和图生成模型。在所有的任务上获得了state-of-the-art的效果。&lt;/p&gt;
&lt;h1 id="preliminaries"&gt;Preliminaries
&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;The Graph Model&lt;/strong&gt;
一个图$\mathcal{G}$表示为一个对$(V, E)$，$V = \lbrace v_1, &amp;hellip;, v_n \rbrace $是顶点集，$E \in V \times V$是边集。我们不区分有向和无向，因为我们的模型都能做。每个图可以表示为一个$n \times n$的邻接矩阵$A$，如果$v_i$到$v_j$有边，$A_{i,j} = 1$否则为$0$。基于邻接矩阵，我们可以得到距离函数$d(v_i, v_j)$表示$v_i$到$v_j$的距离（最短距离）。此外，我们认为每个顶点$v_i$与一个特征向量$X_i \in \mathcal{R}^m$相关，我们使用$X = (X^T_1, X^T_2, &amp;hellip;, X^T_n) \in \mathcal{R}^{n \times m}$表示特征矩阵。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Graph Convolutional Networks (GCNs)&lt;/strong&gt;
首先，一个顶点$v_j$在图$\mathcal{G}$上的卷积操作可以表示为：
&lt;/p&gt;
$$
L\_{conv}(j) = \sum\_{i \in \mathcal{N}\_j} w\_{ij} X\_i + b\_j
$$&lt;p&gt;
其中$X_i \in R_m$是顶点$v_i$的输入特征，$b_j$是偏置，$w_{ij}$是权重，对$j$是非平稳且变化的。集合$\mathcal{N}_j$表示scope of convolution。对于传统的应用，CNN通常设计成低维度的网格且对每个顶点有着相同的连接。举个例子，图像可以看作二维网格，图$\mathcal{G}$通过邻接的像素组成。$\mathcal{N}_j$可以简单的定义为一个围绕在像素$j$的固定大小的block或window。&lt;/p&gt;
&lt;p&gt;在更一般的图上，可以将$\mathcal{N}_j$定义为顶点$v_j$的邻接顶点的集合。比如，在Duvenaud et al. 2015的工作中，fingerprint(FP)卷积操作器的核心是计算邻居的均值，也就是对于所有的$(i, j)$，$w_{ij} = 1$。通过邻接矩阵$A$，我们可以将这个操作写为
&lt;/p&gt;
$$\tag{1}
L\_{FP} = AX.
$$&lt;p&gt;$A$和特征矩阵$X$的乘积得到一个所有邻居顶点特征的均值。Kipf &amp;amp; Welling 2016a提出的node-GCN使用线性组合与非线性变换得到的均值为：
&lt;/p&gt;
$$\tag{2}
L\_{node-GCN} = \sigma(AXW).
$$&lt;p&gt;权重矩阵$W$，函数$\sigma(\cdot)$分别是特征$X$上的线性组合与非线性变换。&lt;/p&gt;
&lt;p&gt;Bruna et al., 2013与Defferrard et al., 2016提采用了不同的方式在图的拉普拉斯矩阵的谱上做了卷积。$H$为拉普拉斯矩阵，正交分解为$H = U \Lambda U^T$($U$是正交矩阵，$\Lambda$是对角矩阵)。与其在式2中加入权重矩阵，谱卷积考虑的是$H$上的一个参数化的卷积操作；
&lt;/p&gt;
$$\tag{3}
L\_{spectral} = U g\_\theta U^T X.
$$&lt;p&gt;这里$g_\theta(\cdot)$是一个多项式函数，element-wisely应用在对角矩阵$\Lambda$上。&lt;/p&gt;
&lt;p&gt;在讨论谱图卷积的优点时，作者提到$k$阶多项式多项式$g_\theta(\cdot)$就是图的$k$阶局部，也就是说卷积会到达$k$阶邻居。对比式1和式2一阶邻居的均值，这能使信息在图上快速的传播。然而，考虑到$U \Lambda^n U^T = A^n$，多项式$g_\theta(\cdot)$的选择并没有给$k$阶邻居一个明确的卷积操作，因为在卷积中不是所有的邻居都是占相同的份量。这使得我们提出了我们的高阶卷积操作。这些卷积的其他问题是，他们在图中是不变的。因此几乎不能捕获到使用卷积时不同地方的不同。这使得我们提出了自适应模块，成功的考虑了局部特征和图结构。（这里看的云里雾里的。。。）&lt;/p&gt;
&lt;h1 id="high-order-and-adaptive-graph-convolutional-network-ha-gcn"&gt;High-Order and Adaptive Graph Convolutional Network (HA-GCN)
&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;K-th Order Graph Convolution&lt;/strong&gt;
定义顶点$v_j$的$k$阶邻居：$\mathcal{N}_j = \lbrace v_i \in V \mid d(v_i, v_j) \leq k \rbrace $。可以通过对邻接矩阵$A$连乘得到$k$阶邻居。
&lt;strong&gt;Proposition 1.&lt;/strong&gt; $A$是图$\mathcal{G}$的邻接矩阵，$A^k$的第$i$行第$j$列表示的是顶点$i$到顶点$j$的$k$步路径的个数。
我们定义$k$阶卷积如下：
&lt;/p&gt;
$$\tag{4}
\tilde{L}^{(k)}\_{gconv} = (W\_k \circ \tilde{A}^k)X + B\_k,
$$&lt;p&gt;其中
&lt;/p&gt;
$$\tag{5}
\tilde{A}^k = \min\lbrace A^k + I, 1\rbrace .
$$&lt;p&gt;其中$\circ$和$\min$分别表示element-wise矩阵乘法和最小值。$W_k \in \mathcal{R}^{n \times n}$是权重矩阵，$B_k \in \mathcal{R}^{n \times m}$是偏置矩阵。$\tilde{A}^k$是通过将$A^k + I$砍到1获得的。在$A^k$上增加单位阵是为了让图上的每个顶点都有自连接。砍到1是因为如果$A^k$有大于1的元素，砍到1确实会得到k阶邻居的卷积。卷积的输入$\tilde{L}^{(k)}_{gconv}$是邻接矩阵$A \in \lbrace 0, 1\rbrace ^{n \times n}$和特征矩阵$X \in \mathcal{r}^{n \times m}$。输出的维度和$X$一样。如同名字所示，卷积操作$\tilde{L}^{(k)}_{gconv}$取一个顶点的$k$阶邻居的特征向量作为输入，输出他们的加权平均。&lt;/p&gt;
&lt;p&gt;式4的操作优雅地实现了我们在图上$k$阶邻居的idea，和传统的卷积一样，是kernel size为$k$的卷积。一方面，它可以看作是式2从1阶邻居到高阶的高效的泛化。另一方面，卷积器与式3的谱图卷积关系紧密，因为谱图中的$k$阶多项式也可以被看作是一种范围为$k$阶邻居$\mathcal{N}_j$的操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adaptive Filtering Module&lt;/strong&gt;
基于式4，我们引入图卷积的自适应滤波器模块。它根据一个顶点的特征以及邻居的连接过滤卷积的权重。以化学中分子的图为例，在预测分子性质时，benzene rings比alkyl chains更重要。没有自适应模块，图卷积在空间上不变，而且不能按预期的那样工作。自适应滤波器的引入使得网络自适应地找到卷积目标并且更好的捕获局部的不一致。&lt;/p&gt;
&lt;p&gt;自适应滤波器的想法来源于注意力机制Xu et al., 2015，他们在生成输出序列中对应的词时，自适应地的了有趣的像素。也可以看作是一种门的变体，这个门有选择的让信息通过LSTM。从技术上来讲，我们的自适应滤波器是一个在权重矩阵$W_k$上的非线性操作器$g$：
&lt;/p&gt;
$$\tag{6}
\tilde{W\_k} = g \circ W\_k,
$$&lt;p&gt;其中$\circ$表示element-wise矩阵乘法。事实上，操作器$g$是由$\tilde{A}^k$和$X$共同决定的，反映了顶点特征与图的连接，
&lt;/p&gt;
$$
g = f\_{adp}(\tilde{A}^k, X).
$$&lt;p&gt;我们考虑了函数$f_{adp}$的两个选项：
&lt;/p&gt;
$$\tag{7}
f\_{adp/prod} = \mathrm{sigmoid}(\tilde{A}^kXQ)
$$&lt;p&gt;
和
&lt;/p&gt;
$$\tag{8}
f\_{adp/lin} = \mathrm{sigmoid}(Q \cdot [\tilde{A}^k, X]).
$$&lt;p&gt;这里，$[\cdot, \cdot]$表示矩阵拼接。第一个操作器通过$A$和$X$的内积考虑了顶点特征和图连接的的交互，第二个通过线性变换也实现了这个目的。事实上，我们发现线性自适应滤波器(8)比(7)在大多数任务上表现的更好。因此，我们在实验部分会采用并且记录线性的表现。自适应滤波器为了顶点的权重选择而设计出来，因此我们用了一个sigmoid非线性激活使它二值化。参数矩阵$Q$会让$f_{adp}$的输出的维度与矩阵$A$对齐。不像当前已有的动态滤波器只从顶点或边的特征中生成权重，我们的自适应滤波器模块通过同时考虑顶点特征与图的连通性有着更全面的考虑。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Framework of HA-GCN&lt;/strong&gt;
通过在高阶卷积式4中加入自适应模块式6，我们得到HA的定义为：
&lt;/p&gt;
$$
\tilde{L}^{(k)}\_{HA} = (\tilde{W}\_k \circ \tilde{A}^k)X = B\_k
$$&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/convolution-on-graph-a-high-order-and-adaptive-approach/Fig1.JPG"
loading="lazy"
alt="Fig1"
&gt;
图1给了卷积器和HA-GCN框架的可视化。图1a展示了对于一个顶点，$k = 2$时的$\tilde{L}^{(k)}_{HA}$：自适应滤波器$g$的底层加到权重矩阵$W_1$和$W_2$上，得到了自适应权重$\tilde{W}_1$和$\tilde{W}_2$（橙色和绿色线）；第二层把自适应权重和对应的邻接矩阵拼到一起为了卷积使用。图1b强调了卷积是在图上每个顶点都做的，并且是一层一层的。需要注意的是高阶卷积器和自适应权重可以和其他的神经网络架构/操作一起使用，比如全连接层，池化层，非线性变换。我们将我们的HA操作的图卷积层命名为HA-GCN。&lt;/p&gt;
&lt;p&gt;在所有的卷积层之后，我们将那些来自不同阶的卷积层的输出特征拼接起来：
&lt;/p&gt;
$$
L\_{HA} = [\tilde{L}^{(1)}\_{HA}, ..., \tilde{L}^{(K)}\_{HA}].
$$&lt;p&gt;HA-GCN的架构以特征矩阵$X \in \mathcal{R}^{n \times m}$（$n$是图的顶点数，$m$是顶点特征的维数）作为输入，输出一个维度为$n \times (mK)$的矩阵，特征的维数会乘以$K$倍。现在，我们将详细描述如何将HA-GCN应用到各种各样的问题上。&lt;/p&gt;
&lt;p&gt;**以顶点为中心的预测：**HA-GCN的卷积之后，每个顶点和一个特征向量有关。特征向量可以用于顶点分类或回归。与网络表示学习也密切相关。使用以顶点为中心的设定，意味着我们可以给每个顶点学习出一个向量，向量反映了那个顶点周围的图的局部结构。我们的HA-GCN也给图中的每个顶点输出了一个向量。在这个场景下，HA-GCN可以看成是一个监督的图表示学习框架。&lt;/p&gt;
&lt;p&gt;**以图为中心的预测：**为了解决不同尺度的图，输入的邻接矩阵和特征矩阵在底部和右侧加入了为0的padding。以顶点为中心和以图为中心的一个小不同是：以顶点为中心的任务中，一部分有标签/值的顶点作为训练，其他的作验证和输出，而以图为中心的任务，数据集是一组图（可能尺度不同），分为训练/验证/测试集。HA-GCN在这两种情况都能工作，HA卷积层中的参数是$(n^2)$，$n$是图的size（或是那些图中最大的size）。HA-GCN在顶点为中心的任务中比在图为中心的任务更容易过拟合，我们会在后面的实验部分描述这个问题。&lt;/p&gt;
&lt;p&gt;**图生成模型：**从一组图$\bar{\mathcal{G}} = \lbrace \mathcal{G}_1, &amp;hellip;, \mathcal{G}_N \rbrace $中学习一个概率模型，通过这个模型我可以生成之前没见过但是和$\bar{\mathcal{G}}$中相似的图。通过variational auto-encoder(Kingma and Welling 2013)和adversarial auto-encoder(Makhzani et al., 2015)，图卷积网络可以适用于生成模型的任务甚至是判别模型。&lt;/p&gt;
&lt;p&gt;一个自编码器总是由两部分组成：一个编码器和一个解码器。编码器将输入数据$X \in \mathcal{X}$映射到一个编码向量$Y \in \mathcal{Y}$，解码器将$\mathcal{Y}$映射回$\mathcal{X}$。我们称编码空间$\mathcal{Y}$为隐藏空间。为了使他成为一个生成模型，我们通常假设隐藏空间中有一个概率分布（比如高斯分布）。图生成模型能让我们生成分子的连续表达，通过搜索隐藏空间生成新的化学结构，可以用来指导材料设计和药物筛选。&lt;/p&gt;
&lt;h1 id="实验"&gt;实验
&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Node-centric learning&lt;/strong&gt;
citation graphs上的监督文档分类，每个图包含文档的bag-of-words特征向量，还有文档之间的引用连接。我们把这个网络当成无向图，构建一个二值对称邻接矩阵$A$。每个文档有一个类标，目标是从文档的特征和引用的图对文档标签预测。统计数据(Sen et al., 2008)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center"&gt;Dataset&lt;/th&gt;
&lt;th style="text-align: center"&gt;Nodes&lt;/th&gt;
&lt;th style="text-align: center"&gt;Edges&lt;/th&gt;
&lt;th style="text-align: center"&gt;Classes&lt;/th&gt;
&lt;th style="text-align: center"&gt;Features&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center"&gt;Citeseer&lt;/td&gt;
&lt;td style="text-align: center"&gt;3327&lt;/td&gt;
&lt;td style="text-align: center"&gt;4732&lt;/td&gt;
&lt;td style="text-align: center"&gt;6&lt;/td&gt;
&lt;td style="text-align: center"&gt;3703&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center"&gt;Cora&lt;/td&gt;
&lt;td style="text-align: center"&gt;2708&lt;/td&gt;
&lt;td style="text-align: center"&gt;5429&lt;/td&gt;
&lt;td style="text-align: center"&gt;7&lt;/td&gt;
&lt;td style="text-align: center"&gt;1433&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center"&gt;Pubmed&lt;/td&gt;
&lt;td style="text-align: center"&gt;19717&lt;/td&gt;
&lt;td style="text-align: center"&gt;44338&lt;/td&gt;
&lt;td style="text-align: center"&gt;210&lt;/td&gt;
&lt;td style="text-align: center"&gt;5414&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;**训练和架构：**我们使用和Kipf &amp;amp; Welling同样的GCN网络结构，除了将他们的一阶图卷积层换成了我们的HA层。我们使用$gcn_{1,&amp;hellip;,k}$表示$1$阶到$k$阶的图卷积层。$\mathrm{fc}k$表示有$k$个隐藏单元的全连接层。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center"&gt;Name&lt;/th&gt;
&lt;th style="text-align: center"&gt;Architectures&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center"&gt;GCN&lt;/td&gt;
&lt;td style="text-align: center"&gt;gcn_{1}-fc128-gcn_{1}-fc1-softmax&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center"&gt;gcn_{1, 2}&lt;/td&gt;
&lt;td style="text-align: center"&gt;gcn{1, 2}-fc128-gcn{1, 2}-fc1-softmax&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center"&gt;adp_gcn_{1, 2}&lt;/td&gt;
&lt;td style="text-align: center"&gt;adp_gcn{1, 2}-fc128-adp_gcn{1, 2}-fc1-softmax&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;为了比较不同模型的性能，我们将数据集随机划分成训练/验证/测试集，比例为$7:1.5:1.5$，记录测试集的预测精度，表1。超参数：dropout rate $0.7$，L2 regularization $0.5 \cdot 10^{-8}$，hidden units $128$。从顶点表示学习角度看，前三个是半监督的模型，后四个是半监督的模型。这也解释了为什么后面的模型效果更好。我们的二姐邻居HA图卷积在精度上提升了2%。自适应模块没能继续提升。这是因为自适应模块是为了对不同的图生成不同的滤波器权重。然而，在顶点为中心的任务中，只有一个图，卷积权重直接就学出来了。因此自适应模块在顶点为中心的任务中是冗余的。
&lt;img src="https://davidham3.github.io/blog/images/convolution-on-graph-a-high-order-and-adaptive-approach/Table1.JPG"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Graph-centric learning&lt;/strong&gt;
预测分子图。目标是给定分子图，预测分子性质。我们使用Duvenaud et al., 2015描述的数据集，评估三种属性：
Solubility, Drug efficacy, Organic photovolatic efficiency。
训练和架构：
l1_gcn和l2_gcn分别表示有一个和两个卷积层的卷积神经网络。我们记录了RMSE（表2）。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center"&gt;Name&lt;/th&gt;
&lt;th style="text-align: center"&gt;Architectures&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center"&gt;l1_gcn&lt;/td&gt;
&lt;td style="text-align: center"&gt;gcn_{1,2,3}-ReLU-fc64-ReLU-fc16-ReLU-fc1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center"&gt;l1_adp_gcn&lt;/td&gt;
&lt;td style="text-align: center"&gt;adp_gcn{1,2,3}-ReLU-fc64-ReLU-fc16-ReLU-fc1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center"&gt;l2_gcn&lt;/td&gt;
&lt;td style="text-align: center"&gt;[gcb_{1,2,3}-ReLU]*2-fc64-ReLU-fc16-ReLU-fc1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center"&gt;l2_adp_gcn&lt;/td&gt;
&lt;td style="text-align: center"&gt;[adp_gcn_{1,2,3}-ReLU]*2-fc64-ReLU-fc16-ReLU-fc1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/convolution-on-graph-a-high-order-and-adaptive-approach/Table2.JPG"
loading="lazy"
alt="Table2"
&gt;&lt;/p&gt;
&lt;p&gt;node-GCN就是没有自适应滤波模块的一阶HA-GCN。对比node-GCN,l1_gcn,l2_gcn，可以看到我们的卷积层的效果。有自适应滤波器的比没有的效果好。&lt;/p&gt;
&lt;p&gt;还有一部分图生成模型，就不说了。&lt;/p&gt;</description></item><item><title>Graph Attention Networks</title><link>https://davidham3.github.io/blog/p/graph-attention-networks/</link><pubDate>Fri, 13 Jul 2018 16:54:31 +0000</pubDate><guid>https://davidham3.github.io/blog/p/graph-attention-networks/</guid><description>&lt;p&gt;ICLR 2018。图注意力网络，使用 self-attention 来构建 graph attentional layer，attention 会考虑当前顶点所有的邻居对它的重要性，基于谱图理论的模型不能应用到其他不同结构的图上，而这个基于attention的方法能有效的解决这个问题。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener"
&gt;Graph Attention Networks&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="摘要"&gt;摘要
&lt;/h1&gt;&lt;p&gt;我们提出了图注意力网络(GAT)，新型图神经网络，利用自注意力层解决基于图卷积及其相似方法的缺点。通过堆叠这种层（层中的顶点会注意邻居的特征），我们可以给邻居中的顶点指定不同的权重，不需要任何一种耗时的矩阵操作（比如求逆）或依赖图结构的先验知识。我们同时解决了基于谱的图神经网络的几个关键挑战，并且使我们的模型很轻松的应用在 inductive 或 transductive 问题上。我们的 GAT 模型在4个 transductive 和 inductive 图数据集上达到且匹敌当前最先进的算法：&lt;em&gt;Cora&lt;/em&gt;, &lt;em&gt;Citeseer&lt;/em&gt;, &lt;em&gt;Pubmed citation networks&lt;/em&gt;，&lt;em&gt;protein-protin interaction&lt;/em&gt;。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 INTRODUCTION
&lt;/h1&gt;&lt;p&gt;CNN 结构可以有效的重复使用卷积核，在网格型的数据上应用。然而很多问题都是基于图结构的。&lt;/p&gt;
&lt;p&gt;早期的工作使用RNN来处理图结构中数据的表示。2005 年和 2009 年提出了 GNN（Graph Neural Networks）的概念，作为 RNN 的泛化，可以直接处理更一般的图结构，比如带环图、有向、无向图。GCN 包含了迭代的过程，迭代时顶点状态向前传播，后面使用一个神经网络来产生输出。Li et al., 2016使用了Cho et al., 2014提出的门控循环单元进行改进。&lt;/p&gt;
&lt;p&gt;进一步的研究分为谱方法和非谱方法。&lt;/p&gt;
&lt;p&gt;一方面，谱方法使用图的谱表示，成功应用到了顶点分类的问题上。Bruna et al., 2014在傅里叶域中定义了卷积操作，通过计算拉普拉斯矩阵的特征值分解，由于潜在的大量的计算，引出了后续的非谱方法。Henaff et al., 2015引入了带有 smooth coefficients 的谱滤波器，可以使他们在空间局部化。后来 Defferrard et al., 2016 提出了通过拉普拉斯矩阵的切比雪夫多项式展开的近似表达。最后，Kipf &amp;amp; Welling 2017 通过限制滤波器只考虑 1 阶邻居从而简化了之前的方法。然而，前面提到的所有的谱方法，学习到的卷积核参数都依赖于拉普拉斯特征值分解后的特征向量，也就是说依赖于图结构。因此，训练在一个指定图结构的模型不能应用到不同结构的图上。&lt;/p&gt;
&lt;p&gt;另一方面，还有一些非谱方法 (Duvenaud et al., 2015; Atwood &amp;amp; Towsley, 2016; Hamilton et al., 2017)，这些方法直接在图上定义卷积操作，直接在空间上相近的邻居上应用卷积操作。这些方法的一个挑战是需要定义一个能处理不同数量邻居的卷积操作，并且保证 CNN 权重共享的性质。在某些情况下，这需要学习为每个 node degree 学习一个权重矩阵 (Duvenaud et al., 2015)，在对每个 input channel 和 neighborhood degree 训练权重时，使用转移矩阵的幂定义邻居 (Atwood &amp;amp; Towsley, 2016)，或是对有着固定数量顶点的邻居进行提取和归一化(Niepert et al., 2016)。Monti et al., 2016提出了混合模型 CNN，(MoNet)，这个空间方法提供了一个 CNN 在图上泛化的统一的模型。最近，Hamilton et al., 2017提出了 GraphSAGE，对每个顶点采样采出一个固定数量的邻居，然后使用一个指定的聚合操作聚集他们（比如取所有采样邻居的均值，或是将他们放进RNN后产生的结果）。这个方法在几个大规模的 inductive 数据集上获得了很惊人的效果。&lt;/p&gt;
&lt;p&gt;注意力机制在很多基于序列的任务中已经成为了一个标准 (Bahdanau et al., 2015; Gehring et al., 2016)。注意力机制的一个好处是可以处理变长输入，专注于输入中最相关的部分来做决策。使用注意力机制计算一个序列的表示时，一般提到的是 &lt;em&gt;self-attention&lt;/em&gt; 或 &lt;em&gt;intra-attention&lt;/em&gt;。与 RNN 或卷积一起使用时，self-attention 在机器阅读(Cheng et al., 2016)和学习句子表示(Lin et al., 2017)这些任务上很有用。然而，Vaswani et al., 2017的研究表明，self-attention 不仅可以提升 RNN 和卷积的模型，在机器翻译任务上也是可以构建出性能最强的模型的。&lt;/p&gt;
&lt;p&gt;受最近工作的启发，我们提出了基于 attention 的架构对图结构的顶点进行分类。思路是通过对顶点邻居的注意，计算图中每个顶点的表示，然后使用一个 self-attention 机制。注意力架构有几个有趣的性质：(1) 操作高效，因为它可以“顶点-邻居”对上并行计算；(2) 通过指定对邻居任意的权重，它可以在有着不同度的顶点上使用；(3) 模型可以直接应用在 inductive learning 任务上，包括模型必须要生成完整的未见过的图等任务。我们在4个 benchmark 上验证了我们的方法：&lt;em&gt;Cora&lt;/em&gt;，&lt;em&gt;Citeseer&lt;/em&gt;，&lt;em&gt;Pubmed citation networks&lt;/em&gt;，&lt;em&gt;protein-protein interaction&lt;/em&gt;，获得了比肩 state-of-the-art 的结果，展现了基于 attention 的模型在处理任意结构的图的可能性。&lt;/p&gt;
&lt;p&gt;值得注意的是，如 Kipf &amp;amp; Welling 2017和Atwood &amp;amp; Towsley 2016，我们的工作可以重写为 MoNet(Monti et al., 2016)的一种特殊形式。除此以外，我们的分享神经网络跨边计算时是对关系网络公式的联想(Santoro et al., 2017)和VAIN(Hoshen, 2017)，在这两篇文章中，object 和 agent 间的关系被聚合成对，通过使用一种共享机制。相似地，我们的注意力模型可以与 Duan et al., 2017 和 Denil et al., 2017 的工作相连，他们使用一个邻居注意力操作来计算环境中不同 object 的注意力系数。其他相关的方法包括局部线性嵌入(LLE)(Roweis &amp;amp; Saul, 2000)和记忆网络(Weston et al., 2014)。LLE 在每个数据点选择了固定数量的邻居，为每个邻居学习了权重系数，以此将每个数据点重构为邻居的加权之和。之后的优化提取了顶点嵌入的特征。记忆网络与我们的工作也有关系，如果我们将一个顶点的邻居解释为记忆，通过注意它的值可以计算顶点特征，之后通过在同样的位置存储新特征进行更新。&lt;/p&gt;
&lt;h1 id="2-gat-architecture"&gt;2 GAT ARCHITECTURE
&lt;/h1&gt;&lt;p&gt;我们会在这部分描述如何创建 block layer 来构造任意的 graph attention networks，并且指明理论和实际上的优点以及相比于之前在神经图处理上的工作的缺点。&lt;/p&gt;
&lt;h2 id="21-graph-attentional-layer"&gt;2.1 Graph Attentional Layer
&lt;/h2&gt;&lt;p&gt;首先描述单个 graph attentional layer，因为这种层会在整个 GAT 架构中使用。我们使用的 attention 和 Bahdanau et al., 2015 的工作相似。&lt;/p&gt;
&lt;p&gt;输入是一组顶点特征，${\mathbf{h}} = \lbrace \vec{h}_1, \vec{h}_2, &amp;hellip;, \vec{h}_N \rbrace , \vec{h}_i \in \mathbb{R}^F$，其中 $N$ 是顶点数，$F$ 是每个顶点的特征数。这个层会生成一组新的顶点特征，${\mathbf{h}&amp;rsquo;} = \lbrace \vec{h}&amp;rsquo;_1, \vec{h}&amp;rsquo;_2, &amp;hellip;, \vec{h}&amp;rsquo;_N\rbrace , \vec{h}&amp;rsquo;_i \in \mathbb{R}^{F&amp;rsquo;}$，作为输出。&lt;/p&gt;
&lt;p&gt;为了在将输入特征变换到高维特征时获得充足的表现力，至少需要一个可学习的线性变换。为了到达这个目的，每个顶点都会使用一个共享参数的线性变换，参数为 ${\mathbf{W}} \in \mathbb{R}^{F&amp;rsquo; \times F}$。然后在每个顶点上做一个 self-attention ——一个共享的attention机制 $a : \mathbb{R}^{F&amp;rsquo;} \times \mathbb{R}^{F&amp;rsquo;} \rightarrow \mathbb{R}$ 来计算注意力分数 &lt;em&gt;attention coefficients&lt;/em&gt;：&lt;/p&gt;
$$\tag{1}
e\_{ij} = a(\mathbf{W} \vec{h}\_i, \mathbf{W} \vec{h}\_j)
$$&lt;p&gt;表示顶点 $j$ 的特征对顶点 $i$ 的重要性(&lt;em&gt;importance&lt;/em&gt;)。在一般的公式中，模型可以使每个顶点都注意其他的每个顶点，扔掉所有的结构信息。我们使用 &lt;em&gt;mask attention&lt;/em&gt; 使得图结构可以注入到注意力机制中——我们只对顶点 $j \in \mathcal{N_i}$ 计算$e_{ij}$，其中$\mathcal{N_i}$ 是顶点 $i$ 在图中的一些邻居。在我们所有的实验中，这些是 $i$ 的一阶邻居（包括 $i$ ）。为了让系数在不同的顶点都可比，我们对所有的 $j$ 使用 softmax 进行了归一化：&lt;/p&gt;
$$\tag{2}
\alpha\_{ij} = \mathrm{softmax}\_j (e\_{ij}) = \frac{\exp{e\_{ij}}}{\sum\_{k \in \mathcal{N}\_i} \exp{e\_{ik}}}
$$&lt;div align="center"&gt;![Figure1](/blog/images/graph-attention-networks/Fig1.JPG)&lt;/div&gt;
&lt;p&gt;在我们的实验中，注意力机制 $a$ 是一个单层的前向传播网络，参数为权重向量 $\vec{\text{a}} \in \mathbb{R}^{2F&amp;rsquo;}$，使用LeakyReLU作为非线性层（斜率$\alpha = 0.2$）。整个合并起来，注意力机制计算出的分数（如图1左侧所示）表示为：&lt;/p&gt;
$$\tag{3}
\alpha\_{ij} = \frac{ \exp{ ( \mathrm{LeakyReLU} ( \vec{\text{a}}^T [\mathbf{W} \vec{h}\_i \Vert \mathbf{W} \vec{h}\_j ] ))}}{\sum\_{k \in \mathcal{N\_i}} \exp{(\mathrm{LeakyReLU}(\vec{\text{a}}^T [\mathbf{W} \vec{h}\_i \Vert \mathbf{W} \vec{h}\_k]))}}
$$&lt;p&gt;其中 $·^T$ 表示转置，$\Vert$ 表示concatenation操作。&lt;/p&gt;
&lt;p&gt;得到归一化的分数后，使用归一化的分数计算对应特征的线性组合，作为每个顶点最后的输出特征（最后可以加一个非线性层，$\sigma$）：&lt;/p&gt;
$$\tag{4}
\vec{h}'\_i = \sigma(\sum\_{j \in \mathcal{N}\_i} \alpha\_{ij} \mathbf{W} \vec{h}\_j)
$$&lt;p&gt;为了稳定 self-attention 的学习过程，我们发现使用 &lt;em&gt;multi-head attention&lt;/em&gt; 来扩展我们的注意力机制是很有效的，就像 Vaswani et al., 2017。特别地，$K$ 个独立的 attention 机制执行 式4 这样的变换，然后他们的特征连(concatednated)在一起，就可以得到如下的输出：&lt;/p&gt;
$$\tag{5}
\vec{h}'\_i = \Vert^{K}\_{k=1} \sigma(\sum\_{j \in \mathcal{N}\_i} \alpha^k\_{ij} \mathbf{W}^k \vec{h}\_j)
$$&lt;p&gt;其中 $\Vert$ 表示concatenation，$\alpha^k_{ij}$ 是通过第 $k$ 个注意力机制 $(a^k)$ 计算出的归一化的注意力分数，$\mathbf{W}^k$ 是对应的输入线性变换的权重矩阵。注意，在这里，最后的返回输出 $\mathbf{h}&amp;rsquo;$，每个顶点都会有 $KF&amp;rsquo;$ 个特征（不是 $F&amp;rsquo;$ ）。
特别地，如果我们在网络的最后一层使用 multi-head attention，concatenation 就不再可行了，我们会使用 &lt;em&gt;averaging&lt;/em&gt;，并且延迟使用最后的非线性层（分类问题通常是 softmax 或 sigmoid ）：&lt;/p&gt;
$$
\vec{h}'\_i = \sigma(\frac{1}{K} \sum^K\_{k=1} \sum\_{j \in \mathcal{N}\_i} \alpha^k\_{ij} \mathbf{W}^k \vec{h}\_j)
$$&lt;p&gt;multi-head 图注意力层的聚合过程如图1右侧所示。&lt;/p&gt;
&lt;h2 id="22-comparisons-to-related-work"&gt;2.2 Comparisons to related work
&lt;/h2&gt;&lt;p&gt;2.1节描述的图注意力层直接解决了之前在图结构上使用神经网络建模的方法的几个问题：
· 计算高效：self-attention层的操作可以在所有的边上并行，输出特征的计算可以在所有顶点上并行。没有耗时的特征值分解。单个的GAT计算$F&amp;rsquo;$个特征的时间复杂度可以压缩至$O(\vert V \vert F F&amp;rsquo; + \vert E \vert F&amp;rsquo;)$，$F$是输入的特征数，$\vert V \vert$和$\vert E \vert$是图中顶点数和边数。复杂度与Kipf &amp;amp; Welling, 2017的GCN差不多。尽管使用multi-head attention可以并行计算，但也使得参数和空间复杂度变成了$K$倍。
· 对比GCN，我们的模型允许对顶点的同一个邻居分配不同的重要度，使得模型能力上有一个飞跃。不仅如此，对学习到的attentional权重进行分析可以得到更好的解释性，就像机器翻译领域一样（比如Bahdanau et al., 2015的定性分析）。
· 注意力机制以一种共享的策略应用在图的所有的边上，因此它并不需要在之前就需要得到整个图结构或是所有的顶点的特征（很多之前的方法的缺陷）。因此这个方法有几个影响：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;图不需要是无向的（如果边$j \rightarrow i$没有出现，我们可以直接抛弃掉$\alpha_{ij}$的计算）&lt;/li&gt;
&lt;li&gt;这个方法可以直接应用到&lt;em&gt;inductive learning&lt;/em&gt;——包括在训练过程中在完全未见过的图上评估模型的任务上。最近发表的Hamilton et al., 2017的inductive方法为了保持计算过程的一致性，对每个顶点采样采了一个固定数量的邻居；这就使得这个方法在推断的时候不能考虑所有的邻居。此外，在使用一个基于LSTM的邻居聚合方式时（Hochreiter &amp;amp; Schmidhuber, 1997），这个方法在某些时候能达到最好的效果。这意味着存在邻居间存在一个一致的顶点序列顺序，作者通过将随机顺序的序列输入至LSTM来验证它的一致性。我们的方法不会受到这些问题中任意一个的影响——它在所有的邻居上运算（虽说会有计算上的开销，但扔能和GCN这样的速度差不多），并且假设任意顺序都可以。
· 如第一节提到的，GAT可以重写成MoNet(Monti et al., 2016)的一种特殊形式。更具体的来说，设pseudo-coordinate function为$u(x, y) = f(x) \Vert f(y)$，$f(x)$表示$x$的特征（可能是MLP变换后的结果），$\Vert$表示concatenation；权重函数为$w_j(u) = \mathrm{softmax}(\mathrm{MLP}(u))$（softmax在一个顶点所有的邻居上计算）会使MoNet的patch operator和我们的很相似。尽管如此，需要注意到的是，对比之前MoNet的实例，我们的模型使用顶点特征计算相似性，而不是顶点的结构性质（这需要之前就已经直到图结构）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们可以做出一种使用稀疏矩阵操作的GAT层，将空间复杂度降低到顶点和边数的线性级别，使得GAT模型可以在更大的图数据集上运行。然而，我们使用的tensor操作框架只支持二阶tensor的稀疏矩阵乘法，限制了当前实现的版本的模型能力（尤其在有多个图的数据集上）。解决这个问题是未来的一个重要研究方向。在这些使用稀疏矩阵的场景下，在某些图结构下GPU的运算并不能比CPU快多少。另一个需要注意的地方是我们的模型的感受野的大小的上届取决于网络的深度（与GCN和其他模型相似）。像skip connections(He et al., 2016)这样的技术可以来近似的扩展模型的深度。最后，在所有边上的并行计算，尤其是分布式的计算可以设计很多冗余的计算，因为图中的邻居往往高度重叠。&lt;/p&gt;
&lt;h1 id="3-evaluation"&gt;3 Evaluation
&lt;/h1&gt;&lt;p&gt;我们与很多强力的模型进行了对比，在四个基于图的数据集上，达到了state-of-the-art的效果。这部分将总结一下我们的实验过程与结果，并对GAT提取特征表示做一个定性分析。
&lt;img src="https://davidham3.github.io/blog/images/graph-attention-networks/Table1.JPG"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;
&lt;h2 id="31-datasets"&gt;3.1 Datasets
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Transductive learning&lt;/strong&gt; 我们使用了三个标准的citation network benchmark数据集——Cora, Citeseer和Pubmed(Sen et al., 2008)——并按Yang et al., 2016做的transductive实验。这些数据集中，顶点表示文章，边（无向）表示引用。顶点特征表示文章的BOW特征。每个顶点有一个类标签。我们使用每类20个顶点用来训练，训练算法使用所有的顶点特征。模型的预测性能是在1000个测试顶点上进行评估的，我们使用了500个额外的顶点来验证意图（像Kipf &amp;amp; Welling 2017）。Cora数据集包含了2708个顶点，5429条边，7个类别，每个顶点1433个特征。Citeseer包含3327个顶点，4732条边，6类，每个顶点3703个特征。Pubmed数据集包含19717个顶点，44338条边，3类，每个顶点500个特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inductive learning&lt;/strong&gt; 我们充分利用protein-protein interaction(PPI)数据集，这个数据集包含了不同的人体组织（Zitnik &amp;amp; Leskovec, 2017）。数据集包含了20个图来训练，2个验证，2个测试。关键的是，测试的图包含了训练时完全未见过的图。为了构建图，我们使用Hamilton et al., 2017预处理后的数据。平均每个图的顶点数为2372个。每个顶点有50个特征，组成了positional gene sets，motif gene sets and immunological signatures。从基因本体获得的每个顶点集有121个标签，由Molecular Signatures Database(Subramanian et al., 2005)收集，一个顶点可以同时拥有多个标签。&lt;/p&gt;
&lt;p&gt;这些数据集的概貌在表1中给出。&lt;/p&gt;
&lt;h2 id="32-state-of-the-art-methods"&gt;3.2 State-of-the-art methods
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Transductive learning&lt;/strong&gt; 对于transductive learning任务，我们对比了Kipf &amp;amp; Welling 2017的工作，以及其他的baseline。包括了label propagation(LP)(Zhu et al., 2003)，半监督嵌入(SemiEmb)(Weston et al., 2012)，manifold regulariization(ManiReg)(Belkin et al., 2006)，skip-gram based graph embeddings(DeepWalk)(Perozzi et al., 2014)，the iterative classification algorithm(ICA)(Lu &amp;amp; Getoor, 2003)和Planetoid(Yang et al., 2016)。我们也直接对比了GCN(Kipf &amp;amp; Welling 2017)，还有利用了高阶切比雪夫的图卷积模型(Defferrard et al., 2016)，还有Monti et al., 2016提出的MoNet。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inductive learning&lt;/strong&gt; 对于inductive learning任务，我们对比了Hamilton et al., 2017提出的四个不同的监督的GraphSAGE 这些方法提供了大量的聚合特征：GraphSAGE-GCN（对图卷积操作扩展inductive setting），GraphSAGE-mean（对特征向量的值取element-wise均值），GraphSAGE-LSTM（通过将邻居特征输入到LSTM进行聚合），GraphSAGE-pool（用一个共享的多层感知机对特征向量进行变换，然后使用element-wise取最大值）。其他的transductive方法要么在inductive中完全不合适，要么就认为顶点是逐渐加入到一个图中，使得他们不能在完全未见过的图上使用（如PPI数据集）。&lt;/p&gt;
&lt;p&gt;此外，对于两种任务，我们提供了每个顶点共享的MLP分类器（完全没有整合图结构信息）的performance。&lt;/p&gt;
&lt;h2 id="33-experimental-setup"&gt;3.3 Experimental Setup
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Transductive learning&lt;/strong&gt; 我们使用一个两层的GAT模型。超参数在Cora上优化过后在Citeseer上复用。第一层包含$K = 8$个attention head，计算得到$F&amp;rsquo; = 8$个特征（总共64个特征），之后接一个指数线性单元（ELU）（Clevert et al., 2016）作为非线性单元。第二层用作分类：一个单个的attention head计算$C$个特征（其中$C$是类别的数量），之后用softmax激活。处理小训练集时，在模型上加正则化。在训练时，我们使用$L_2$正则化，$\lambda = 0.0005$。除此以外，两个层的输入都使用了$p = 0.6$的dropout(Srivastava et al., 2014)，在&lt;em&gt;normalized attention coefficients&lt;/em&gt;上也使用了（也就是在每轮训练时，每个顶点都被随机采样邻居）。如Monti et al., 2016观察到的一样，我们发现Pubmed的训练集大小(60个样本)需要微调：我们使用$K = 8$个attention head，加强了$L_2$正则，$\lambda = 0.001$。除此以外，我们的结构都和Cora和Citeseer的一样。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inductive learning&lt;/strong&gt;
我们使用一个三层的GAT模型。前两层$K = 4$，计算$F&amp;rsquo; = 256$个特征（总共1024个特征），然后使用ELU。最后一层用于多类别分类：$K = 6$，每个计算121个特征，取平均后使用logistic sigmoid激活。训练集充分大所以不需要使用$L_2$正则或dropout——但是我们使用了skip connections(He et al., 2016)在attentional layer间。训练时batch size设置为2个图。为了严格的衡量出使用注意力机制的效果（与GCN相比），我们也提供了&lt;em&gt;constant attention mechanism&lt;/em&gt;，$a(x, y) = 1$，使用同样的架构——也就是每个邻居上都有相同的权重。&lt;/p&gt;
&lt;p&gt;两个模型都使用了Glorot初始化(Glorot &amp;amp; Bengio, 2010)，使用Adam SGD(Kingma &amp;amp; Ba, 2014)优化cross-entropy，Pubmed上初始学习率是0.01，其他数据集是0.005。我们在cross-entropy loss和accuracy(transductive)或micro-F1(inductive)上都使用了early stopping策略，迭代次数为100轮。
代码：https://github.com/PetarV-/GAT&lt;/p&gt;
&lt;h2 id="34-results"&gt;3.4 Results
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/graph-attention-networks/Table2.JPG"
loading="lazy"
alt="Table2"
&gt;
对于transductive任务，我们提交了我们的方法100次的平均分类精度（还有标准差），也用了Kipf &amp;amp; Welling., 2017和Monti et al., 2016的metrics。特别地，对于基于切比雪夫方法(Defferrard et al., 2016)，我们提供了二阶和三阶最好的结果。为了公平的评估注意力机制的性能，我们还评估了一个计算出64个隐含特征的GCN模型，并且尝试了ReLU和ELU激活，记录了100轮后更好的那个结果（GCN-64*）（结果显示ReLU更好）。
&lt;img src="https://davidham3.github.io/blog/images/graph-attention-networks/Table3.JPG"
loading="lazy"
alt="Table3"
&gt;
对于inductive任务，我们计算了micro-averaged F1 score在两个从未见过的测试图上，平均了10次结果，也使用了Hamilton et al., 2017的metrics。特别地，因为我们的方法是监督的，我们对比了GraphSAGE。为了评估聚合所有的邻居的优点，我们还提供了我们通过修改架构（三层GraphSAGE-LSTM分别计算[512, 512, 726]个特征，128个特征用来聚合邻居）所能达到的GraphSAGE最好的结果（GraphSAGE*）。最后，为了公平的评估注意力机制对比GCN这样的聚合方法，我们记录了我们的constant attention GAT模型的10轮结果（Const-GAT）。
结果展示出我们的方法在四个数据集上都很好，和预期一致，如2.2节讨论的那样。具体来说，在Cora和Citeseer上我们的模型上升了1.5%和1.6%，推测应该是给邻居分配不同的权重起到了效果。值得注意的是在PPI数据集上：我们的GAT模型对于最好的GraphSAGE结果提升了20.5%，这意味着我们的模型可以应用到inductive上，通过观测所有的邻居，模型会有更强的预测能力。此外，针对Const-GAT也提升3.9%，再一次展现出给不同的邻居分配不同的权重的巨大提升。&lt;/p&gt;
&lt;p&gt;学习到的特征表示的有效性可以定性分析——我们提供了t-SNE(Maaten &amp;amp; Hinton, 2008)的可视化——我们对在Cora上面预训练的GAT模型中第一层的输出做了变换（图2）。representation在二维空间中展示出了可辩别的簇。注意，这些簇对应了数据集的七个类别，验证了模型在Cora上对七类的判别能力。此外，我们可视化了归一化的attention系数（对所有的8个attention head取平均）的相对强度。像Bahdanau et al., 2015那样适当的解释这些系数需要更多的领域知识，我们会在未来的工作中研究。
&lt;img src="https://davidham3.github.io/blog/images/graph-attention-networks/Fig2.JPG"
loading="lazy"
alt="Figure2"
&gt;&lt;/p&gt;
&lt;h1 id="4-conclusions"&gt;4 Conclusions
&lt;/h1&gt;&lt;p&gt;我们展示了图注意力网络(GAT)，新的卷积风格神经网络，利用masked self-attentional层。图注意力网络计算高效（不需要耗时的矩阵操作，在图中的顶点上并行计算），处理不同数量的邻居时对邻居中的不同顶点赋予不同的重要度，不需要依赖整个图的结构信息——因此解决了之前提出的基于谱的方法的问题。我们的这个利用attention的模型在4个数据集针对transductive和inductive（特别是对完全未见过的图），对顶点分类成功地达到了state-of-the-art的performance。&lt;/p&gt;
&lt;p&gt;未来在图注意力网络上有几点可能的改进与扩展，比如解决2.2节描述的处理大批数据时的实际问题。还有一个有趣的研究方向是利用attention机制对我们的模型进行一个深入的解释。此外，扩展我们的模型从顶点分类到图分类也是一个更具应用性的方向。最后，扩展我们的模型到整合边的信息（可能制视了顶点关系）可以处理更多的问题。&lt;/p&gt;</description></item><item><title>GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction</title><link>https://davidham3.github.io/blog/p/geoman-multi-level-attention-networks-for-geo-sensory-time-series-prediction/</link><pubDate>Mon, 09 Jul 2018 17:03:00 +0000</pubDate><guid>https://davidham3.github.io/blog/p/geoman-multi-level-attention-networks-for-geo-sensory-time-series-prediction/</guid><description>&lt;p&gt;IJCAI 2018，看了一部分，还没看完。原文链接：&lt;a class="link" href="https://www.ijcai.org/proceedings/2018/476" target="_blank" rel="noopener"
&gt;GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;大量的监测器被部署在各个地方，连续协同地监测周围的环境，如空气质量。这些检测器生成很多时空序列数据，之间有着空间相关性。预测这些时空数据很有挑战，因为预测受很多因素影响，比如动态的时空关联和其他因素。我们在这篇论文中使用多级基于注意力机制的 RNN 模型，结合空间、气象和检测器数据来预测未来的监测数值。我们的模型由两部分组成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;多级注意力机制对时空依赖关系建模&lt;/li&gt;
&lt;li&gt;一个通用的融合模块对多领域的外部信息进行融合&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;实验用了两个真实的数据集，空气质量数据和水质监测数据，结果显示我们的模型比9个baselines都要好。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 Introduction
&lt;/h1&gt;&lt;p&gt;现实世界中有大量的检测器，如空气监测站。每个监测站都有一个地理位置，不断地生成时间序列数据。一组检测器不断的监测一个区域的环境，数据间有空间依赖关系。我们成这样的监测数据为 &lt;em&gt;geosensory time series&lt;/em&gt;。此外，一个检测器生成多种 geo-sensory 时间序列是很常见的，因为这个检测器同时监测不同的目标。举个例子，图1a，路上的循环检测器实时记录车辆通行情况，也记录他们的速度。图1b 展示了检测器每五分钟生成的关于水质的三个不同的气候指标。除了监测，对于 geo-sensory 时间序列预测还有一个重要的需求就是交通预测。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/geoman-multi-level-attention-networks-for-geo-sensory-time-series-prediction/Fig1.PNG"
loading="lazy"
alt="Fig1"
&gt;&lt;/p&gt;
&lt;p&gt;然而，预测 geo-sensory 时间序列很有挑战性，主要受两个因素影响：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;动态时空关系
·检测器间复杂的关系。图1c展示了不同检测器的时间序列间的空间关系是高度动态的，随着时间不断改变。除此以外，geo-sensory时间序列根据地区有非线性的变化。当对动态关系建模时，传统方法（如概率图模型）的计算量会很大，因为他们有很多参数。
·检测器内部的动态关系。首先，一个geo-sensory时间序列通常由一种周期模式（如，图1c中的$S_1$），这种模式一直变化，并且地理上也有改变。其次，检测器记录经常有很大的振动，很快地减少前一个数值的影响。因此，如何选择一个时间间隔来预测也是一个问题。&lt;/li&gt;
&lt;li&gt;外部因素。
检测器数据也被周围的环境影响，比如气象（例如强风），几点（比如早晚高峰）还有土地使用情况等。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;为了解决这些挑战，我们提出了一个多级注意力网络（GeoMAN）来预测一个检测器未来几个小时的数值。我们的贡献有三点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;多级注意力机制&lt;/em&gt;。我们研发了一个多级注意力机制对动态时空关系建模。具体来说，第一级，我们提出了一个新的注意力机制，由局部空间注意力和全局空间注意力组成，用来捕获不同检测器时间序列间的复杂空间关系。第二级，时间注意力对一个时间序列中的动态时间关系进行建模。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;外部因素融合模型&lt;/em&gt;。我们设计了一个通用融合模型融合不同领域的外部因素。学习到的隐含表示会输入至多级注意力网络中来提升这些外部因素的重要性。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;真实的评价&lt;/em&gt;。我们基于两个真实的数据集评估我们的方法。大量的实验证明了我们的方法相比于baseline的优越性。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="2-preliminary"&gt;2 Preliminary
&lt;/h1&gt;&lt;h2 id="21-notations"&gt;2.1 Notations
&lt;/h2&gt;&lt;p&gt;假设，有 $N_g$ 个检测器，每个都生成 $N_l$ 种时间序列。我们指定其中的一个为 &lt;em&gt;target series&lt;/em&gt; 来预测，其它序列作为特征。时间窗为 $T$，我们使用 $\mathbf{Y} = (\mathbf{y}^1, \mathbf{y}^2, &amp;hellip;, \mathbf{y}^{N_g}) \in \mathbb{R}^{N_g \times T}$ 来表示所有目标序列在过去 $T$ 个小时的监测值，其中 $\mathbf{y}^i \in \mathbb{R}^T$ 属于监测器 $i$。我们使用 $\mathbf{X}^i = (\mathbf{x}^{i, 1}, \mathbf{x}^{i, 2}, &amp;hellip;, \mathbf{x}^{i, N_l})^{\rm T} = (\mathbf{x}^i_1, \mathbf{x}^i_2, &amp;hellip;, \mathbf{x}^i_T) \in \mathbb{R}^{N_l \times T}$ 作为检测器 $i$ 的局部特征，其中 $\mathbf{x}^{i,k} \in \mathbb{R}^T$ 是这个检测器的第 $k$ 个时间序列，$\mathbf{x}^i_t = (x^{i,1}_t, x^{i,2}_t, &amp;hellip;, x^{i,N_l}_t)^{\rm T} \in \mathbb{R}^{N_l}$ 表示检测器 $i$ 在时间 $t$ 的所有时间序列的值。除了检测器 $i$ 的局部特征，由于不同检测器间的空间关系，其他的检测器会共享大量对于预测有用的信息。为了这个目的，我们将每个检测器的局部特征融合到集合 $\mathcal{X}^i = \lbrace \mathbf{X}^1, \mathbf{X}^2, &amp;hellip;, \mathbf{X}^{N_g}\rbrace$ 中作为检测器 $i$ 的全局特征。&lt;/p&gt;
&lt;h2 id="22-problem-statement"&gt;2.2 Problem Statement
&lt;/h2&gt;&lt;p&gt;给定每个检测器之前的值和外部因素，预测检测器 $i$ 在未来 $\tau$ 个小时的值，表示为 $\hat{\mathbf{y}}^i = (\hat{y}^i_{T+1}, \hat{y}^i_{T+2}, &amp;hellip;, \hat{y}^i_{T+\tau})^{\rm T} \in \mathbb{R}^{\tau}$.&lt;/p&gt;
&lt;h1 id="3-multi-level-attention-networks"&gt;3 Multi-level Attention Networks
&lt;/h1&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/geoman-multi-level-attention-networks-for-geo-sensory-time-series-prediction/Fig2.PNG"
loading="lazy"
alt="Fig2"
&gt;&lt;/p&gt;
&lt;p&gt;图 2 展示了我们方法的框架。基于编码解码架构[Cho et al., 2014b]，我们用两个分开的 LSTM，一个对输入序列编码，也就是对历史的 geo-sensory 时间序列，另一个来预测输出的序列 $\hat{y}^i$。更具体的讲，我们的模型 GeoMAN 有两个主要部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;多级注意力机制。包含一个带有两类空间注意力机制的编码器和一个带有时间注意力的解码器。在编码器，我们开发了两种不同的注意力机制，局部空间注意力和全局空间注意力，如图 2 所示，这两种注意力机制通过前几步编码器的隐藏状态、前几步检测器的值和空间信息（检测器网络），可以在每个时间步上捕获检测器间的复杂关系。在解码器，我们使用了一个时间注意力机制来自适应地选择之前的时间段来做预测。&lt;/li&gt;
&lt;li&gt;外部因素融合。这个模块用来处理外部因素的影响，输出会作为解码器的一部分输入。这里，我们使用 $h_t \in \mathbb{R}^m$ 和 $s_t \in \mathbb{R}^m$ 来表示编码器在时间 $t$ 的隐藏状态和细胞状态。$d_t \in \mathbb{R}^n$ 和 $s&amp;rsquo; \in \mathbb{R}^n$ 表示解码器的隐藏状态和细胞状态。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="31-spatial-attention"&gt;3.1 Spatial Attention
&lt;/h2&gt;&lt;h3 id="local-spatial-attention"&gt;Local Spatial Attention
&lt;/h3&gt;&lt;p&gt;我们先介绍空间局部注意力机制。对应一个监测器，在它的局部时间序列上有复杂的关联性。举个例子，一个空气质量检测站会记录不同的时间序列如 PM2.5，NO 和 SO2。事实上，PM2.5 的浓度通常被其他时间序列影响，包括其他的空气污染物和局部空气质量 [Wang et al., 2005]。为了解决这个问题，给定第 $i$ 个检测器第 $k$ 个局部特征向量 $\mathbf{x}^{i,k}$，我们使用注意力机制自适应地捕获目标序列和每个局部特征间的动态关系：&lt;/p&gt;
$$\tag{1}
e^k\_t = \mathbf{v}^T\_l \text{tanh} (\mathbf{W}\_l [\mathbf{h}\_{t-1};\mathbf{s}\_{t-1}] + \mathbf{U}\_l \mathbf{x}^{i,k} + \mathbf{b}\_l)
$$$$\tag{2}
\alpha^k\_t = \frac{\text{exp}(e^k\_t)}{\sum^{N\_l}\_{j=1}\text{exp}(e^j\_t)}.
$$&lt;p&gt;其中 $[\cdot;\cdot]$ 是拼接操作（论文这里写的是 concentration，我怎么觉得是concatenation呢。。。）。$\mathbf{v}_l, \mathbf{b}_l \in \mathbb{R}^T, \mathbf{W}_l \in \mathbb{R}^{T \times 2m}, \mathbf{U}_l \in \mathbb{R}^{T \times T}$ 是参数。局部特征的注意力权重通过编码器中输入的局部特征和历史状态共同决定。这个注意力分数语义上表示每个局部特征的重要性，局部空间注意力在时间步 $t$ 的输出向量通过下式计算：&lt;/p&gt;
$$\tag{3}
\tilde{\mathbf{x}}^{local}\_t = (\alpha^1\_t x^{i,1}\_t, \alpha^2\_t x^{i,2}\_t, \dots, \alpha^{N\_l}\_t x^{i,N\_l}\_t)^{\rm T}.
$$&lt;h3 id="global-spatial-attention"&gt;Global Spatial Attention
&lt;/h3&gt;&lt;p&gt;对于一个监测器记录的目标时间序列，其他监测器是时间序列对其有直接影响。然而，影响权重是高度动态地，随时间变化。因为可能有很多不相关的序列，直接使用各种时间序列作为编码器的输入来捕获不同监测器之间的关系会导致很高的计算开销并且降低模型的能力。而且这样的影响权重受其他监测器的局部条件影响。举个例子，当强风从一个遥远地地方吹过来，这个区域的空气质量回比之前受影响的多。受这个现象的启发，我们开发了一个新的注意力机制捕获不同监测器间的动态关系。给定第 $i$ 个监测器作为我们的预测目标，另一个监测器 $l$，我们计算他们之间的注意力分数如下：&lt;/p&gt;
$$
g^l\_t = \mathbf{v}^{\rm T}\_g \text{tanh} (\mathbf{W}\_g [\mathbf{h}\_{t-1}; \mathbf{s}\_{t-1}] + \mathbf{U}\_g \mathbf{y}^l + \mathbf{W}'\_g \mathbf{X}^l \mathbf{u}\_g + \mathbf{b}\_g),
$$&lt;p&gt;其中 $\mathbf{v}_g, \mathbf{u}_g, \mathbf{b}_g \in \mathbb{R}^T, \mathbf{W}_g \in \mathbb{R}^{T \times 2m}, \mathbf{U}_g \in \mathbb{R}^{T \times T}, \mathbf{W}&amp;rsquo;_g \in \mathbb{R}^{T \times N_l}$ 是参数。通过考虑目标序列和其他检测器的局部特征，这个注意力机制可以自适应地选择相关的监测器来做预测。同时，通过考虑编码器内前一隐藏状态和细胞状态，历史信息会跨时间流动。&lt;/p&gt;
&lt;p&gt;注意，空间因素也会对不同监测器之间的关系做出贡献。一般来说，geo-sensors 通过一个明确的或隐含的网络连接起来。这里，我们使用一个矩阵 $\mathbf{P} \in \mathbb{R}^{N_g \times N_g}$ 来衡量地理空间相似度（如地理距离的倒数），$P_{i,j}$ 表示监测器 $i$ 和 $j$ 之间的相似度。不同于注意力权重，地理相似度可以看作是先验知识。特别的说，如果 $N_g$ 很大，一个方法是使用最近邻或最相近的一组而不是所有的监测器。之后，我们使用一个 softmax 函数，确定所有的注意力权重之和为1，两个方法共同考虑地理相似度如下：&lt;/p&gt;
$$\tag{4}
\beta^l\_t = \frac{\text{exp}((1-\lambda)g^l\_t + \lambda P\_{i,l})}{\sum^{N\_g}\_{j=1} \text{exp}((1-\lambda)g^j\_t + \lambda P\_{i,j})},
$$&lt;p&gt;其中，$\lambda$ 是一个可调的超惨。如果 $\lambda$ 大，这项会强制注意力权重等于地理相似度。全局注意力的输出向量计算如下：&lt;/p&gt;
$$\tag{5}
\tilde{\mathbf{x}}^{global}\_t = (\beta^1\_t y^1\_t, \beta^2\_t y^2\_t, \dots, \beta^{N\_g}\_t y^{N\_g}\_t)^{\rm T}.
$$&lt;h2 id="32-temporal-attention"&gt;3.2 Temporal Attention
&lt;/h2&gt;&lt;p&gt;因为编码解码结构会随着长度增长会很快的降低性能，一个重要的扩展是增加时间注意力机制，可以自适应地选择编码器相关的隐藏状态来生成输出序列，即模型对目标序列中不同时间间隔的动态时间关系建模。特别来说，为了计算每个输出时间 $t&amp;rsquo;$ 对编码器每个隐藏状态的的注意力向量，我们定义：&lt;/p&gt;
$$\tag{6}
u^o\_{t'} = \mathbf{v}^{\rm T}\_d \text{tanh} (\mathbf{W}'\_d [\mathbf{d}\_{t'-1}; \mathbf{s}'\_{t'-1}] + \mathbf{W}\_d \mathbf{h}\_o + \mathbf{b}\_d),
$$$$\tag{7}
\gamma^o\_{t'} = \frac{\text{exp} (u^o\_{t'})}{\sum^T\_{o=1} \gamma^o\_{t'} \mathbf{h}\_o},
$$$$\tag{8}
\mathbf{c}\_{t'} = \sum^T\_{o=1} \gamma^o\_{t'} \mathbf{h}\_o,
$$&lt;h2 id="33-external-factor-fusion"&gt;3.3 External Factor Fusion
&lt;/h2&gt;&lt;p&gt;Geo-sensory 时间序列和空间因素有强烈的相关性，如 POI 和监测器网络。这些因素一起表示一个区域的功能。而且还有很多时间因素影响监测器的数值，如气象或时间。受之前工作的启发 [Liang et al., 2017; Wang et al., 2018] 专注时空应用中的外部因素的影响，我们设计了一个简单有效的组件来处理这些因素。&lt;/p&gt;
&lt;p&gt;如图 2 所示，我们先将包含时间、气象特征的时间因素和表示目标监测器的监测器ID融合。因为未来的天气条件未知，我们使用天气预报来提升性能。这些因素的大部分都是离散特征，不能直接放到神经网络里面，我们通过将离散特征分开放入不同的嵌入层，将离散特征转换为低维向量。根据空间因素，我们使用不同 POI 类型的密度作为特征。因为监测器的属性依赖实际情况，我们只使用网络的结构特征，如邻居数和交集等。最后，我们将获得的嵌入向量和空间特征拼接作为这个模块的输出，表示为 $\mathbf{ex}_{t&amp;rsquo;} \in \mathbb{R}^{N_e}$，其中 $t&amp;rsquo;$ 是解码器中未来的时间步。&lt;/p&gt;
&lt;h2 id="34-encoder-decoder--model-training"&gt;3.4 Encoder-decoder &amp;amp; Model Training
&lt;/h2&gt;&lt;p&gt;编码器中，我们简单地从局部空间注意力和全局空间注意力聚合输出：&lt;/p&gt;
$$\tag{9}
\tilde{\mathbf{x}}\_t = [\tilde{\mathbf{x}}^{local}\_t; \tilde{\mathbf{x}}^{global}\_t],
$$&lt;p&gt;其中 $\tilde{\mathbf{x}}_t \in \mathbb{R}^{N_l + N_g}$。我们将拼接 $\tilde{\mathbf{x}}_t$ 作为编码器的新输入，使用 $\mathbf{h}_t = f_e(\mathbf{h}_{t-1}, \tilde{\mathbf{x}}_t)$ 更新时间 $t$ 的隐藏状态，$f_e$ 是一个 LSTM 单元。&lt;/p&gt;
&lt;p&gt;解码器中，一旦我们获得了时间 $t&amp;rsquo;$ 的上下文向量 $\mathbf{c}_{t&amp;rsquo;}$ 的带权和，我们将他与外部因素融合模块的输出 $\mathbf{ex}_{t&amp;rsquo;}$ 还有解码器的最后一个输出 $\hat{y}^i_{t&amp;rsquo;-1}$ 融合，用 $\mathbf{d}_{t&amp;rsquo;} = f_d (\mathbf{d}_{t&amp;rsquo;-1}, [\hat{y}^i_{t&amp;rsquo;-1}; \mathbf{ex}_{t&amp;rsquo;}; \mathbf{c}_{t&amp;rsquo;}])$ 更新解码器的隐藏状态，$f_d$ 是解码器中使用的 LSTM 单元。然后，我们讲上下文向量 $\mathbf{c}_{t&amp;rsquo;}$ 和隐藏状态 $\mathbf{d}_{t&amp;rsquo;}$ 拼接，得到新的隐藏状态，然后做最后的预测：&lt;/p&gt;
$$\tag{10}
\hat{y}^i\_{t'} = \mathbf{v}^{\rm T}\_y (\mathbf{W}\_m [\mathbf{c}\_{t'}; \mathbf{b}\_{t'}] + \mathbf{b}\_m) + b\_y,
$$&lt;p&gt;其中，$\mathbf{W}_m \in \mathbb{R}^{n \times (m + n))}$ 和 $\mathbf{b}_m \in \mathbb{R}^n$ 将 $[\mathbf{c}_{t&amp;rsquo;}; \mathbf{d}_{t&amp;rsquo;}] \in \mathbb{R}^{m + n}$ 映射到解码器隐藏状态的空间。最后，我们用一个线性变换生成最终结果。&lt;/p&gt;
&lt;p&gt;因为我们的方法是平滑且可微的，可以通过反向传播训练。在训练时，我们使用 Adam，最小化 MSE。&lt;/p&gt;
$$\tag{11}
\mathcal{L}(\theta) = \Vert \hat{\mathbf{y}}^i - \mathbf{y}^i \Vert^2\_2,
$$&lt;h1 id="4-experiments"&gt;4 Experiments
&lt;/h1&gt;&lt;h2 id="41-settings"&gt;4.1 Settings
&lt;/h2&gt;&lt;h3 id="datasets"&gt;Datasets
&lt;/h3&gt;&lt;p&gt;我们在两个数据集中开展了实验，每个数据集包含三个子集：气象数据、POI、监测器网络数据。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;水质数据集：中国东南的一个城市的供水系统中的监测器提供了长达三年的每5分钟一个记录的数据，包含了残余氯(RC)、浑浊度和PH值等。我们将 RC 作为目标时间序列，因为它在环境科学中作为常用的水质指标。一共有 14 个监测器，监测 10 个指标，它们之间通过管道网络相连。我们使用 Liu et al., 2016a 提出的指标作为这个数据集的相似度矩阵。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;空气质量：从一个公开数据集抓取的，这个数据集包含不同污染物的浓度，还有气象数据，北京地区一共 35 个监测器。主要污染物是 PM2.5，因此我们将它作为目标。我们只使用空间距离的倒数表示两个监测器之间的相似度。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/geoman-multi-level-attention-networks-for-geo-sensory-time-series-prediction/Table1.JPG"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;
&lt;p&gt;对于水质数据集，我们将数据分成了不重叠的训练集、验证集和测试集，去年的前一半作为验证机，去年的后半段作为测试集。可惜的是，我们在第二个数据集上没能获得很多的数据，因此我们使用了8：1：1的比例划分。&lt;/p&gt;
&lt;h3 id="evaluation-metrics"&gt;Evaluation Metrics
&lt;/h3&gt;&lt;p&gt;我们使用多个标准评价模型，RMSE 和 MAE。&lt;/p&gt;
&lt;h3 id="hyperparameters"&gt;Hyperparameters
&lt;/h3&gt;&lt;p&gt;我们令 $\tau = 6$，做短期预测。在训练阶段，batch size 256，学习率 0.001。外部因素融合模块，我使用 $\mathbb{R}^6$ 嵌入监测器 ID，时间特征 $\mathbb{R}^10$。我们的模型一共 4 个超参数，$\lambda$ 根据经验设置，从 0.1 到 0.5。对于窗口长度 $T$，我们设为 $T \in \lbrace 6, 12, 24, 36, 48 \rbrace$。为了简单，我们将编码器和解码器采用同样的隐藏维数，网格搜索 $\lbrace 32, 64, 128, 256\rbrace$。我们堆叠 LSTM 来提高性能，层数记为 $q$。验证集上得到的最好参数是 $q = 2, m = n = 64, \lambda = 2$。&lt;/p&gt;
&lt;h2 id="42-baselines"&gt;4.2 Baselines
&lt;/h2&gt;&lt;p&gt;ARIMA, VAR, GBRT, FFA, stMTMVL, stDNN, LSTM, Seq2seq, DA-RNN。&lt;/p&gt;
&lt;p&gt;对于 ARIMA，我们用前六个小时的数据作为输入。stMTMVL 和 FFA，我们使用和作者一样的设置。和 GeoMAN 类似，我们使用前 $T \in \lbrace 6, 12, 24, 36, 48\rbrace$ 个小时的数据作为其他模型的输入。最后，我们测试了不同的超参数，得到了每个模型的最好效果。&lt;/p&gt;
&lt;h2 id="43-model-comparsion"&gt;4.3 Model Comparsion
&lt;/h2&gt;&lt;p&gt;我们在两个数据集上比较了模型和 baselines。为了公平，我们在表 2 展示了每个方法的最好性能。&lt;/p&gt;
&lt;p&gt;我们的方法在水质预测上得到了最好的性能。比 state-of-the-art 的 DA-RNN 在两个指标上分别提升了 14.2% 和 13.5%。因为 RC 浓度有一个确定的周期模式，stDNN 和 基于 RNN 的模型比 stMTMVL 和 FFA 获得了更好的效果，因为他们能捕获更长的时间依赖。对比 LSTM 智能预测一个未来的时间步，GeoMAN 和 Seq2seq 由于解码器的存在有很大的提升。GBRT 比大部分方法也要好，体现了集成学习的优势。&lt;/p&gt;
&lt;p&gt;对比数据相对稳定的水质数据集，PM2.5 的浓度有些时候震荡得很厉害，使得很难预测。表 2 展示了北京的空气质量数据集上一个全面的对比。可以看到我们的模型有最好的效果。我们主要讨论下 MAE。我们的方法比这些方法的 MAE 相对低 7.2% 和 63.5%，展示出了比其他方法更好的泛化效果。另一个有趣的现象是 stMTMVL 在水质预测上表现很好，在空气质量上&lt;/p&gt;</description></item><item><title>Semi-Supervised Classification With Graph Convolutional Networks</title><link>https://davidham3.github.io/blog/p/semi-supervised-classification-with-graph-convolutional-networks/</link><pubDate>Mon, 02 Jul 2018 20:04:20 +0000</pubDate><guid>https://davidham3.github.io/blog/p/semi-supervised-classification-with-graph-convolutional-networks/</guid><description>&lt;p&gt;ICLR 2017。图卷积中谱图领域理论上很重要的一篇论文，提升了图卷积的性能，使用切比雪夫多项式的1阶近似完成了高效的图卷积架构。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1609.02907v4" target="_blank" rel="noopener"
&gt;Semi-Supervised Classification with Graph Convolutional Networks. Kipf &amp;amp; Welling 2017&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="摘要"&gt;摘要
&lt;/h1&gt;&lt;p&gt;我们提出了一种在图结构数据上的半监督可扩展学习方法，基于高效的图卷积变体。契机是通过一个谱图卷积的局部一阶近似得到的我们的图卷积结构。我们的模型与图的边数呈线性关系，学习到的隐藏层可以对图的顶点和局部图结构同时进行编码。在引文网络和一个知识图谱数据集上的大量实验结果表明我们的方法比相关方法好很多。&lt;/p&gt;
&lt;h1 id="引言"&gt;引言
&lt;/h1&gt;&lt;p&gt;我们考虑一个对图顶点进行分类的问题，只有一小部分的顶点有标签。这个问题可以通过基于图的半监督学习任务建模，通过某些明确的图正则化方法(Zhu et al., 2003; Zhou et al., 2004; Belkin et al., 2006; Weston et al., 2012)可以平滑标签信息，举个例子，通过在loss function使用一个图拉普拉斯正则项：
&lt;/p&gt;
$$\tag{1} \mathcal{L} = \mathcal{L\_0} + \lambda \mathcal{L\_{reg}}, \rm with \ \mathcal{L\_{reg}} = \sum\_{i.j}A\_{ij} \Vert f(X\_i) - f(X\_j) \Vert^2 = f(X)^T \Delta f(X)$$&lt;p&gt;
其中，$\mathcal{L_0}$表示对于图的标签部分的监督损失，$f(\cdot)$可以是一个神经网络类的可微分函数，$\lambda$是权重向量，$X$是定点特征向量$X_i$的矩阵。$N$个顶点$v_i \in \mathcal{V}$，边$(v_i, v_j) \in \varepsilon$，邻接矩阵$A \in \mathbb{R}^{N \times N}$（二值的或者带权重的），还有一个度矩阵$D_{ii} = \sum_jA_{ij}$。式1依赖于“图中相连的顶点更有可能具有相同的标记”这一假设。然而，这个假设，可能会限制模型的能力，因为图的边并不是必须要编码成相似的，而是要包含更多的信息。
在我们的研究中，我们将图结构直接通过一个神经网络模型$f(X, A)$进行编码，并且在监督的目标$\mathcal{L_0}$下对所有有标记的顶点进行训练，因此避免了损失函数中刻意的对图进行正则化。在图的邻接矩阵上使用$f(\cdot)$可以使模型从监督损失$\mathcal{L_0}$中分布梯度信息，并且能够从有标记和没有标记的顶点上学习到他们的表示。
我们的贡献有两点，首先，我们引入了一个简单的，表现很好的针对神经网络的对层传播规则，其中，这个神经网络是直接应用到图上的，并且展示了这个规则是如何通过谱图卷积的一阶近似启发得到的。其次，我们展示了这种形式的基于图的神经网络可以用于对图中的顶点进行更快更可扩展的半监督分类任务。在大量数据集上的实验表明我们的模型在分类精度和效率上比当前在半监督学习中的先进算法要好。&lt;/p&gt;
&lt;h1 id="图上的快速近似卷积"&gt;图上的快速近似卷积
&lt;/h1&gt;&lt;p&gt;在这部分，我们会讨论一个特殊的基于图的神经网络$f(X, A)$。考虑一个多层图卷积网络(GCN)，通过以下的传播规则：
&lt;/p&gt;
$$\tag{2} H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)})$$&lt;p&gt;
其中，$\tilde{A} = A + I_N$是无向图$\mathcal{G}$加了自连接的邻接矩阵。$I_N$是单位阵，$\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$，$W^{(l)}$是一个针对层训练的权重矩阵。$\sigma(\cdot)$表示一个激活函数，比如$\rm ReLU(\cdot) = \rm max(0, \cdot)$，$H^{(l)} \in \mathbb{R}^{N \times D}$是第$l$层的激活矩阵；$H^{(0)} = X$。接下来我们将展示通过图上的一阶近似局部谱滤波器(Hammond et al., 2011; Defferrard et al., 2016)的传播过程。&lt;/p&gt;
&lt;h2 id="谱图卷积-spectral-graph-convolutions"&gt;谱图卷积 spectral graph convolutions
&lt;/h2&gt;&lt;p&gt;定义图上的谱图卷积为信号$x \in \mathbb{R}^N$和一个滤波器$g_\theta = \rm diag(\theta)$，参数是傅里叶域中的$\theta \in \mathbb{R}^N$，也就是：
&lt;/p&gt;
$$\tag{3} g\_\theta \ast x = U g\_\theta U^T x$$&lt;p&gt;
其中$U$是归一化的拉普拉斯矩阵$L = I_N - D^{-\frac{1}{2}}AD^{-\frac{1}{2}} = U \Lambda U^T$的特征向量组成的矩阵，$\Lambda$是特征值组成的对角阵，$U^Tx$是$x$的图傅里叶变换。可以认为$g_\theta$是关于$L$的特征值的函数，也就是说$g_\theta(\Lambda)$。式3的计算量很大，因为特征向量矩阵$U$的乘法的时间复杂度是$O(N^2)$。此外，对于大尺度的图来说，对$L$进行特征值分解是计算量非常大的一件事。为了避开这个问题，Hammond et al.(2001)建议使用$K$阶切比雪夫多项式$T_k(x)$来近似$g_\theta(\Lambda)$：
&lt;/p&gt;
$$\tag{4} g\_\theta' \approx \sum^K\_{k=0} \theta'\_k T\_k(\tilde{\Lambda})$$&lt;p&gt;
其中，$\tilde{\Lambda} = \frac{2}{\lambda_{max}} \Lambda - I_N$。$\lambda_{max}$表示$L$的最大特征值。$\theta&amp;rsquo; \in \mathbb{R}^K$是切比雪夫系数向量。切比雪夫多项式的定义是：$T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x)$，$T_0(x) = 1$，$T_1(x) = x$。
回到我们对于一个信号$x$和一个滤波器$g_\theta&amp;rsquo;$的卷积的定义：
&lt;/p&gt;
$$\tag{5} g\_\theta' \ast x \approx \sum^K\_{k=0} \theta'\_k T\_k(\tilde{L}) x$$&lt;p&gt;
其中，$\tilde{L} = \frac{2}{\lambda x_{max}} L - I_N$；注意$(U \Lambda U^T)^k = U \Lambda^k U^T$。这个表达式目前是$K$阶局部的，因为这个表达式是拉普拉斯矩阵的$K$阶多项式，也就是说从中心节点向外最多走$K$步，$K$阶邻居。式5的时间复杂度是$O(\vert \varepsilon \vert)$，也就是和边数呈线性关系。Defferrard et al.(2016)使用这个$K$阶局部卷积定义了在图上的卷积神经网络。&lt;/p&gt;
&lt;h2 id="按层的线性模型-layer-wise-linear-model"&gt;按层的线性模型 layer-wise linear model
&lt;/h2&gt;&lt;p&gt;一个基于图卷积的神经网络模型可以通过堆叠式5这样的多个卷积层来实现，每层后面加一个非线性激活即可。现在假设$K=1$，也就是对$L$线性的一个函数，因此得到一个在图拉普拉斯谱(graph Laplacian spectrum)上的线性函数。这样，我们仍然能通过堆叠多个这样的层获得一个卷积函数，但是我们就不会再受限于明显的参数限制，比如切比雪夫多项式。我们直觉上期望这样一个模型可以减轻在度分布很广泛的图上局部图结构模型过拟合的问题，如社交网络、引文网络、知识图谱和其他很多真实数据集。此外，这个公式可以让我们搭建更深的网络，一个可以提升模型学习能力的实例是He et al., 2016。
在GCN的线性公式中，我们让$\lambda_{max}$近似等于2，因为我们期望神经网络参数可以在训练中适应这个变化。在这个近似下，式5可以简化为：
&lt;/p&gt;
$$\tag{6} g\_\theta' \ast x \approx \theta'\_0x + \theta'\_1 (L - I\_N)x = \theta'\_0x - \theta'\_1 D^{-\frac{1}{2}} A D^{-\frac{1}{2}} x$$&lt;p&gt;
两个参数$\theta&amp;rsquo;_0$和$\theta&amp;rsquo;_1$。滤波器参数可以在整个图上共享。连续的使用这种形式的卷积可以有效的对一个顶点的$k$阶邻居进行卷积，$k$是连续的卷积操作或模型中卷积层的个数。
实际上，通过限制参数的数量可以进一步的解决过拟合的问题，并且最小化每层的操作数量（比如矩阵乘法）。这时的我们得到了下面的式子：
&lt;/p&gt;
$$\tag{7} g\_\theta \ast x \approx \theta(I\_N + D^{-\frac{1}{2}} A D^{-\frac{1}{2}}) x$$&lt;p&gt;
只有一个参数$\theta = \theta&amp;rsquo;_0 = - \theta&amp;rsquo;_1$。注意，$I_N + D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$现在的特征值在$[0, 2]$之间。在深层模型中重复应用这个操作会导致数值不稳定和梯度爆炸、消失的现象。为了减轻这个问题，我们引入了如下的重新正则化技巧：$I_N + D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \to \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$，$\tilde{A} = A + I_N$，$\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$。
我们可以将这个定义泛化到一个有着$C$个通道的信号$X \in \mathbb{R}^{N \times C}$上，也就是每个顶点都有一个$C$维的特征向量，对于$F$个滤波器或$F$个feature map的卷积如下：
&lt;/p&gt;
$$\tag{8} Z = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} X \Theta$$&lt;p&gt;
其中$\Theta \in \mathbb{R}^{C \times F}$是一个滤波器的参数矩阵，$Z \in \mathbb{R}^{N \times F}$是卷积的信号矩阵。卷积操作的时间复杂度是$O(\vert \varepsilon \vert F C)$，因为$\tilde{A} X$可以被实现成一个稀疏矩阵和一个稠密矩阵的乘积。&lt;/p&gt;
&lt;h1 id="半监督顶点分类"&gt;半监督顶点分类
&lt;/h1&gt;&lt;p&gt;介绍过这个简单、灵活的可以在图上传播信息的模型$f(X, A)$后，我们回到半监督顶点分类的问题上。如介绍里面所说的，我们可以减轻在基于图的半监督学习任务中的假设，通过在图结构上的数据$X$和邻接矩阵$A$上使用模型$f(X, A)$。我们期望这个设置可以在邻接矩阵表达出数据$X$没有的信息的这种情况时表现的很好，比如引文网络中，引用的关系或是知识图谱中的关系。整个模型是一个多层的GCN，如图1所示。
&lt;img src="https://davidham3.github.io/blog/images/semi-supervised-classification-with-graph-convolutional-networks/Fig1.PNG"
loading="lazy"
alt="Fig1"
&gt;&lt;/p&gt;
&lt;h2 id="例子"&gt;例子
&lt;/h2&gt;&lt;p&gt;我们考虑一个两层GCN对图中的顶点进行半监督分类，邻接矩阵是对称的。我们首先在预处理中计算$\hat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$。前向传播模型的形式如下：
&lt;/p&gt;
$$\tag{9} Z = f(X, A) = \rm softmax( \hat{A} \ \rm ReLU( \hat{A}XW^{(0)})W^{(1)})$$&lt;p&gt;
这里，$W^{(0)} \in \mathbb{R}^{C \times H}$是输入到隐藏层的权重矩阵，有$H$个feature map。$W^{(1)} \in \mathbb{R}^{H \times F}$是隐藏层到输出的权重矩阵。softmax激活函数定义为$\rm softmax(x_i) = \frac{1}{\mathcal{Z}} \exp(x_i)$，$\mathcal{Z} = \sum_i \exp(x_i)$，按行使用。对于半监督多类别分类，我们使用交叉熵来衡量所有标记样本的误差：
&lt;/p&gt;
$$\tag{10} \mathcal{L} = - \sum\_{l \in \mathcal{Y}\_L} \sum^F\_{f = 1} Y\_{lf} \ln(Z\_{lf})$$&lt;p&gt;
其中，$\mathcal{Y}_L$是有标签的顶点的下标集合。
神经网络权重$W^{(0)}$和$W^{(1)}$使用梯度下降训练。我们每次训练的时候都是用全部的训练集来做梯度下降，只要数据集能放到内存中。对$A$进行稀疏矩阵的表示，内存的使用量是$O(\vert \varepsilon \vert)$。训练过程中使用了dropout增加随机性。我们将在未来的工作使用mini-batch随机梯度下降。&lt;/p&gt;
&lt;h2 id="实现"&gt;实现
&lt;/h2&gt;&lt;p&gt;我们使用Tensorflow实现了基于GPU的，稀疏稠密矩阵乘法形式。式9的时间复杂度是$O(\vert \varepsilon \vert C H F)$。&lt;/p&gt;</description></item><item><title>Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title><link>https://davidham3.github.io/blog/p/graph-convolutional-neural-networks-for-web-scale-recommender-systems/</link><pubDate>Sun, 17 Jun 2018 21:24:48 +0000</pubDate><guid>https://davidham3.github.io/blog/p/graph-convolutional-neural-networks-for-web-scale-recommender-systems/</guid><description>&lt;p&gt;KDD 2018。使用图卷积对顶点进行表示，学习顶点的 embedding ，通过卷积将该顶点的邻居信息融入到向量中。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1806.01973v1" target="_blank" rel="noopener"
&gt;Graph Convolutional Neural Networks for Web-Scale Recommender Systems&lt;/a&gt;。&lt;/p&gt;
&lt;h1 id="abstract"&gt;ABSTRACT
&lt;/h1&gt;&lt;p&gt;最近在图数据上的深度神经网络在推荐系统上表现的很好。然而，把这些算法应用到数十亿的物品和数百亿的用户上仍然是个挑战。&lt;/p&gt;
&lt;p&gt;我们提出了一种在 Pinterest 上的大规模深度推荐引擎，开发了一种高效的图卷积算法 PinSage，融合了随机游走和图卷积，来生成顶点（物品）的表示，同时整合了顶点信息和图结构。对比之前的 GCN 方法，我们研究的模型基于高效的随机游走来结构化卷积操作，而且还设计了一个新型的训练策略，这个策略依赖于 harder-and-harder 训练样本，来提高模型的鲁棒性和收敛能力。&lt;/p&gt;
&lt;p&gt;我们的 PinSage 在 Pinterest 上面的75亿个样本上进行训练，图上有30亿个顶点表示 &lt;em&gt;pins&lt;/em&gt; 和 &lt;em&gt;boards&lt;/em&gt;，180亿条边。根据离线指标、用户研究和 A/B 测试，PinSage 生成了相比其他深度学习和基于图的方法更高质量的推荐结果。据我们所知，这是深度图表示目前规模最大的应用，并且为新一代基于图卷积结构的大规模推荐系统奠定了基础。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1 INTRODUCTION
&lt;/h1&gt;&lt;p&gt;深度学习方法在推荐系统中越来越重要，用来学习图像、文本、甚至是用户的有效的低维表示。使用深度学习学习到的表示可以用来补充、或是替换像协同过滤这样传统的推荐算法。这些表示很有用，因为他们可以在各种推荐任务中重复使用。举个例子，使用深度模型学习得到的物品的表示，可以用来做 “物品-物品” 推荐，也可以来按主题推荐（比如，歌单、或是 Feed流的内容）。&lt;/p&gt;
&lt;p&gt;近些年可以看到这个领域的很多重要的发展，尤其是新的可以学习图结构的深度学习方法的发展，是一些推荐应用的基础（比如在用户-物品网络上或社交网络上推荐）。&lt;/p&gt;
&lt;p&gt;在这些成功的深度学习框架中比较重要的是图卷积网络（GCN）。核心的原理是学习如何迭代地使用神经网络从局部图邻居中聚合特征信息（图1）。这里，一个简单的卷积操作从一步邻居中变换并聚合特征信息，并且通过堆叠多个这样的卷积，信息可以传播到图中很广的地方。不像纯基于内容的深度模型（如 RNN ），GCN 利用内容信息和图结构。基于 GCN 的模型的方法已经在无数推荐系统中形成了新的标准（参见[19]的综述）。然而，这些b enchmark 上面获得的提升，还没有被转换到真实环境的应用中去。&lt;/p&gt;
&lt;p&gt;主要挑战是要将训练和基于 GCN 的顶点表示在数十亿的顶点和数百亿的边的图中进行。扩展 GCN 很困难，因为很多在大数据环境中，很多基于这些 GCN 设计的假设都不成立了。比如，所有的基于 GCN 的推荐系统需要在训练时使用图的拉普拉斯矩阵，但是当顶点数很大的时候，这就不现实了，因为算不出来。&lt;/p&gt;
&lt;h1 id="3-method"&gt;3 METHOD
&lt;/h1&gt;&lt;p&gt;在这部分，我们将描述 PinSage 的结构和训练的技术细节，也会讲一下使用训练好的 PinSage 模型来高效地生成 embedding 的MapReduce pipeline。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/graph-convolutional-neural-networks-for-web-scale-recommender-systems/Fig1.PNG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;我们方法的计算关键在于局部图卷积的表示(notion)。我们使用多个卷积模块来聚合一个顶点局部的邻域特征信息（图1），来生成这个顶点的 embedding（比如一个物品）。每个模块学习如何从一个小的图邻域中聚合信息，并且通过堆叠多个这样的模块，我们的方法可以获得局部网络的拓扑结构信息。更重要的是，这些局部卷积模块的参数对所有的顶点来说是共享的，这使得我们的方法的参数的计算复杂度与输入的图的大小无关。&lt;/p&gt;
&lt;h1 id="31-problem-setup"&gt;3.1 Problem Setup
&lt;/h1&gt;&lt;p&gt;Pinterest 是一个内容挖掘应用，在这里用户与 &lt;em&gt;pins&lt;/em&gt; 进行交互，这些 &lt;em&gt;pins&lt;/em&gt; 是在线内容的可见标签（比如用户做饭时的食谱，或者他们想买的衣服）。用户用 &lt;em&gt;boards&lt;/em&gt; 将 &lt;em&gt;pins&lt;/em&gt; 组织起来，&lt;em&gt;boards&lt;/em&gt; 里面包含了 &lt;em&gt;pins&lt;/em&gt; 组成的集合，这些 &lt;em&gt;pins&lt;/em&gt; 在用户看来是主题相关的。Pinterest 组成的图包含了 20 亿的 &lt;em&gt;pins&lt;/em&gt;，10 亿的 &lt;em&gt;boards&lt;/em&gt;，超过 180 亿的边（也就是 &lt;em&gt;pins&lt;/em&gt; 对应 &lt;em&gt;boards&lt;/em&gt; 的关系）。&lt;/p&gt;
&lt;p&gt;我们的任务是生成可以用于推荐的高质量的 embedding 或 &lt;em&gt;pins&lt;/em&gt; 的表示（比如，使用最近邻来查找 &lt;em&gt;pin&lt;/em&gt; 的推荐，或是使用下游的再评分系统进行推荐）。为了学习这些 embedding，我们对 Pinterest 环境进行建模，得到一个二部图，顶点分为两个不相交的集合，$\mathcal{I}$ 表示 &lt;em&gt;pins&lt;/em&gt;，$\mathcal{C}$ 表示 &lt;em&gt;boards&lt;/em&gt;。当然，我们的方法是可以泛化到其他方面的，比如 $\mathcal{I}$ 看作是物品，$\mathcal{C}$ 看作是用户定义的环境或收藏品集合等。&lt;/p&gt;
&lt;p&gt;再来说说图结构，我们假设 &lt;em&gt;pins/items&lt;/em&gt; $u \in \mathcal{I}$ 与特征 $x_u \in \mathbb{R}^d$ 相关。通常来说，这些特征可能是物品的元数据或上下文信息，在 Pinterest 的例子中，&lt;em&gt;pins&lt;/em&gt; 是和富文本与图片特征相关的。我们的目标是利用这些输入特征，也利用二部图的图结构性质来生成高质量的 embedding。这些 embedding 可以用于推荐系统，通过最近邻查找来生成推荐，或是作为用评分来推荐的机器学习系统的特征。&lt;/p&gt;
&lt;p&gt;为了符号的简洁，我们使用 $\mathcal{V} = \mathcal{I} \cup \mathcal{C}$ 来表示图中的顶点集，没有特殊需要不区分 &lt;em&gt;pin&lt;/em&gt; 和 &lt;em&gt;board&lt;/em&gt; 顶点，一律使用 &lt;em&gt;node&lt;/em&gt; 来表示顶点。&lt;/p&gt;
&lt;h2 id="32-model-architecture"&gt;3.2 Model Architecture
&lt;/h2&gt;&lt;p&gt;我们使用局部卷积模块对顶点生成 embeddings。首先输入顶点的特征，然后学习神经网络，神经网络会变换并聚合整个图上的特征来计算顶点的 embeddings（图1）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Forward propagation algorithm.&lt;/strong&gt; 考虑对顶点 $u$ 生成 embedding $z_u$ 的任务，需要依赖顶点的输入特征和这个顶点周围的图结构。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/graph-convolutional-neural-networks-for-web-scale-recommender-systems/algo1.PNG"
loading="lazy"
alt="“Algorithm 1”"
&gt;&lt;/p&gt;
&lt;p&gt;我们的 PinSage 算法是一个局部卷积操作，我们可以通过这个局部卷积操作学到如何从 $u$ 的邻居聚合信息（图1）。这个步骤在算法1 CONVOLVE 中有所描述。从本质上来说，我们通过一个全连接神经网络对 $\forall{v} \in \mathcal{N}(u)$，也就是 $u$ 的邻居的表示 $z_v$ 进行了变换，之后在结果向量集合上用一个聚合/池化函数（例如：一个 element-wise mean 或是加权求和，表示为 $\gamma$）（Line 1）。这个聚合步骤生成了一个 $u$ 的邻居$\mathcal{N}(u)$ 的表示 $n_u$。之后我们将这个聚集邻居向量 $n_u$ 和 $u$ 的当前表示向量进行拼接后，输入到一个全连接神经网络做变换（Line 2）。通过实验我们发现使用拼接操作会获得比平均操作[21]好很多的结果。除此以外，第三行的 normalization 使训练更稳定，而且对近似最近邻搜索来说归一化的 embeddings 更高效（Section 3.5）。算法的输出是集成了 $u$ 自身和他的局部邻域信息的表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Importance-based neighborhoods.&lt;/strong&gt; 我们方法中的一个重要创新是如何定义的顶点邻居 $\mathcal{N}(u)$，也就是我们在算法1中是如何选择卷积的邻居集合。尽管之前的 GCN 方法简单地检验了 k-hop 邻居，在 PinSage 中我们定义了基于重要性的邻域，顶点 $u$ 的邻居定义为 $T$ 个顶点，这 $T$ 个顶点对 $u$ 是最有影响力的。具体来说，我们模拟了从顶点 $u$ 开始的随机游走，并且计算了通过随机游走&lt;a class="link" href="https://arxiv.org/abs/1711.07601" target="_blank" rel="noopener"
&gt;[14]&lt;/a&gt;对顶点的访问次数的 $L_1$ 归一化值。$u$ 的邻居因此定义为针对顶点 $u$ 来说 $T$ 个最高的归一化的访问数量的顶点。&lt;/p&gt;
&lt;p&gt;这个基于重要性的邻域定义的优点有两点。第一点是选择一个固定数量的邻居顶点来聚集可以在训练过程中控制内存开销[18]。第二，在算法1中聚集邻居的向量表示时可以考虑邻居的重要性。特别地，我们在算法1中实现的 $\gamma$ 是一个加权求均值的操作，权重就是 $L_1$ 归一化访问次数。我们将这个新的方法称为重要度池化(&lt;em&gt;importance pooling&lt;/em&gt;)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stacking convolutions.&lt;/strong&gt; 每次使用算法1的 CONVOLVE 操作都会得到一个顶点的新的表示，我们可以在每个顶点上堆叠卷积来获得更多表示顶点 $u$ 的局部邻域结构的信息。特别地，我们使用多层卷积，其中对第 $k$ 层卷积的输入依赖于 $k-1$ 层的输出（图1），最初的表示（&amp;ldquo;layer 0&amp;rdquo;）等价于顶点的输入特征。需要注意的是，算法1中的模型参数（$Q$, $q$, $W$ 和 $w$）在顶点间是共享的，但层与层之间不共享。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/graph-convolutional-neural-networks-for-web-scale-recommender-systems/algo2.PNG"
loading="lazy"
&gt;&lt;/p&gt;
&lt;p&gt;算法2详细描述了如何堆叠卷积操作，针对一个 minibatch 的顶点 $\mathcal{M}$ 生成 embeddings。首先计算每个顶点的邻居，然后使用 $K$ 个卷积迭代来生成目标顶点的 K 层表示。最后一层卷积层的输出之后会输入到一个全连接神经网络来生成最后的 embedding $z_u$，$\forall{u} \in \mathcal{M}$。&lt;/p&gt;
&lt;p&gt;模型需要学习的参数有：每个卷积层的权重和偏置（$Q^{(k)}$，$q^{(k)}$，$W^{(k)}$，$w^{(k)}$，$\forall{k} \in \lbrace 1,&amp;hellip;,K\rbrace $），还有最后的全连接网络中的参数 $G_1$，$G_2$，$g$。算法1的第一行的每层输出的维度（也就是 $Q$ 的列空间的维度）设为 $m$。为了简单起见，我们将所有卷积层（算法1的第三行的输出）的输出都设为同一个数，表示为 $d$。模型最后的输出（算法2第18行之后）也设为 $d$。&lt;/p&gt;
&lt;h2 id="33-model-training"&gt;3.3 Model Training
&lt;/h2&gt;&lt;p&gt;我们使用 max-margin ranking loss 来训练 PinSage。在这步，假设我们有了一组标记的物品对 $\mathcal{L}$，$(q,i) \in \mathcal{L}$ 认为是相关的，也就是当查询 $q$ 时，物品 $i$ 是一个好的推荐候选项。训练阶段的目标是优化 PinSage 的参数，使得物品对 $(q,i) \in \mathcal{L}$ 的 embedding 在标记集合中尽可能的接近。&lt;/p&gt;
&lt;p&gt;我们先来看看 margin-based loss function。首先我们来看看我们使用的可以高效地计算并且使 PinSage 快速收敛的一些技术，这些技术可以让我们训练包含数十亿级别的顶点的图，以及数十亿训练样本。最后，我们描述我们的 curriculum-training scheme，这个方法可以全方位的提升我们的推荐质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Loss function.&lt;/strong&gt; 为了训练模型的参数，我们使用了一个基于最大边界的损失函数。基本的思想是我们希望最大化正例之间的内积，也就是说，查询物品的 embedding 和对应的相关物品的 embedding 之间的内积。与此同时我们还想确保负例之间的内积，也就是查询物品的 embedding 和那些不相关物品 embedding 之间的内积要小于通过提前定义好的边界划分出的正例的内积。对于单个顶点对 embeddings $(z_q, z_i):(q, i) \in \mathcal{L}$ 的损失函数是：&lt;/p&gt;
$$
J\_{\mathcal{G}}(z\_qz\_i) = \mathbb{E}\_{n\_k \thicksim p\_n(q)}\max\lbrace 0, z\_q \cdot z\_{n\_k}-z\_q \cdot z\_i + \Delta\rbrace
$$&lt;p&gt;其中，$P_n(q)$ 表示物品 $q$ 的负样本分布，$\Delta$ 表示 margin 超参数。一会儿会讲负样本采样。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multi-GPU training with large minibatches.&lt;/strong&gt; 为了在训练中充分利用单台机器的多个 GPU，我们以一种 multi-tower 的方法运行前向和反向传播。我们首先将每个 minibatch（图1底部）分成相等大小的部分。每个 GPU 获得 minibatch 的一部分，使用同一组参数来运算。在反向传播之后，所有 GPU 上的针对每个参数的梯度进行汇集，然后使用一个同步的 SGD。由于训练需要极大数量的样本，我们在运行时使用了很大的 batch size，范围从 512 到 4096。&lt;/p&gt;
&lt;p&gt;我们使用与 Goyal et al.[16] 提出的相似的技术来确保快速收敛，而且在处理大 batch size 时训练的稳定和泛化精度。我们在第一轮训练的时候根据线性缩放原则使用一个 gradual warmup procedure，使学习率从小增大到一个峰值。之后学习率以指数级减小。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Producer-consumer minibatch construction.&lt;/strong&gt; 在训练的过程中，由于邻接表和特征矩阵有数十亿的顶点，所以放在了 CPU 内存中。然而，在训练 PinSage 的 CONVOLVE 步骤时，每个 GPU 需要处理邻居和顶点邻居的特征信息。从 GPU 访问 CPU 内存中的数据时会有很大的开销。为了解决这个问题，我们使用了一个 &lt;em&gt;re-indexing&lt;/em&gt; 的方法创建包含了顶点和他们的邻居的子图 $G&amp;rsquo; = (V&amp;rsquo;, E&amp;rsquo;)$，在当前的 minibatch 中会被加入到计算中。只包含当前 minibatch 计算的顶点特征信息的小的特征矩阵会被抽取出来，顺序与 $G&amp;rsquo;$ 中顶点的 index 一致。$G&amp;rsquo;$ 的邻接表和小的特征矩阵会在每个 minibatch 迭代时输入到 GPU 中，这样就没有了 GPU 和 CPU 间的通信开销了，极大的提高了 GPU 的利用率。&lt;/p&gt;
&lt;p&gt;训练过程改变了 CPU 和 GPU 的使用方式。模型计算是在 GPU，特征抽取、re-indexing、负样本采样是在 CPU 上运算的。使用 multi-tower 训练的 GPU 并行和 CPU 计算使用了 OpenMP[25]，我们设计了一个生产者消费者模式在当前迭代中使用 GPU 计算，在下一轮使用 CPU 计算，两者并行进行。差不多减少了一半的时间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sampling negative items.&lt;/strong&gt; 负样本采样在我们的损失函数中作为 edge likelihood[23] 的归一化系数的近似值。为了提升 batch size 较大时的训练效率，我们采样了 500 个负样本作为一组，每个 minibatch 的训练样本共同使用这一组。相比于对每个顶点在训练时都进行负样本采样，这极大地减少了每次训练时需要计算的 embeddings 的数量。从实验上来看，我们发现这两种方法在表现上没什么特别大的差异。&lt;/p&gt;
&lt;p&gt;在最简单的情况中，我们从整个样本集中使用均匀分布的抽样方式。然而，确保正例($(q, i)$)的内积大于 $q$ 和 500 个负样本中每个样本的内积是非常简单的，而且这样做不能提供给系统足够学习的分辨率。我们的推荐算法应该能从 200 亿个商品中找到对于物品 $q$ 来说最相关的 1000 个物品。换句话说，我们的模型应该能从超过 2 千万的物品中区分/辨别出 1 件物品。但是通过随机采样的 500 件物品，模型的分辨率只是 $\frac{1}{500}$。因此，如果我们从 200 亿物品中随机抽取 500 个物品，这些物品中的任意一个于当前这件查询的物品相关的几率都很小。因此，模型通过训练不能获得好的参数，同时也不能对相关的物品进行区分的概率很大。为了解决上述问题，对于每个正训练样本（物品对$(q, i)$），我们加入了&amp;quot;hard&amp;quot;负例，也就是那些与查询物品 $q$ 有某种关联的物品，但是又不与物品 $i$ 有关联。我们称这些样本为&amp;quot;hard negative items&amp;quot;。通过在图中根据他们对查询物品 $q$ 的个性化 PageRank 分数来生成[14]。排名在 2000-5000 的物品会被随机采样为 hard negative items。如图2所示，hard negative examples 相比于随机采样的负样本更相似于查询物品，因此对模型来说挑战是排名，迫使模型学会在一个好的粒度上分辨物品。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/graph-convolutional-neural-networks-for-web-scale-recommender-systems/Fig2.PNG"
loading="lazy"
&gt;&lt;/p&gt;
&lt;p&gt;使用 hard negative items 会让能使模型收敛的训练轮数翻倍。为了帮助模型收敛，我们使用了 curriculum training scheme[4]。在训练的第一轮，不适用 hard negative items，这样算法可以快速地找到 loss 相对较小的参数空间。之后我们在后续的训练中加入了 hard negative items，专注于让模型学习如何从弱关系中区分高度关联的pins。在第 $n$ 轮，我们对每个物品的负样本集中加入了 $n-1$ 个 hard negative items。&lt;/p&gt;
&lt;h2 id="34-node-embeddings-via-mapreduce"&gt;3.4 Node Embeddings via MapReduce
&lt;/h2&gt;&lt;p&gt;在模型训练结束后，对于所有的物品（包括那些在训练中未见过的物品）直接用训练好的模型生成 embeddings 还是有挑战的。使用算法2对顶点直接计算 embedding 的方法会导致重复计算，这是由顶点的K-hop 邻居导致的。如图1所示，很多顶点在针对不同的目标顶点生成 embedding 的时候被很多层重复计算多次。为了确保推算的有效性，我们使用了一种 MapReduce 架构来避免在使用模型进行推算的时候的重复计算问题。&lt;/p&gt;
&lt;p&gt;我们发现顶点的 embedding 在推算的时候会很好的将其自身带入到 MR 计算模型中。图3详细地表述了 pin-to-board Pinterest 二部图上的数据流，我们假设输入（&amp;ldquo;layer-0&amp;rdquo;）顶点是 pins/items（layer-1 顶点是 boards/contexts）。MR pipeline 有两个关键的组成部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;一个 MapReduce 任务将所有的 pins 投影到一个低维隐空间中，在这个空间中会进行聚合操作（算法1，第一行）&lt;/li&gt;
&lt;li&gt;另一个 MR 任务是将结果的 pins 表示和他们出现在的 boards 的 id 进行连接，然后通过 board 的邻居特征的池化来计算 board 的 embedding。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意，我们的方法避免了冗余的计算，对于每个顶点的隐向量只计算一次。在获得 boards 的 embedding 之后，我们使用两个以上的 MR 任务，用同样的方法计算第二层 pins 的 embedding，这个步骤也是可以迭代的（直到 K 个卷积层）。&lt;/p&gt;
&lt;h3 id="35-efficient-nearest-neighbor-lookups"&gt;3.5 Efficient nearest-neighbor lookups
&lt;/h3&gt;&lt;p&gt;由 PinSage 生成的 embeddings 可以用在很多下游推荐任务上，在很多场景中我们可以直接使用这些 embeddings 来做推荐，通过在学习到的嵌入空间中使用最近邻查找。也就是，给定一个查询物品 $q$，我们使用 K-近邻的方式来查找查询物品 embedding 的 K 个邻居的嵌入。通过局部敏感哈希[2]的近似 K 近邻算法很高效。在哈希函数计算出后，查找物品可以通过一个基于 Weak AND 操作[5]的两阶段查询实现。PinSage 模型是离线计算的并且所有节点的表示通过 MR 计算后存放到数据库中，高效的最近邻查找方法可以使系统在线提供推荐结果。&lt;/p&gt;</description></item><item><title>Lattice LSTM 中文NER</title><link>https://davidham3.github.io/blog/p/lattice-lstm-%E4%B8%AD%E6%96%87ner/</link><pubDate>Wed, 23 May 2018 16:54:12 +0000</pubDate><guid>https://davidham3.github.io/blog/p/lattice-lstm-%E4%B8%AD%E6%96%87ner/</guid><description>&lt;p&gt;ACL 2018，基于LSTM+CRF，用word2vec对字符进行表示，然后用大规模自动分词的预料，将词进行表示，扔进LSTM获得细胞状态，与基于字符的LSTM的细胞状态相结合，得到序列的隐藏状态，然后套一个CRF。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1805.02023" target="_blank" rel="noopener"
&gt;Chinese NER Using Lattice LSTM&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="摘要"&gt;摘要
&lt;/h2&gt;&lt;p&gt;我们调查了lattice-structured LSTM模型在中文分词上的表现，这个模型将输入的字符序列和所有可能匹配到词典中的词进行编码。对比基于字符的方法，我们的模型明显的利用了词与词序列的信息。对于基于词的方法，lattice LSTM不会受到错误分词的影响。门控循环细胞可以使模型从序列中选取最相关的字符和单词获得更好的NER结果。实验在各种数据集上都显示出lattice LSTM比基于词和基于字的LSTM要好，获得了最好的效果。&lt;/p&gt;
&lt;h2 id="引言"&gt;引言
&lt;/h2&gt;&lt;p&gt;信息抽取中最基础的任务，NER近些年受到了广泛的关注。NER以往被当作一个序列标注问题来解决，实体的边界和类别标签是同时进行预测的。当前最先进的英文命名实体识别的方法是使用集成进单词表示的字符信息的LSTM-CRF模型（Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018）。
中文NER与分词联系的很紧密。尤其是命名实体的边界也是词的边界。一个直观的想法是先分词，再标注词。然而这个pipeline会受到错误分词的影响，因为命名实体是分词中OOV中的很重要的一部分，而且不正确的实体边界划分会导致错误的NER。这个问题在open domain中很严重，因为跨领域的分词还是为解决的问题（Liu and Zhang, 2012; Jiang et al., 2013; Liu et al., 2014; Qiu and Zhang, 2015; Chen et al., 2017; Huang et al., 2017）。基于字符的方法比基于词的方法在中文NER中表现的好（He and Wang, 2008; Liu et al., 2010; Li et al., 2014）。
然而，基于字符的NER的一个缺点是，词与词的序列信息不能被完全利用到，然而这部分信息可能很有用。为了解决这个问题，我们通过使用一个lattice LSTM表示句子中的lexicon words，在基于字符的LSTM-CRF模型中集成了latent word information。如图1所示，我们通过使用一个大型的自动获取的词典来匹配一个句子，构建了一个词-字lattice。结果是，词序列，像“长江大桥”，“长江”，“大桥”可以用来在上下文中区分潜在的相关的命名实体，比如人名“江大桥”。
&lt;img src="https://davidham3.github.io/blog/images/lattice-lstm-%e4%b8%ad%e6%96%87ner/Fig1.PNG"
loading="lazy"
alt="Fig1"
&gt;
&lt;img src="https://davidham3.github.io/blog/images/lattice-lstm-%e4%b8%ad%e6%96%87ner/Fig2.PNG"
loading="lazy"
alt="Fig2"
&gt;
因为在lattice中有很多潜在的词-字路径，我们利用了一个lattice-LSTM结构来自动地控制句子的开始到结尾的信息流。如图2所示，门控细胞被用于动态规划信息从不同的路径到每个字符上。在NER数据上训练的lattice LSTM可以学习到如何从上下文中找到有用的单词，自动地提高NER的精度。对比基于字符的和基于单词的NER方法，我们的模型的优势在于利用在字符序列标签上的单词信息，且不会受到错误分词的影响。
结果显示我们的模型比字符序列标注模型和使用LSTM-CRF的单词序列标注模型都要好很多，在很多中文跨领域的NER数据集上都获得了很好的结果。我们的模型和数据在https://github.com/jiesutd/LatticeLSTM。&lt;/p&gt;
&lt;h2 id="相关工作"&gt;相关工作
&lt;/h2&gt;&lt;p&gt;我们的工作与当前处理NER的神经网络一致。Hammerton(2003)尝试解决使用一个单向的LSTM解决这个问题，这个第一个处理NER的神经网络。Collobert et al. (2011)使用了一个CNN-CRF的结构，获得了和最好的统计模型相当的结果。dos Santos et al. (2015)使用了字符CNN来增强CNN-CRF模型。大部分最近的工作利用了LSTM-CRF架构。Huang et al. (2015)使用手工的拼写特征；Ma和Hovy（2016）以及Chiu and Nichols（2016）使用了一个字符CNN来表示拼写的字符；Lample et al.（2016）使用一个字符LSTM，没有使用CNN。我们的baseline基于词的系统使用了与这些相似的架构。
字符序列标注是处理中文NER的主要方法（Chen et al., 2006b; Lu et al., 2016; Dong et al., 2016）。已经有讨论基于词的和基于字符的方法的统计的方法对比，表明了后者一般有更好的表现（He and Wang, 2008; Liu et al., 2010; Li et al., 2014）。我们发现有着恰当的表示设定，结论同样适用于神经NER。另一方面，lattice LSTM相比于词LSTM和字符LSTM是更好的一个选择。
如何更好的利用词的信息在中文NER任务中受到了持续的关注（Gao et al., 2015），分词信息在NER任务中作为soft features（Zhao and Kit, 2008; Peng and Dredze, 2015; He and Sun, 2017a），使用对偶分解的分词与NER联合学习也被人研究了（Xu et al., 2014），多任务学习（Peng and Dredze, 2016）等等。我们的工作也是，聚焦于神经表示学习。尽管上述的方法可能会被分词训练数据和分词的错误影响，我们的方法不需要一个分词器。这个模型不需要考虑多任务设定，因此从概念上来看就更简单。
NER可以利用外部信息。特别地，词典特征已经被广泛地使用了（Collobert et al., 2011; Passos et al., 2014; Huang et al., 2015; Luo et al., 2015）。Rei(2017)使用了一个词级别的语言模型目的是增强NER的训练，在大量原始语料上实现多任务学习。Peters et al.(2017)预训练了一个字符语言模型来增强词的表示。Yang et al.(2017b)通过多任务学习探索了跨领域和跨语言的知识。我们通过在大量自动分词的文本上预训练文本嵌入词典利用了外部信息，尽管半监督技术如语言模型are orthogonal to而且也可以在我们的lattice LSTM模型中使用。
Lattice结构的RNN可以被看作是一个树状结构的RNN（Tai et al., 2015）对DAG的自然扩展。他们已经有被用来建模运动力学（Sun et al., 2017），dependency-discourse DAGs(Peng et al., 2017)，还有speech tokenization lattice（Sperber et al., 2017）以及对NMT（neural machine translation）编码器的多粒度分词输出。对比现在的工作，我们的lattice LSTM在动机和结构上都是不同的。比如，对于以字符为中心的lattice-LSTM-CRF序列标注设计的模型，它有循环细胞但是没有针对词的隐藏向量。据我们所知，我们第一个设计了一个新型的lattice LSTM对字母和词进行混合的表示，也是第一个使用一个基于词的lattice处理不分词的中文NER任务的。&lt;/p&gt;
&lt;h2 id="模型"&gt;模型
&lt;/h2&gt;&lt;p&gt;我们跟从最好的英语NER模型（Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016），使用LSTM-CRF作为主要的网络结构。使用$s=c_1, c_2, &amp;hellip;, c_m$表示输入的句子，其中$c_j$表示第$j$个字符。$s$可以被看作一个单词序列$s=w_1, w_2, &amp;hellip;, w_n$，其中$w_i$表示序列中的第$i$个单词，由一个中文分词器获得。我们使用$t(i, k)$表示句子中第$i$个单词的第$k$个字符表示下标$j$。取图1的句子作为例子。如果分词结果是“南京市 长江大桥”，下标从1开始，那么$t(2, 1)=4$（长），$t(1, 3)=3$（市）。我们使用BIOES标记（Ratinov and Roth, 2009）对基于词和基于字的NER进行标记。
&lt;img src="https://davidham3.github.io/blog/images/lattice-lstm-%e4%b8%ad%e6%96%87ner/Fig3.PNG"
loading="lazy"
alt="Fig3"
&gt;&lt;/p&gt;
&lt;h3 id="基于字符的模型"&gt;基于字符的模型
&lt;/h3&gt;&lt;p&gt;基于字符的模型如图3(a)所示。它在$c_1, c_2, &amp;hellip;, c_m$上使用了LSTM-CRF模型。每个字符$c_j$表示为
&lt;/p&gt;
$$x^c\_j = e^c(c\_j)$$&lt;p&gt;
其中$e^c$表示一个字符嵌入到了lookup table中。
一个双向LSTM（与式11同结构）被使用在$x_1, x_2, &amp;hellip;, x_m$来获取从左到右的$\overrightarrow{h}^c_1, \overrightarrow{h}^c_2, &amp;hellip;, \overrightarrow{h}^c_m$和从右到左的$\overleftarrow{h}^c_1, \overleftarrow{h}^c_2, &amp;hellip;, \overleftarrow{h}^c_m$隐藏状态，这两个隐藏状态有两组不同的参数。每个字符的隐藏向量表示为
&lt;/p&gt;
$$h^c\_j = [\overrightarrow{h}^c\_j, \overleftarrow{h}^c\_j]$$&lt;p&gt;
一个标准的CRF模型被用在$h^c_1, h^c_2, &amp;hellip;, h^c_m$上来进行序列标注。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;字符+双字符
Character bigrams在分词中用来表示字符已经很有用了（Chen et al., 2015; Yang et al., 2017a）。我们提出了通过拼接双元字符嵌入和字符嵌入的基于字符的模型：
$$x^c\_j = [e^c(c\_j); e^b(c\_j, c\_{j+1})]$$
其中$e^b$表示一个character bigram lookup table。&lt;/li&gt;
&lt;li&gt;字符+softword
已经有实验表明使用分词作为soft features对于基于字符的NER模型可以提升性能（Zhao and Kit, 2008; Peng and Dredze, 2016）。我们提出的通过拼接分词标记嵌入和字符嵌入的带有分词信息的字符表示：
$$x^c\_j = [e^c(c\_j); e^s(seg(c\_j))]$$
其中$e^s$表示一个分词标签嵌入查询表。$seg(c_j)$表示一个分词器在字符$c_j$上给出的分词标签。我们使用了BMES策略来表示分词（Xue, 2003）
$$h^w\_i = [\overrightarrow{h^w\_i}, \overleftarrow{h^w\_i}]$$
与基于字符的情况类似，一个标准的CRF模型在序列标记中被用在了$h^w_1, h^w_2, &amp;hellip;, h^w_m$上。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="基于词的模型"&gt;基于词的模型
&lt;/h3&gt;&lt;p&gt;基于词的模型如图3（b）所示，它将word embedding $e^w(w_i)$作为每个词$w_i$的表示：
&lt;/p&gt;
$$x^w\_i = e^w(w\_i)$$&lt;p&gt;
其中$e^w$表示一个词嵌入查找表。一个双向LSTM被用来获取词序列$w_1, w_2, &amp;hellip;, w_n$上一个从左到右的隐藏状态$\overrightarrow{h}^w_1, \overrightarrow{h}^w_2, &amp;hellip;, \overrightarrow{h}^w_n$和一个从右到左的隐藏状态序列$\overleftarrow{h}^w_1, \overleftarrow{h}^w_2, &amp;hellip;, \overleftarrow{h}^w_n$。最后，对于每个词$w_i$，$\overrightarrow{h^w_i}$和$\overleftarrow{h^w_i}$会被拼在一起成为它的表示：
&lt;strong&gt;集成字符表示&lt;/strong&gt;
字符CNN（Ma and Hovy, 2016）和LSTM（Lample et al., 2016）两种方法都被用于过表示一个单词中的字符序列。我们在中文NER中对两个方法都进行了实验。我们使用$x^c_i$表示$w_i$中的字符，通过拼接$e^w(w_i)$和$x^c_i$可以获得一个新词的表示：
&lt;/p&gt;
$$x^w\_i = [e^w(w\_i; x^c\_i)]$$&lt;ol&gt;
&lt;li&gt;词+字符LSTM
将每个输入字符的嵌入记作$e^c(c_j)$，我们使用一个双向LSTM来学习词$w_i$的字符$c_{t(i, 1)}, &amp;hellip;, c_{t(i, len(i))}$的隐藏状态$\overrightarrow{h}^c_{t(i, 1)}, &amp;hellip;, \overrightarrow{h}^c_{t(i, len(i))}$和$\overleftarrow{h}^c_{t(i, 1)}, &amp;hellip;, \overleftarrow{h}^c_{t(i, len(i))}$，其中$len(i)$表示词$w_i$的字符个数。最后$w_i$的字符表示为：
$$x^c\_i = [\overrightarrow{h}^c\_{t(i, len(i))};\overleftarrow{h}^c\_{t(i, 1)}]$$&lt;/li&gt;
&lt;li&gt;词+字符LSTM'
我们调查了一种词+字符LSTM的变形，这个模型使用单向的LSTM对每个字符获取$\overrightarrow{h}^c_j$和$\overleftarrow{h}^c_j$。与Liu et al. (2018)的结构相似但是没有使用highway layer。使用了相同的LSTM结构和相同的方法集成字符隐藏状态进词嵌入中。&lt;/li&gt;
&lt;li&gt;词+字符CNN
我们使用标准的CNN（LeCun et al., 1989）应用在词的字符序列上获得字符表示$x^c_i$。将字符$c_j$的嵌入记为$e^c(c_j)$，向量$x^c_i$通过以下式子得到：
$$x^c\_i = \max\_{t(i,1) \leq j \leq t(i, len(i))}(W^T\_{CNN} \begin{bmatrix}
e^c(c\_{j-\frac{ke-1}{2}}) \\
... \\
e^c(c\_{j+\frac{ke-1}{2}})
\end{bmatrix}+ b\_{CNN})$$
其中，$W_{CNN}$和$b_{CNN}$和参数，$ke=3$是核的大小，$max$表示最大池化。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="lattice模型"&gt;Lattice模型
&lt;/h3&gt;&lt;p&gt;图2中展示了词-字lattice模型的整个结构，可以看作是基于字的模型的扩展，集成了基于词的细胞和用来控制信息流的额外的门。
图3（c）展示了模型的输入是一个字符序列$c_1, c_2, &amp;hellip;, c_m$，与之一起的还有所有字符序列，字符都能在词典$\mathbb{D}$中匹配到。如部分2中指示的，我们使用自动分词的大型原始语料来构建$\mathbb{D}$。使用$w^d_{b,e}$来表示一个起始字符下标为$b$，结尾字符下标为$e$，图1中的$w^d_{1,2}$是“南京（Nanjing）”，$w^d_{7,8}$是“大桥（Bridge）”。
模型涉及到了四种类型的向量，分别是输入向量、输出隐藏向量、细胞向量、门向量。作为基本的组成部分，一个字符输入向量被用来表示每个字符$c_j$，就像在基于字符的模型中：
$x^c_j = e^c(c_j)$
基本的循环结构是通过一个在每个字符$c_j$上的字符细胞向量$\mathbf{c}^c_j$和一个隐藏向量$\mathbf{h}^c_j$构造的，其中$\mathbf{c}^c_j$提供句子的开始到$c_j$的信息流，$\mathbf{h}^c_j$用于CRF序列标注。
基础的循环LSTM函数如下：
&lt;/p&gt;
$$
\begin{bmatrix}
i^c\_j \\
o^c\_j \\
f^c\_j \\
\widetilde{c}^c\_j
\end{bmatrix} =
\begin{bmatrix}
\sigma \\
\sigma \\
\sigma \\
tanh
\end{bmatrix}({W^c}^T
\begin{bmatrix}
x^c\_j \\
h^c\_{j-1}
\end{bmatrix}+b^c)
$$&lt;p&gt;
&lt;/p&gt;
$$c^c\_j = f^c\_j \odot c^c\_{j-1} + i^c\_j \odot \hat{c}^c\_j$$&lt;p&gt;
&lt;/p&gt;
$$h^c\_j = o^c\_j \odot tanh(c^c\_j)$$&lt;p&gt;
其中，$i^c_j$，$f^c_j$和$o^c_j$表示一组输入、遗忘和输出门。${w^c}^T$和$b^c$是模型参数。$\sigma()$表示sigmoid function。
不同于基于字符的模型，现在计算$c^c_j$的时候需要考虑句子中词典序列$w^d_{b,e}$。特别地，每个序列$w^d_{b,e}$被表示为：
&lt;/p&gt;
$$x^w\_{b,e} = e^w(w^d\_{b,e})$$&lt;p&gt;
其中$e^w$表示3.2节相同的词嵌入查询表。
此外，一个词细胞$c^w_{b,e}$用来表示$x^w_{b,e}$从句子开始的循环状态。$c^w_{b,e}$通过以下式子计算得到：
&lt;/p&gt;
$$
\begin{bmatrix}
i^w\_{b,e} \\
f^w\_{b,e} \\
\widetilde{c}^w\_{b,e}
\end{bmatrix} = \begin{bmatrix}
\sigma \\
\sigma \\
tanh
\end{bmatrix}({w^w}^T \begin{bmatrix}
x^w\_{b,e} \\
h^c\_b
\end{bmatrix} + b^w)
$$&lt;p&gt;
&lt;/p&gt;
$$c^w\_{b,e} = f^w\_{b,e} \odot c^c\_b + i^w\_{b,e} \odot \widetilde{c}^w\_{b,e}$$&lt;p&gt;
其中$i^w_{b,e}$和$f^w_{b,e}$是一组输入和遗忘门。对于词细v胞来说没有输出门因为标记只在字符层面上做。
有了$c^w_{b,e}$，会有很多路径可以使信息流向每个$c^c_j$。比如，在图2中，对于$c^c_7$的输入包含$x^c_7$（桥Bridge），$c^w_{6,7}$（大桥Bridge）和$c^w_{4,7}$（长江大桥Yangtze River Bridge）。我们将$c^w_{b,e}$和$b \in \lbrace b&amp;rsquo; \mid w^d_{b&amp;rsquo;,e} \in \mathbb{D}\rbrace$连接到细胞$c^c_e$。我们使用额外的门$i^c_{b,e}$对每个序列细胞$c^w_{b,e}$来控制它对$c^c_{b,e}$的贡献：
&lt;/p&gt;
$$i^c\_{b,e} = \sigma({w^l}^T \begin{bmatrix}
x^c\_e \\
c^w\_{b,e}
\end{bmatrix} + b^l)$$&lt;p&gt;
因此，$c^c_j$的计算变为：
&lt;/p&gt;
$$c^c\_j = \sum\_{b \in \lbrace b' \mid w^d\_{b',j} \in \mathbb{D}\rbrace } \alpha^c\_{b,j} \odot c^w\_{b,j} + \alpha^c\_j \odot \widetilde{c}^c\_j$$&lt;p&gt;
在上式中，门$i^c_{b,j}$和$i^c_{j}$的值被归一化到$\alpha^c_{b,j}$和$\alpha^c_j$，和为1。
&lt;/p&gt;
$$
\alpha^c\_{b,j} = \frac{exp(i^c\_{b,j})}{exp(i^c\_j)+\sum\_{b' \in \lbrace b'' \mid w^d\_{b'',j} \in \mathbb{D}\rbrace}exp(i^c\_{b',j})}
$$&lt;p&gt;
&lt;/p&gt;
$$
\alpha^c\_{j} = \frac{exp(i^c\_{j})}{exp(i^c\_j)+\sum\_{b' \in \lbrace b'' \mid w^d\_{b'',j} \in \mathbb{D}\rbrace}exp(i^c\_{b',j})}
$$&lt;p&gt;
最后的隐藏向量$h^c_j$仍然由之前的LSTM计算公式得到。在NER训练过程中，损失值反向传播到参数$w^c, b^c, w^w, b^w, w^l$和$b^l$使得模型可以动态地在NER标注过程中关注更相关的词。&lt;/p&gt;
&lt;h3 id="解码和训练"&gt;解码和训练
&lt;/h3&gt;&lt;p&gt;一个标准的CRF层被用在$h_1, h_2, &amp;hellip;, h_{\tau}$上面，其中$\tau$对于基于字符的模型来说是$n$，对于基于词的模型来说是$m$。一个标签序列$y = l_1, l_2, &amp;hellip;, l_{\tau}$的概率是
&lt;/p&gt;
$$
p(y \mid s) = \frac{exp(\sum\_i(w^{l\_i}\_{CRF} h\_i + b^{(l\_{i-1}, l\_i)}\_{CRF}))}{\sum\_{y'}exp(\sum\_i(w^{l'\_i}\_{CRF} h\_i + b^{(l'\_{i-1}, l'\_i)}\_{CRF}))}
$$&lt;p&gt;
这里$y&amp;rsquo;$表示一个任意标签序列，$W^{l_i}_{CRF}$是针对于$l_i$的模型参数，$b^{(l_{i-1},l_i)}_{CRF}$是针对$l_{i-1}$和$l_i$的偏置。
我们使用一阶维特比算法来寻找一个基于词或基于字符的输入序列中得分最高的标签序列。给定一组手动标注的训练数据$\lbrace (s_i, y_i)\rbrace \mid^N_{i=1}$，带有L2正则项的句子层面的log-likelihood作为loss，训练模型：
&lt;/p&gt;
$$L = \sum^N\_{i=1} log(P(y\_i \mid s\_i)) + \frac{\lambda}{2}\Vert \Theta \Vert^2$$&lt;p&gt;
其中，$\lambda$是L2正则项系数，$\Theta$表示了参数集合。&lt;/p&gt;</description></item><item><title>门控卷积网络语言建模</title><link>https://davidham3.github.io/blog/p/%E9%97%A8%E6%8E%A7%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E5%BB%BA%E6%A8%A1/</link><pubDate>Wed, 23 May 2018 10:54:44 +0000</pubDate><guid>https://davidham3.github.io/blog/p/%E9%97%A8%E6%8E%A7%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E5%BB%BA%E6%A8%A1/</guid><description>&lt;p&gt;ICML 2017，大体思路：卷积+一个线性门控单元，替代了传统的RNN进行language modeling，后来的Facebook将这个用于机器翻译，提出了卷积版的seq2seq模型。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1612.08083" target="_blank" rel="noopener"
&gt;Language Modeling with Gated Convolutional Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="摘要"&gt;摘要
&lt;/h2&gt;&lt;p&gt;当前流行的语言建模模型是基于RNN的。在这类任务上的成功经常和他们捕捉unbound context有关。这篇文章中我们提出了一个通过堆叠convolutions的finite context方法，卷积可以变得更有效因为他们可以在序列上并行。我们提出了一个新型的简单的门控机制，这个门控机制表现的要比Oord et al.(2016b)要好，我们也探究了关键架构决策的影响。我们提出的方法在WikiText-103上达到了最好的效果，even though it features long-term dependencies，在Google Billion Words上也达到了最好的效果。Our model reduces the latency to score a sentece by an order of magnitude compared to a recurrent baseline. 据我们所知，这是在大规模语言任务上第一次一个非循环结构的方法超越了强有力的循环模型。&lt;/p&gt;
&lt;h2 id="引言"&gt;引言
&lt;/h2&gt;&lt;p&gt;统计语言模型估计一个单词序列的概率分布，通过给定当前的单词序列，对下一个单词的概率进行建模
&lt;/p&gt;
$$P(w\_0, ..., w\_N) = P(w\_0)\prod^N\_{i=1}P(w\_i \mid w\_0, ..., w\_{i-1})$$&lt;p&gt;
其中$w_i$是单词表中的单词下标。语言模型对语音识别(Yu &amp;amp; Deng, 2014)和机器翻译(Koehn, 2010)来说是很重要的一部分。
最近，神经网络(Bengio et al., 2014; Mikolov et al., 2010; Jozefowicz et al., 2016)已经展示出了比传统n-gram模型(Kneser &amp;amp; Ney, 1995; Chen &amp;amp; Goodman, 1996)更好的语言模型。这些传统模型不能解决数据稀疏的问题，这个问题导致这些方法很难对大量的上下文进行表示，因此也不能回长范围的依赖进行表示。神经语言模型通过在连续空间中对词的嵌入解决了这个问题。当前最好的语言模型是基于LSTM（Hochreiter et al., 1997）的模型，LSTM理论上可以对任意长度的依赖进行建模。
在这篇文章中，我们引入了新的门控卷积网络，并且用它进行语言建模。卷积网络可以被堆叠起来来表示大量的上下文并且在越来越长的有着抽象特征（LeCun &amp;amp; Bengio, 1995）的上下文中提取层次特征。这使得这些模型可以通过上下文为$N$，卷积核宽度为$k$，$O(\frac{N}{k})$的操作对长时间的依赖关系进行建模。相反，循环网络将输入看作是一个链式结构，因此需要一个线性时间$O(N)$的操作。
层次的分析输入与传统的语法分析相似，传统的语法分析建立粒度增加的语法树结构，比如，包含名词短语和动词短语的句子，短语中又包含了更内在的结构（Manning &amp;amp; Schutze, 1999; Steedman, 2002）。层次结构也会让学习变得更简单，因为对于一个给定的上下文大小，相比链式结构，非线性单元的数量会减少，因此减轻了梯度消失的问题（Glorot &amp;amp; Bengio, 2010）。
现代的硬件对高度并行的模型支持的很好。在循环神经网络中，下一个输出依赖于之前的隐藏状态，而之前的隐藏状态在序列中元素上是不能并行的。然而，卷积神经网络对这个计算流程支持的很好因为卷积是可以在输入元素上同时进行的。
对于RNN来说想要达到很好的效果（Jozefowicz et al., 2016），门的作用很重要。我们的门控线性单元为深层的结构对梯度提供了一条线性的通道，同时又保留了非线性的特性，减少了梯度消失的现象。
我们展示了门控卷积网络比其他的已经发表的语言模型都要好，比如在Google Billion Word Benchmark（Chelba et al., 2013）上的LSTM。我们也评估了我们的模型在处理长范围依赖关系WikiText-103上的能力，在这个数据集上，模型是以段落为条件进行输入的，而不是一个句子，我们在这个数据集（Merity et al., 2016）上获得了最好的效果。最后，我们展示了门控线性单元获得了更好的精度以及相比于Oord et al., 2016的LSTM门收敛的更快。&lt;/p&gt;
&lt;h2 id="方法"&gt;方法
&lt;/h2&gt;&lt;p&gt;在这篇文章中我们引入了一种新的神经语言模型，这种模型使用门控时间卷积替代了使用在循环神经网络中使用的循环链接。神经语言模型（Bengio et al., 2003）提供了一种对每个单词$w_0, &amp;hellip;, w_N$的上下文表示$H=[h_0, &amp;hellip;, h_N]$用来预测下一个词的概率$P(w_i \mid h_i)$。循环神经网络$f$通过一个循环函数$h_i = f(h_{i-1}, w_{i-1})$计算$H$，这个循环函数本质上是一种不能并行处理的序列操作。
我们提出的方法使用函数$f$对输入进行卷积来获得$H = f \ast w$并且因此没有时间上的依赖，所以它能更好的在句子中的单词上并行计算。这个过程将会把许多前面出现的单词作为一个函数进行计算。对比卷积神经网络，上下文的大小是有限的，但是我们展示出了有限的上下文大小不是必须的，而且我们的模型可以表示足够大的上下文并表现的很好。
&lt;img src="https://davidham3.github.io/blog/images/%e9%97%a8%e6%8e%a7%e5%8d%b7%e7%a7%af%e7%bd%91%e7%bb%9c%e8%af%ad%e8%a8%80%e5%bb%ba%e6%a8%a1/Fig1.PNG"
loading="lazy"
alt="Fig1"
&gt;
图1展示了模型的架构。词通过一个嵌入的向量进行表示，这些表示存储在lookup table$\mathbf{D}^{\vert \mathcal{V} \vert \times e}$中，其中$\vert \mathcal{V} \vert$是词库中单词的数量，$e$是嵌入的大小。我们模型的输入是一个词序列$w_0, &amp;hellip;, w_N$，这个序列被词向量表示为$E = [D_{w_0}, &amp;hellip;, D_{w_N}]$。我们将隐藏层$h_0, &amp;hellip;, h_L$计算为
&lt;/p&gt;
$$h\_l(X) = (X \ast W + b) \otimes \sigma(X \ast V + c)$$&lt;p&gt;
其中，$m$，$n$分别是输入和输出的feature map的数量，$k$是patch size，$X \in \mathbb{R}^{N \times m}$是层$h_l$的输入（要么是词嵌入，要么是前一层的输出）,$W \in \mathbb{R}^{k \times m \times n}$，$b \in \mathbb{R}^n$，$V \in \mathbb{R}^{k \times m \times n}$，$c \in \mathbb{R}^n$是学习到的参数，$\sigma$是sigmoid function，$\otimes$是矩阵间的element-wise product。
当卷积输入时，我们注意$h_i$不包含未来单词的信息。我们通过移动卷积输入来防止卷积核看到未来的上下文（Oord et al., 2016a）来解决这个问题。特别地，我们在序列的开始加入了$k-1$宽度的0作为padding补全，假设第一个输入的元素是序列的开始元素，起始的标记我们是不预测的，$k$是卷积核的宽度。
每层的输出是一个线性变换$X \ast W + b$通过门$\sigma(X \ast W + b)$调节。与LSTM相似的是，这些门乘以矩阵的每个元素$X \ast W + b$，并且以层次的形式控制信息的通过。我们称这种门控机制为Gated Linear Units(GLU)。通过在输入$E$上堆叠多个这样的层，可以得到每个词$H = h_L \circ &amp;hellip; \circ h_0(E)$的上下文表示。我们将卷积和门控线性单元放在了一个preactivation residual block，这个块将输入与输出相加（He et al., 2015a）。这个块有个bottleneck结构，可以使计算更高效并且每个块有5层。
获得模型预测结果最简单的是使用softmax层，但是这个选择对于语料库很大和近似来说一般计算起来很慢，像noise contrastive estimation(Gutmann &amp;amp; Hyvarinen)或层次softmax(Morin &amp;amp; Bengio, 2005)一般更常用。我们选了后者的改良版adaptive softmax，这个算法将higher capacity分配给出现频率更高的单词，lower capacity分配给频率低的单词（Grave et al., 2016a）。这使得在训练和测试的时候内存占用更少且计算速度更快。&lt;/p&gt;
&lt;h2 id="门控机制"&gt;门控机制
&lt;/h2&gt;&lt;p&gt;门控机制控制了网络中信息流通的路径，在循环神经网络中已经证明了是非常有效的手段（Hochreiter &amp;amp; Schumidhuber, 1997）。LSTM通过输入和遗忘门控制分离的细胞使得LSTM获得长时间的记忆。这使得信息可以不受阻碍的流通多个时间步。没有这些门，信息会在通过时间步的转移时轻易地消失。与之相比，卷积神经网络不会遇到这样的梯度消失现象，我们通过实验发现卷积神经网络不需要遗忘门。
因此，我们认为模型只需要输出门，这个门可以控制信息是否应该通过这些层。我们展示了这个模型对语言建模很有效，因为它可以使模型选择预测下一个单词的时候哪个单词是相关的。和我们同时进行研究的，Oord et al.(2016b)展示了LSTM风格的门控机制，$tanh(X \ast W + b) \otimes \sigma(X \ast V + c)$在对图像进行卷积建模的有效性。后来，Kalchbrenner et al. (2016)在翻译和字符级别的语言建模上使用额外的门扩展了这个机制。
门控线性单元是一种简化的门控机制，基于Dauphin &amp;amp; Grangier(2015)对non-deterministic gates的研究，这个门可以通过和门组合在一起的线性单元减少梯度消失的问题。这个门尽管允许梯度通过线性单元进行传播而不发生缩放的变化，但保持了层非线性的性质。我们称之为gated tanh unit(GTU)的LSTM风格的门的梯度是：
&lt;/p&gt;
$$\nabla[tanh(X) \otimes \sigma(X)]=tanh'(X) \nabla X \otimes \sigma(X) + \sigma'(X) \nabla X \otimes tanh(X)$$&lt;p&gt;
注意到随着我们堆叠的层数的增加，它会渐渐地消失，因为$tanh&amp;rsquo;(X)$和$\sigma&amp;rsquo;(X)$这两个因数的数值范围在减小。相对来说，门控线性单元的梯度：
&lt;/p&gt;
$$\nabla [X \otimes \sigma(X)] = \nabla X \otimes \sigma(X) + X \otimes \sigma'(X) \nabla X$$&lt;p&gt;
有一条路径$\nabla X \otimes \sigma(X)$对于在$\sigma(X)$中的激活的门控单元没有减小的因数。这可以被理解为一个跳过乘法的连接帮助梯度传播过这些层。我们通过实验比较了不同的门策略后发现门控线性单元可以收敛地更快且困惑度的值更好。&lt;/p&gt;</description></item><item><title>Convolutional Sequence to Sequence Learning</title><link>https://davidham3.github.io/blog/p/convolutional-sequence-to-sequence-learning/</link><pubDate>Tue, 22 May 2018 11:30:51 +0000</pubDate><guid>https://davidham3.github.io/blog/p/convolutional-sequence-to-sequence-learning/</guid><description>&lt;p&gt;ICML 2017. Facebook 2017年的卷积版seq2seq。卷积加注意力机制，外加GLU，训练速度很快，因为RNN训练时依靠上一个元素的隐藏状态，CNN可以并行训练。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1705.03122" target="_blank" rel="noopener"
&gt;Convolutional Sequence to Sequence Learning&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="abstract"&gt;Abstract
&lt;/h1&gt;&lt;p&gt;流行的序列到序列的学习方法将输入的序列通过循环神经网络映射到一个变长输出序列。我们提出了一个完全基于卷积神经网络的架构。对比循环神经网络模型，在训练过程中对所有元素的运算都可以并行，而且可以充分利用GPU资源，优化过程也会变得更加容易，因为非线性单元的个数是固定的，而且与输入长度无关。我们使用的门控线性单元（GLU）可以帮助梯度传播，我们在每个解码层上部署了一个独立的注意力模块。我们算法表现在WMT’14英语-德语和WMT’14英语-法语两个数据集上，都要比Wu等人的深度LSTM拥有更高的精度，且在GPU和CPU上训练速度都快了一个量级。&lt;/p&gt;
&lt;h1 id="1-introduction"&gt;1. Introduction
&lt;/h1&gt;&lt;p&gt;序列到序列模型在很多任务中都大获成功，如机器翻译、语音识别（Sutskever et al., 201v4; Chorowski et al., 2015），文本摘要（Rush et al., 2015; Nallapati et al., 2016; Shen et al., 2016）等等。当今主流的方法将输入序列使用一个双向循环神经网络进行编码，并且用另一个循环神经网络生成一个变长输出序列，这两个模型通过soft-attention（Bahdanau et al., 2014; Luong et al., 2015）相连。在机器翻译中，这个架构已经比传统的基于短语的模型要好出很多了（Sennrich et al., 2016b; Zhou et al., 2016; Wu et al., 2016）。&lt;/p&gt;
&lt;p&gt;尽管卷积神经网络有很多优点，但是很少应用在序列建模中（Waibel et al., 1989; LeCun &amp;amp; Bengio, 1995）。对比循环神经网络中的循环层，卷积是对定长的内容生成表示，然而，有效的卷积长度可以通过简单的堆叠卷积层变得逐渐增加。这就使得我们可以精确地控制要建模的依赖关系的最大长度。卷积神经网络不需要依赖于之前时间步的计算，因此可以在序列中的任意一个地方进行并行运算。RNN需要维持一个含有整个过去信息的隐藏状态，导致对序列进行计算时不能并行。&lt;/p&gt;
&lt;p&gt;多层卷积神经网络在整个输入序列上构建了层次表示，在这个层次表示中，底层是临近的输入元素交互，高层是离得较远的元素交互。层级结构相比于链式结构的循环神经网络，提供了一条更短的路径来捕获长范围的依赖关系。比如，我们可以用一个滑动窗在 $n$ 个单词上，使用复杂度为 $O(\frac{n}{k})$ 的卷积操作，卷积核宽度为 $k$，来获取单词间的关系，提取到一个特征表示，如果用循环神经网络，复杂度为 $O(n)$。卷积神经网络的输入会被放入一个有着固定数目的卷积核和非线性单元的网络中，然而循环神经网络对第一个单词进行了 $n$ 次操作和非线性变换后，对最后一个单词只进行了一组操作。固定数目的非线性操作也可以简化学习过程。&lt;/p&gt;
&lt;p&gt;最近在卷积神经网络应用于序列建模的工作，像 Bradbury et al.(2016)，他们在一连串的卷积层间引入了循环池化。Kalchbrenner et al.(2016) 解决了没有注意力机制的神经机器翻译的问题。然而，这些方法的表现没有一个在大型的数据集上超越当前最先进的技术。门控卷积在之前已经被 Meng et al.(2015) 用于了机器翻译，但是他们的评估方法受限于小数据集，而且模型与传统的基于计数的模型相串联。有一部分是卷积层的架构在大数据集上展现了更好的效果但是他们的解码器仍然是循环的。&lt;/p&gt;
&lt;p&gt;在这篇文章中，我们提出了一个针对序列到序列的模型，这个模型完全是基于卷积的。我们的模型使用了门控线性单元（Dauphin et al., 2016）和残差连接（He et al., 2015a）。我们在每个解码层也使用了注意力机制，而且显示出每个注意力层只增加了一点点可以忽略不计的开销。这些方法的融合可以让我们处理大规模的数据集。&lt;/p&gt;
&lt;p&gt;我们在几个大型的机器翻译数据集上评估了我们的方法，并且和文章中现有的最好的架构们进行了对比与总结。在 WMT’16 英语-罗马尼亚语数据集上，我们的算法是最好的，比之前最好的结果要好 1.9 BLEU。在 WMT’14 英语-德语上，我们比 Wu et al.(2016) 的 strong LSTM 好 0.5 BLEU。在 WMT’14 英语-法语上，我们比 Wu et al.(2016) 的 likelihood trained system 好 1.6 BLEU。除此以外，我们的模型可以翻译从未见过的句子，速度比 Wu et al.(2016) 的算法在 GPU 和 CPU 上都快出一个数量级。&lt;/p&gt;
&lt;h1 id="2-recurrent-sequence-to-sequence-learning"&gt;2. Recurrent Sequence to Sequence Learning
&lt;/h1&gt;&lt;p&gt;序列到序列建模又称基于循环神经网络的编码器-解码器架构（Sutskever et al., 2014; Bahdanau et al., 2014）。编码器 RNN 处理输入序列 $\mathbf{x} = (x_1, …, x_m)$ $m$ 个元素，返回状态表示 $\mathbf{z} = (z_1, …, z_m)$。解码器 RNN 接受 $\bf{z}$ 并且从左到右生成输出序列 $\mathbf{y} = (y_1, …, y_n)$，一次一个元素。为了生成输出 $y_{i+1}$，解码器基于之前的隐藏状态 $h_i$ ，前一个目标单词 $y_i$ 的嵌入表示 $g_i$，还有一个源自编码器输出 $\bf{z}$ 的条件输入 $c_i$，计算一个新的隐藏状态 $h_{i+1}$。基于这个大体的规则，各种各样的编码-解码架构被相继提出，他们之间主要差别是条件输入和RNN的类型不同。&lt;/p&gt;
&lt;p&gt;没有注意力机制的模型通过对所有的 $i$ 设定 $c_i = z_m$，只考虑最后的编码状态 $z_m$（Cho et al., 2014），或是简单地将第一个解码器的隐藏状态初始化为 $z_m$（Sutskever et al., 2014），在后面这种情况中没有用到 $c_i$。带注意力机制的架构（Bahdanau et al., 2014; Luong et al., 2015）在每个时间步，计算 $(z_1, &amp;hellip;, z_m)$ 的加权和得到 $c_i$。求和时每一项的权重称为注意力分数，可以使网络在输出序列时关注于输入序列的不同部分。注意力分数本质上是通过比较每个编码器状态 $z_j$ 和之前的解码器隐藏状态 $h_i$ 与最后的预测结果 $y_i$ 的线性组合，计算得到；结果通过归一化得到了在输入序列上的一个分布。&lt;/p&gt;
&lt;p&gt;当前流行的编解码器使用的模型是长短时记忆网络（LSTM; Hochreiter &amp;amp; Schmidhuber, 1997）和门控循环单元（GRU; Cho et al., 2014）。这两个都是使用门控机制对Elman的RNN（Elman, 1990）进行了扩展，门控机制使得模型可以记得前一时间步获得的信息，以此来对长期依赖进行建模。大多数最近的方法也依赖于双向编码器对过去和未来的上下文环境同时地构建表示（Bahdanau et al., 2014; Zhou et al., 2016; Wu et al., 2016）。通常带有很多层的模型会依赖于残差网络的残差连接（He et al., 2015a; Zhou et al., 2016; Wu et al., 2016）。&lt;/p&gt;
&lt;h1 id="3-a-convolutional-architecture"&gt;3. A Convolutional Architecture
&lt;/h1&gt;&lt;p&gt;接下来我们介绍一个序列到序列的完全卷积架构。我们使用卷积神经网络替代 RNN 来计算中间的编码器状态 $\bf{z}$ 和解码器状态 $\bf{h}$。&lt;/p&gt;
&lt;h2 id="31-position-embeddings"&gt;3.1. Position Embeddings
&lt;/h2&gt;&lt;p&gt;首先，我们将输入序列 $\mathbf{x} = (x_1, &amp;hellip;, x_m)$ 嵌入到一个分布的空间内 $\mathbf{w} = (w_1, &amp;hellip;, w_m)$中，其中 $w_j \in \mathbb{R}^f$ 是嵌入矩阵 $\mathcal{D} \in \mathbb{R}^{V \times f}$ 的一列。我们也通过嵌入输入元素 $\mathbf{p} = (p_1, &amp;hellip;, p_m)$ 的绝对位置使模型对顺序敏感，其中 $p_j \in \mathbb{R}^f$。这两者做加和获得输入元素的表示 $\mathbf{e} = (w_1+p_1, &amp;hellip;, w_m+p_m)$。我们对解码器输出的元素也做相似的工作来生成输出元素表示 $\mathbf{g} = (g_1, &amp;hellip;, g_n)$，然后再传入解码网络。位置嵌入在我们的架构中很有用，因为他们能让模型知道当前正在处理的输入或输出序列中的哪个部分。&lt;/p&gt;
&lt;h2 id="32-convolutional-block-structure"&gt;3.2. Convolutional Block Structure
&lt;/h2&gt;&lt;p&gt;编码器和解码器网络共享一个简单的块状结构，这个块状结构会基于固定长度的输入元素计算出中间状态。我们将解码器的第 $l$ 个块的输出记为 $\mathbf{h}^l = (h^l_1, &amp;hellip;, h^l_n)$，编码器的第 $l$ 个输出记为 $\mathbf{z}^l=(z^l_1, &amp;hellip;, z^l_m)$；我们交替地称块和层。每个块包含一个一维卷积加一个非线性单元。对于一个有着一个块和宽度为 $k$ 的卷积核的解码网络来说，每个结果状态 $h^l_i$ 包含了过去 $k$ 个输入元素的信息。堆叠多个块增加了在一个状态中可以表示的输入元素的数量。比如，堆叠6个宽度为 $k=5$ 的块可以得到输入空间为25个元素的表示，也就是每个输出依赖于25个输入。非线性单元允许网络利用整个输入空间，或者在必要时关注更少的元素。&lt;/p&gt;
&lt;p&gt;每个卷积核的参数为 $W \in \mathbb{R}^{2d \times kd}$，$b_w \in \mathbb{R}^{2d}$，接收的输入为 $X \in \mathbb{R}^{k \times d}$，输入是 $k$ 个输入元素嵌入到 $d$ 维空间得到的向量的拼接，然后将他们映射到一个有着输入元素维数两倍的输出元素 $Y \in \mathbb{R}^{2d}$ 上；后续的层对前一层的 $k$ 个输出元素进行操作。我们选择门控线性单元（GLU; Dauphin et al., 2016）作为非线性激活单元，这是一个在卷积 $Y=[A \ B] \in \mathbb{R}^{2d}$ 的输出上实现的一个简单的门控机制：&lt;/p&gt;
$$
v([A B])=A \otimes \sigma(B)
$$&lt;p&gt;其中 $A, B \in \mathbb{R}^d$ 是非线性层的输入，$\otimes$ 是元素对元素相乘，输出 $v([A B]) \in \mathbb{R}^d$ 是 $Y$ 的大小的一半。门 $\sigma(B)$ 控制当前的上下文环境中哪个输入 $A$ 是相关的。一个相似的非线性单元由 Oord et al. (2016b) 提出，他们在 $A$ 上加了 $tanh$ 但是 Dauphin et al. (2016) 展示了GLUs在语言模型中表现的更好。&lt;/p&gt;
&lt;p&gt;为了让深度卷积网络可行，我们在每个卷积的输入和块的输出间加入了残差连接（He et al., 2015a）。&lt;/p&gt;
$$
h^l\_i=v(W^l[h^{l-1}\_{i-k/2}, ..., h^{l-1}\_{i+k/2}] + b^l\_w) + h^{l-1}\_i
$$&lt;p&gt;对于编码器网络，我们确信卷积层的输出通过在每层增加 padding 可以匹配输入长度。然而，对于解码器网络我们需要注意对于解码器来说没有可用的未来信息（Oord et al., 2016a）。详细来说就是，我们在输入的左右两侧都加了 $k-1$ 个 0 作为 padding，并且从卷积输出的末端删除了 $k$ 个元素。&lt;/p&gt;
&lt;p&gt;我们也在嵌入的大小 $f$ 和 卷积的输出，也就是大小为 $2d$ 的空间中做了线性映射关系。我们在将嵌入表示输入到编码器的时候，对 $\bf{w}$ 使用了这样的变换，也对编码器输出 $z^y_j$，解码器在 softmax $\bf{h^L}$ 之前的最后一层，以及计算注意力值之前的解码器的每一层$h^l$。&lt;/p&gt;
&lt;p&gt;最后，我们计算了在 $T$ 个可能的下一个目标元素 $y_{i+1}$ 上的分布，通过将解码器最上面的输出做权重为 $W_o$ 偏置为 $b_o$ 的线性组合得到：&lt;/p&gt;
$$
p(y\_{i+1} \mid y\_1, ..., y\_i, \mathbf{x}) = \text{softmax} (W\_oh^L\_i+b\_o) \in \mathbb{R}^T
$$&lt;h2 id="33-multi-step-attention"&gt;3.3. Multi-step Attention
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/convolutional-sequence-to-sequence-learning/Fig1.JPG"
loading="lazy"
alt="Figure1"
&gt;&lt;/p&gt;
&lt;p&gt;我们对于每一个解码层引入了一个分开的注意力机制。为了计算注意力，我们融合了当前解码器状态$h^l_i$和前一个目标元素$g_i$的嵌入表示：
&lt;/p&gt;
$$d^l\_i=W^l\_dh^l\_i + b^l\_d + g\_i$$&lt;p&gt;
解码器层$l$的注意力$a^l_{ij}$的状态$i$和源元素$j$是通过解码器状态的汇总$d^l_i$和最后一个编码器块$u$的每个输出$z^u_j$的点乘得到：
&lt;/p&gt;
$$a^l\_{ij}=\frac{\exp(d^l\_i \cdot z^u\_j)}{\sum^m\_{t=1}\exp(d^l\_i \cdot z^u\_t)}$$&lt;p&gt;
对当前解码器层的条件输入$c^l_i$是编码器输出的加权求和，也就是输入元素嵌入$e_j$（图1，中右侧）：
&lt;/p&gt;
$$c^l\_i=\sum^m\_{j=1}a^l\_{ij}(z^u\_j+e\_j)$$&lt;p&gt;
这与循环神经网络的方法稍有不同，在循环神经网络中，注意力和$z^u_j$的加权求和是同时计算的(This is slightly different to recurrent approaches which compute both the attention and the weighted sum over $z^u_j$ only)。我们发现加入$e_j$更有效，而且它与键值记忆网络相似，后者的键是$z^u_j$，值是$z^u_j+e_j$（Miller et al., 2016）。编码器输出$z^u_j$潜在地表达了大量的输入上下文，$e_j$提供了一个在做预测时有效的输入元素的点信息。一旦$c^l_i$被计算得出，它就会被简单的加到对应解码层$h^l_i$的输出上。
对比单步注意力机制（Bahdanau et al., 2014; Luong et al., 2015; Zhou et al., 2016; Wu et al., 2016）来说，这可以被看作是多步“跳跃”（Sukhbaater et al., 2015）的注意力机制。特别地，第一层的注意力决定了被输入到第二层的一个有效的源上下文，第二层在计算注意力的时候考虑了这个信息。解码器也可以直接获取注意力的前$k-1$步历史，因为条件输入$c^{l-1}_{i-k}, &amp;hellip;, c^{l-1}_{i}$是$h^{l-1}_{i-k}, &amp;hellip;, h^{l-1}_i$的一部分，而后者是$h^l_i$的输入。对于循环神经网络来说，前几步的输入在隐藏状态中，并且需要经过多个非线性单元后还保存下来，但是对于卷积的网络来说，考虑已经被加入的前几步信息更加简单。总的来说，我们的注意力机制考虑了我们之前已经加入的哪个单词，并且每个时间步都实现了多次跳跃注意力机制。在后记$C$中，我们绘制了对于深层解码器的注意力分数，显示出了不同层，被考虑的源输入的不同位置。
我们的卷积架构与RNN相比可以批量的计算一个序列的所有元素的注意力（图1，中间）。我们分别地计算了每个解码器层。&lt;/p&gt;
&lt;h3 id="正则化的策略"&gt;正则化的策略
&lt;/h3&gt;&lt;p&gt;我们通过小心的权重初始化稳定了学习过程$\text{\S3.5}$，并且通过缩放网络的部分使透过网络中的方差不会剧烈的变化。特别地，我们也对残差块的输出和注意力进行缩放来保持激活后的方差。我们将输入的加和和一个残差块的输出乘以了$\sqrt{0.5}$，来使和的方差减半。这假设了被加数有着相同的方差，虽说不一定总是能这样，但是在使用的时候还是很有效的。
通过注意力机制生成的条件输入$c^l_i$是$m$个向量的加权求和，我们通过缩放$m\sqrt{1/m}$抵消了在方差上的一个变化；我们假设注意力分数是均匀分布的，对输入乘以了$m$来使他从原来的大小放大。一般我们不这么干，但是我们发现在实际情况中表现得还挺好。
对于带有多个注意力机制的卷积解码器，我们根据我们的使用的注意力机制的数量对编码层的梯度进行缩放；我们排除了源单词的嵌入。我们发现这样可以稳定训练过程，因为如果不这样编码器就会接受到很大的梯度。&lt;/p&gt;
&lt;h3 id="初始化"&gt;初始化
&lt;/h3&gt;&lt;p&gt;在对不同层的输出进行加和的时候，比如残差连接，对激活单元进行归一化时需要很小心的权重初始化。我们的初始化策略是受到了归一化的启发：保持网络前向和反向传播时激活后的值的方差。所有的嵌入表示从一个均值为0，标准差为0.1的正态分布中初始化得到。对于输出不直接输入到门控线性单元的层，我们从$\mathcal{N}(0, \sqrt{1/n_l})$中初始化权重，其中$n_l$是每个神经元输入连接数。这确保了一个均匀分布输入的方差是保持不变的。
对于通过GLU激活的层来说，我们提出了一个通过adapting the derivations in（He et al., 2015b; Glorot &amp;amp; Bengio, 2010; Appendix A）的初始化机制。如果GLU的输入服从均值为0的分布，并且有充分小的方差，那么我们可以用输入方差的四分之一来近似输出方差（Appendix A.1）。因此，我们初始化权重，使得GLU激活的输入有着4倍的输入的方差。这可以通过对$\mathcal{N}(0, \sqrt{r/n_l})$采样初始化得到。偏置项在网络构建时均匀的设置为0。
我们在某些层的输入使用了dropout，以便输入保持一个概率$p$。这可以看作是一个伯努利随机变量取值为$1/p$的概率为$p$，其他为0（Srivastava et al., 2014）。dropout的应用会使得方差被$1/p$缩放。我们的目标是通过使用大的权重来初始化各个层来恢复进入的方差(We aim to restore the incoming variance by initializing the respective layers with larger weights)。特别地，我们使用$\mathcal{N}(0, \sqrt{4p/n_l})$初始化那些输出会输入至GLU的层，使用$\mathcal{N}(0, \sqrt{p/n_l})$来初始化其他的。&lt;/p&gt;</description></item><item><title>Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic</title><link>https://davidham3.github.io/blog/p/spatio-temporal-graph-convolutional-networks-a-deep-learning-framework-for-traffic/</link><pubDate>Thu, 10 May 2018 15:35:47 +0000</pubDate><guid>https://davidham3.github.io/blog/p/spatio-temporal-graph-convolutional-networks-a-deep-learning-framework-for-traffic/</guid><description>&lt;p&gt;IJCAI 2018，大体思路：使用Kipf &amp;amp; Welling 2017的近似谱图卷积得到的图卷积作为空间上的卷积操作，时间上使用一维卷积对所有顶点进行卷积，两者交替进行，组成了时空卷积块，在加州PeMS和北京市的两个数据集上做了验证。但是图的构建方法并不是基于实际路网，而是通过数学方法构建了一个基于距离关系的网络。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1709.04875v4" target="_blank" rel="noopener"
&gt;Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="摘要"&gt;摘要
&lt;/h1&gt;&lt;p&gt;实时精确的交通预测对城市交通管控和引导很重要。由于交通流的强非线性以及复杂性，传统方法并不能满足中长期预测的要求，而且传统方法经常忽略对时空数据的依赖。在这篇文章中，我们提出了一个新的深度学习框架，时空图卷积(Spatio-Temporal Graph Convolutional Networks)，来解决交通领域的时间序列预测问题。我们在图上将问题形式化，并且建立了完全卷积的结构，并不是直接应用传统的卷积以及循环神经单元，这可以让训练速度更快，参数更少。实验结果显示通过在多尺度的交通网络上建模，STGCN模型可以有效地捕获到很全面的时空相关性并且在各种真实数据集上表现的要比很多state-of-the-art算法好。&lt;/p&gt;
&lt;h1 id="引言"&gt;引言
&lt;/h1&gt;&lt;p&gt;交通运输在每个人的生活中都扮演着重要的角色。根据2015年的调查，美国的司机们平均每天要在车上呆48分钟。这种情况下，精确的实时交通状况预测对于路上的用户，private sector和政府来说变得至关重要。广泛使用的交通服务，如交通流控制、路线规划和导航，也依赖于高质量的交通状况预测。总的来说，多尺度的交通预测的研究很有前景而且是城市交通流控制和引导的基础，也是智能交通系统的一个主要功能。&lt;/p&gt;
&lt;p&gt;在交通研究中，交通流的基本变量，也就是速度、流量和密度，通常作为监控当前交通状态以及未来预测的指示指标。根据预测的长度，交通预测大体分为两个尺度：短期(5~30min)，中和长期预测(超过30min)。大多数流行的统计方法(比如，线性回归)可以在短期预测上表现的很好。然而，由于交通流的不确定性和复杂性，这些方法在相对长期的预测上不是那么的有效。&lt;/p&gt;
&lt;p&gt;之前在中长期交通预测上的研究可以大体的分为两类：动态建模和数据驱动的方法。动态建模使用了数学工具（比如微分方程）和物理知识通过计算模拟来形式化交通问题[Vlahogiani, 2015]。为了达到一个稳定的状态，模拟进程不仅需要复杂的系统编程，还需要消耗大量的计算资源。模型中不切实际的假设和化简也会降低预测的精度。因此，随着交通数据收集和存储技术的快速发展，一大群研究者正在将他们的目光投向数据驱动的方法。
典型的统计学和机器学习模型是数据驱动方法的两种体现。在时间序列分析上，自回归移动平均模型（ARIMA）和它的变形是众多统一的方法中基于传统统计学的方法[Ahmed and Cook, 1979; Williams and Hoel, 2003; Lippi $et al.$, 2013]。然而，这种类型的模型受限于时间序列的平稳分布，而且不能考虑时空相关性。因此，这些方法限制了高度非线性的交通流的表示能力。最近，传统的统计方法在交通预测上已经受到了机器学习方法的冲击。这些模型可以获得更高的精度，对更复杂的数据建模，比如k近邻（KNN），支持向量机（SVM）和神经网络（NN）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;深度学习方法&lt;/strong&gt; 最近，深度学习已经被广泛且成功地应用于各式各样的交通任务中，在最近的工作中已经取得了很显著的成果，比如，深度置信网络（DBN）[Jia &lt;em&gt;et al.&lt;/em&gt;, 2016; Huang &lt;em&gt;et al.&lt;/em&gt;, 2014]和层叠自编码器(stacked autoencoder)(SAE)[Lv &lt;em&gt;et al.&lt;/em&gt;, 2015; Chen &lt;em&gt;et al.&lt;/em&gt;, 2016]。然而，这些全连接神经网络很难从输入中提取空间和时间特征。而且，空间属性的严格限制甚至完全缺失，这些网络的表示能力被限制的很严重。&lt;/p&gt;
&lt;p&gt;为了充分利用空间特征，一些研究者使用了卷积神经网络来捕获交通网络中的临近信息，同时也在时间轴上部署了循环神经网络。通过组合长短期记忆网络[Hochreiter and Schmidhuber, 1997]和1维卷积，Wu和Tan[2016]首先提出一个特征层面融合的架构CLTFP来预测短期交通状况。尽管它采取了一个很简单的策略，CLTFP仍然是第一个尝试对时间和空间规律性对齐的方法。后来，Shi &lt;em&gt;et al.&lt;/em&gt;[2015]提出了卷积LSTM，这是一个带有嵌入卷积层的全连接LSTM的扩展。然而，常规的卷积操作限制了模型只能处理常规的网格结构（如图像或视频），而不是其他的大部分领域（比如Graph）。与此同时，循环神经网络对于序列的学习需要迭代训练，这会导致误差的积累。更进一步地说，循环神经网络（包括基于LSTM的RNN）的难以训练和计算量大是众所周知的。&lt;/p&gt;
&lt;p&gt;为了克服这些问题，我们引入了一些策略来有效的对交通流的时间动态和空间依赖进行建模。为了完全利用空间信息，我们通过一个广义图对交通网络建模，而不是将交通流看成各个离散的部分（比如网格或碎块）。为了处理循环神经网络的缺陷，我们在时间轴上部署了一个全卷积结构来阻止累积效应（cumulative effects）并且加速模型的训练过程。综上所述，我们提出了一个新的神经网络架构，时空图卷积网络，来预测交通情况。这个架构由多个时空图卷积块组成，这些都是图卷积层和卷积序列学习层（convolutional sequence learning layers）的组合，用来对时间和空间依赖关系进行建模。&lt;/p&gt;
&lt;p&gt;我们的主要贡献可以归纳为以下三点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们研究了在交通领域时间与空间依赖结合的好处。为了充分利用我们的知识，这是在交通研究中第一次应用纯卷积层来同时从图结构的时间序列中提取时空信息。&lt;/li&gt;
&lt;li&gt;我们提出了一个新的由时空块组成的神经网络结构。由于这个架构中是纯卷积操作，它比基于RNN的模型的训练速度快10倍以上，而且需要的参数更少。这个架构可以让我们更有效地处理更大的路网，这部分将在第四部分展示。&lt;/li&gt;
&lt;li&gt;我们在两个真实交通数据集上验证了提出来的网络。这个实验显示出我们的框架比已经存在的在多长度预测和网络尺度上的模型表现的更好。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="准备工作"&gt;准备工作
&lt;/h1&gt;&lt;h2 id="路网上的交通预测"&gt;路网上的交通预测
&lt;/h2&gt;&lt;p&gt;交通预测是一个典型的时间序列预测问题，也就是预测在给定前M个观测样本接下来H个时间戳后最可能的交通流指标（比如速度或交通流），&lt;/p&gt;
&lt;p&gt;$$&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;\tag{1} \hat{v}\_{t+1}, ..., \hat{v}\_{t+H} = \mathop{\arg\min}\_{v\_{t+1},...,v\_{t+H}}logP(v\_{t+1},...,v\_{t+H}\vert v\_{t-M+1},...v\_t)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$$&lt;/p&gt;
&lt;p&gt;这里$v_t \in \mathbb{R}^n$是$n$个路段在时间戳$t$观察到的一个向量，每个元素记录了一条路段的历史观测数据。&lt;/p&gt;
&lt;p&gt;在我们的工作中，我们在一个图上定义了一个交通网络，并专注于结构化的交通时间序列。观测到的样本$v_t$间不是相互独立的，而是在图中两两相互连接的。因此，数据点$v_t$可以被视为定义在权重为$w_{ij}$，如图1展示的无向图（或有向图）$\mathcal{G}$上的一个信号。在第$t$个时间戳，在图$\mathcal{G_t}=(\mathcal{V_t}, \mathcal{\varepsilon}, W)$, $\mathcal{V_t}$是当顶点的有限集，对应在交通网络中$n$个监测站；$\epsilon$是边集，表示观测站之间的连通性；$W \in \mathbb{R^{n \times n}}$表示$\mathcal{G_t}$的邻接矩阵。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/spatio-temporal-graph-convolutional-networks-a-deep-learning-framework-for-traffic/Fig1.PNG"
loading="lazy"
alt="Fig1"
&gt;&lt;/p&gt;
&lt;h2 id="图上的卷积"&gt;图上的卷积
&lt;/h2&gt;&lt;p&gt;传统网格上的标准卷积很明显是不能应用在广义图上的。现在有两个基本的方法正在探索如何泛化结构化数据上的CNN。一个是扩展卷积的空间定义[Niepert &lt;em&gt;et al.&lt;/em&gt;, 2016]，另一个是使用图傅里叶变换在谱域中进行操作[Bruna &lt;em&gt;et al.&lt;/em&gt;, 2013]。前一个方法重新将顶点安排至确定的表格形式内，然后就可以使用传统的卷积方法了。后者引入了谱框架，在谱域中应用图卷积，经常被称为谱图卷积。一些后续的研究通过将时间复杂度从$O(n^2)$降至线性[Defferrard &lt;em&gt;et al.&lt;/em&gt;, 2016;Kipf and Welling, 2016]使谱图卷积的效果更好。
我们基于谱图卷积的定义引入图卷积操作“$\ast_{\mathcal{G}}$”的符号，也就是一个核$\Theta$和信号$x \in \mathbb{R}^n$的乘法，&lt;/p&gt;
$$\tag{2} \Theta \ast\_{\mathcal{G}}x=\Theta(L)x=\Theta(U \Lambda U^T)x=U\Theta(\Lambda)U^Tx$$&lt;p&gt;这里图的傅里叶基$U \in \mathbb{R}^{n \times n}$是归一化的拉普拉斯矩阵$L=I_n-D^{-1/2}WD^{-1/2}= U \Lambda U^T \in \mathbb{R}^{n \times n}$的特征向量组成的矩阵，其中$I_n$是单位阵，$D \in \mathbb{R}^{n \times n}$是对角的度矩阵$D_{ii}=\sum_j{W_{ij}}$；$\Lambda \in \mathbb{R}^{n \times n}$是$L$的特征值组成的矩阵，卷积核$\Theta(\Lambda)$是一个对角矩阵。通过这个定义，一个图信号$x$是被一个核$\Theta$通过$\Theta$和图傅里叶变换$U^Tx$[Shuman &lt;em&gt;et al.&lt;/em&gt;, 2013]过滤的。&lt;/p&gt;
&lt;h1 id="提出的模型"&gt;提出的模型
&lt;/h1&gt;&lt;h2 id="网络架构"&gt;网络架构
&lt;/h2&gt;&lt;p&gt;在这部分，我们详细说明了时空图卷积网络的框架。如图二所示，STGCN有多个时空卷积块组成，每一个都是像一个“三明治”结构的组成，有两个门序列卷积层和一个空间图卷积层在中间。每个模块的细节如下。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/spatio-temporal-graph-convolutional-networks-a-deep-learning-framework-for-traffic/Fig2.PNG"
loading="lazy"
alt="Fig2"
&gt;&lt;/p&gt;
&lt;p&gt;图二：时空图卷积网络的架构图。STGCN的架构有两个时空卷积块和一个全连接的在末尾的输出层组成。每个ST-Conv块包含了两个时间门卷积层，中间有一个空间图卷积层。每个块中都使用了残差连接和bottleneck策略。输入$v_{t-M+1},&amp;hellip;v_t$被ST-Conv块均匀的（uniformly）处理，来获取时空依赖关系。全部特征由一个输出层来整合，生成最后的预测$\hat{v}$。&lt;/p&gt;
&lt;h2 id="提取空间特征的图卷积神经网络"&gt;提取空间特征的图卷积神经网络
&lt;/h2&gt;&lt;p&gt;交通网络大体上是一个图结构。由数学上的图来构成路网是很自然也很合理的。然而，之前的研究忽视了交通网络的空间属性：因为交通网络被分成了块或网格状，所以网络的全局性和连通性被过分的关注了。即使是在网格上的二维卷积，由于数据建模的折中，也只能捕捉到大体的空间局部性。根据以上情况，在我们的模型中，图卷积被直接的应用在了图结构数据上为了在空间中抽取很有意义的模式和特征。集是在图卷积中由式2可以看出核$\Theta$的计算的时间复杂度由于傅里叶基的乘法可以达到$O(n^2)$，两个近似的策略可以解决这个问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;切比雪夫多项式趋近&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;为了局部化过滤器并且减少参数，核$\Theta$可以被一个关于$\Lambda$的多项式限制起来，也就是$\Theta(\Lambda)=\sum_{k=0}^{K-1} \theta_k \Lambda^k$，其中$\theta \in \mathbb{R}^K$是一个多项式系数的向量。$K$是图卷积核的大小，它决定了卷积从中心节点开始的最大半径。一般来说，切比雪夫多项式$T_k(x)$被用于近似核，作为$K-1$阶展开的一部分，也就是$\Theta(\Lambda) \approx \sum_{k=0}^{K-1} \theta_k T_k(\widetilde{\Lambda})$，其中$\widetilde{\Lambda}=2\Lambda/\lambda_{max}-I_n$（$\lambda_{max}$表示$L$的最大特征值）[Hammond &lt;em&gt;et al.&lt;/em&gt;, 2011]。图卷积因此可以被写成&lt;/p&gt;
$$
\tag{3} \Theta \ast\_{\mathcal{G}} x = \Theta(L)x \approx \sum\_{k=0}^{K-1}\theta\_k T\_k(\widetilde{L})x
$$&lt;p&gt;其中$T_k(\widetilde{L}) \in \mathbb{R}^{n \times n}$是k阶切比雪夫多项式对缩放后（scaled）的拉普拉斯矩阵$\widetilde{L}=2L/\lambda_{max}-I_n$。通过递归地使用趋近后的切比雪夫多项式计算K阶卷积操作，式2的复杂度可以被降低至$O(K\vert \varepsilon \vert)$，如式3所示[Defferrard &lt;em&gt;et al.&lt;/em&gt;, 2016]。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1阶近似&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一个针对层的线性公式可以由堆叠多个使用拉普拉斯矩阵的一阶近似的局部图卷积层[Kipf and Welling, 2016]。结果就是，这样可以构建出一个深的网络，这个网络可以深入地恢复空间信息并且不需要指定多项式中的参数。由于在神经网络中要缩放和归一化，我们可以进一步假设$\lambda_{max} \approx 2$。因此，式3可以简写为&lt;/p&gt;
$$
\begin{aligned}
\Theta \ast\_{\mathcal{G}}x \approx &amp; \theta\_0x+\theta\_1(\frac{2}{\lambda\_{max}}L-I\_n)x\\
\approx &amp; \theta\_0 x- \theta\_1(D^{-\frac{1}{2}} W D^{-\frac{1}{2}}) x
\end{aligned}
$$&lt;p&gt;其中，$\theta_0$，$\theta_1$是核的两个共享参数。为了约束参数并为稳定数值计算，$\theta_0$和$\theta_1$用一个参数$\theta$来替换，$\theta=\theta_0=-\theta_1$；$W$和$D$是通过$\widetilde{W}=W+I_n$和$\widetilde{D}_{ii}=\sum_j\widetilde{W}_{ij}$重新归一化得到的。之后，图卷积就可以表达为&lt;/p&gt;
$$
\begin{aligned}
\Theta \ast\_{\mathcal{G}} x = &amp; \theta(I\_n + D^{-\frac{1}{2}} W D^{\frac{1}{2}})x\\
= &amp; \theta (\widetilde{D}^{-\frac{1}{2}} \widetilde{W} \widetilde{D}^{-\frac{1}{2}})x
\end{aligned}
$$&lt;p&gt;竖直地堆叠一阶近似的图卷积可以获得和平行的K阶卷积相同的效果，所有的卷积可以从一个顶点的$K-1$阶邻居中获取到信息。在这里，$K$是连续卷积操作的次数或是模型中的卷积层数。进一步说，针对层的线性结构是节省参数的，并且对大型的图来说是效率很高的，因为多项式趋近的阶数为1。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;图卷积的泛化&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;图卷积操作$\ast_{\mathcal{G}}$也可以被扩展到多维张量上。对于一个有着$C_i$个通道的信号$X \in \mathbb{R}^{n \times C_i}$，图卷积操作可以扩展为&lt;/p&gt;
$$
y\_j = \sum\_{i=1}^{C\_i} \Theta\_{i,j}(L) x\_i \in \mathbb{R}^n, 1 \leq j \leq C\_o
$$&lt;p&gt;其中，$C_i \times C_o$个向量是切比雪夫系数$\Theta_{i,j} \in \mathbb{R}^K$（$C_i$，$C_o$分别是feature map的输入和输出大小）。针对二维变量的图卷积表示为$\Theta \ast_{\mathcal{G}} X$，其中$\Theta \in \mathbb{R}^{K \times C_i \times C_o}$。需要注意的是，输入的交通预测是由$M$帧路网组成的，如图1所示。每帧$v_t$可以被视为一个矩阵，它的第$i$列是图$\mathcal{G_t}$中第$i$个顶点的一个为$C_i$维的值，也就是$X \in \mathbb{R}^{n \times C_i}$（在这个例子中，$C_i=1$）。对于$M$中的每个时间步$t$，相同的核与相同的图卷积操作是在$X_t \in \mathbb{R}^{n \times C_i}$中并行进行的。因此，图卷积操作也可以泛化至三维，记为$\Theta \ast_{\mathcal{G}} \mathcal{X}$，其中$\mathcal{X} \in \mathbb{R}^{M \times n \times C_i}$&lt;/p&gt;
&lt;h2 id="抽取时间特征的门控卷积神经网络"&gt;抽取时间特征的门控卷积神经网络
&lt;/h2&gt;&lt;p&gt;尽管基于RNN的模型可以广泛的应用于时间序列分析，用于交通预测的循环神经网络仍然会遇到费时的迭代，复杂的门控机制，对动态变化的响应慢。相反，CNN训练快，结构简单，而且不依赖于前一步。受到[Gehring &lt;em&gt;et al.&lt;/em&gt;, 2017]的启发，我们在时间轴上部署了整块的卷积结构，用来捕获交通流的动态时间特征。这个特殊的设计可以让并行而且可控的训练过程通过多层卷积结构形成层次表示。&lt;/p&gt;
&lt;p&gt;如图2右侧所示，时间卷积层包含了一个一维卷积，核的宽度为$K_t$，之后接了一个门控线性单元(GLU)作为激活。对于图$\mathcal{G}$中的每个顶点，时间卷积对输入元素的$K_t$个邻居进行操作，导致每次将序列长度缩短$K_t-1$。因此，每个顶点的时间卷积的输入可以被看做是一个长度为$M$的序列，有着$C_i$个通道，记作$Y \in \mathbb{R}^{M \times C_i}$。卷积核$\Gamma \in \mathbf{R}^{K_t \times C_i \times 2C_o}$是被设计为映射$Y$到一个单个的输出$[P Q] \in \mathbb{R}^{(M-K_t+1) \times (2C_o)}$($P$, $Q$是通道数的一半)。作为结果，时间门控卷积可以定义为：&lt;/p&gt;
$$
\Gamma \ast\_ \tau Y = P \otimes \sigma (Q) \in \mathbb{R}^{(M-K\_t+1) \times C\_o}
$$&lt;p&gt;其中，$P$, $Q$分别是GLU的输入门，$\otimes$表示哈达玛积，sigmoid门$\sigma(Q)$控制当前状态的哪个输入$P$对于发现时间序列中的组成结构和动态方差是相关的。非线性门通过堆叠时间层对挖掘输入也有贡献。除此以外，在堆叠时间卷积层时，实现了残差连接。相似地，通过在每个节点$\mathcal{Y_i} \in \mathbb{R}^{M \times C_i}$(比如监测站)上都使用同样的卷积核$\Gamma$，时间卷积就可以泛化至3D变量上，记作$\Gamma \ast_\tau \mathcal{Y}$，其中$\mathcal{Y} \in \mathbb{R}^{M \times n \times C_i}$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这里我之前认为残差是用了 padding 的，其实不是，看了作者的代码后发现作者是用了一半数量的卷积核完成卷积，这样就和 P 的维度一致了，然后直接和 P 相加，然后与 sigmoid 激活后的值进行点对点的相乘。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="时空卷积块"&gt;时空卷积块
&lt;/h2&gt;&lt;p&gt;为了同时从空间和时间领域融合特征，时空卷积块(ST-Conv block)的构建是为了同时处理图结构的时间序列的。如图2（中）所示，bottleneck策略的应用形成了三明治的结构，其中含有两个时间门控卷积层，分别在上下两层，一个空间图卷积层填充中间的部分。空间卷积层导致的通道数$C$的减小促使了参数的减少，并且减少了训练的时间开销。除此以外，每个时空块都使用了层归一化来抑制过拟合。&lt;/p&gt;
&lt;p&gt;ST-Conv块的输入和输出都是3D张量。对于块$l$的输入$v^l \in \mathbb{R}^{M \times n \times C^l}$，输出$v^{l+1} \in \mathbb{R}^{(M-2(K_t-1)) \times n \times C^{l+1}}$通过以下式子计算得到：&lt;/p&gt;
$$
v^{l+1} = \Gamma^l\_1 \ast\_\tau \rm ReLU(\Theta^l \ast\_{\mathcal{G}}(\Gamma^l\_0 \ast\_\tau v^l))
$$&lt;p&gt;其中$\Gamma^l_0$，$\Gamma^l_1$是块$l$的上下两个时间层；$\Theta^l$是图卷积谱核；$\rm ReLU(·)$表示ReLU激活函数。我们在堆叠两个ST-Conv块后，加了一个额外的时间卷积和全连接层作为最后的输出层（图2左侧）。时间卷积层将最后一个ST-Conv块的输出映射到一个最终的单步预测上。之后，我们可以从模型获得一个最后的输出$Z \in \mathbb{R}^{n \times c}$，通过一个跨$c$个通道的线性变换$\hat{v} = Zw+b$来预测$n$个节点的速度，其中$w \in \mathbb{R}^c$是权重向量,$b$是偏置。对交通预测的STGCN的损失函数可以写成：&lt;/p&gt;
$$
L(\hat{v}; W\_\theta) = \sum\_t \Vert \hat{v}(v\_{t-M+1, ..., v\_t, W\_\theta}) - v\_{t+1} \Vert^2
$$&lt;p&gt;其中，$W_\theta$是模型中所有的训练参数; $v_{t+1}$是ground truth，$\hat{v}(·)$表示模型的预测。&lt;/p&gt;
&lt;p&gt;我们来总结一下我们的STGCN的主要特征：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;STGCN是处理结构化的时间序列的通用框架，不仅可以解决交通网络建模，还可以应用到其他的时空序列学习的挑战中，比如社交网络和推荐系统。&lt;/li&gt;
&lt;li&gt;时空块融合了图卷积和门控时间卷积，可以同时抽取有用的空间信息，捕获本质上的时间特征。&lt;/li&gt;
&lt;li&gt;模型完全由卷积层组成，因此可以在输入序列上并行运算，空间域中参数少易于训练。更重要的是，这个经济的架构可以使模型更高效的处理大规模的网络。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="实验"&gt;实验
&lt;/h1&gt;&lt;h2 id="数据集描述"&gt;数据集描述
&lt;/h2&gt;&lt;p&gt;我们在两个真实的数据集上验证了模型，分别是&lt;strong&gt;BJER4&lt;/strong&gt;和&lt;strong&gt;PeMSD7&lt;/strong&gt;，由北京市交委和加利福尼亚运输部提供。每个数据集包含了交通观测数据的关键属性和对应时间的地图信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BJER4&lt;/strong&gt;是通过double-loop detector获取的东四环周边的数据。我们的实验中有12条道路。交通数据每五分钟聚合一次。时间是从2014年的7月1日到8月31日，不含周末。我们选取了第一个月的车速速度记录作为训练集，剩下的分别做验证和测试。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PeMSD7&lt;/strong&gt;是Caltrans Performance Measurement System(PeMS)通过超过39000个监测站实时获取的数据，这些监测站分布在加州高速公路系统主要的都市部分[Chen &lt;em&gt;et al&lt;/em&gt;., 2001]。数据是30秒的数据样本聚合成5分钟一次的数据。我们在加州的District 7随机选取了一个小的和一个大的范围作为数据源，分别有228和1026个监测站，分别命名为PeMSD7(S)和PeMSD7(L)（如图3左侧所示）。PeMSD7数据集的时间范围是2012年五月和六月的周末。我们使用同样的原则对数据进行了训练集和测试集的划分。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/spatio-temporal-graph-convolutional-networks-a-deep-learning-framework-for-traffic/Fig3.PNG"
loading="lazy"
alt="Fig1"
&gt;&lt;/p&gt;
&lt;h2 id="数据预处理"&gt;数据预处理
&lt;/h2&gt;&lt;p&gt;两个数据集的间隔设定为5分钟。因此，路网中的每个顶点每天就有288个数据点。数据清理后使用了线性插值的方法来填补缺失值。通过核对相关性，每条路的方向和OD(origin-destination)点，环路系统可以被数值化成一个有向图。&lt;/p&gt;
&lt;p&gt;在PeMSD7，路网的邻接矩阵通过交通网络中的监测站的距离来计算。带权邻接矩阵$W$通过以下公式计算：&lt;/p&gt;
$$
w\_{ij} = \begin{cases}
\exp{(-\frac{d^2\_{ij}}{\sigma^2})}&amp;,i \neq j \ \rm and \exp{(-\frac{d^2\_{ij}}{\sigma^2}) \geq \epsilon} \\
0&amp;, \rm otherwise
\end{cases}
$$&lt;p&gt;其中$w_{ij}$是边的权重，通过$d_{ij}$得到，也就是$i$和$j$之间的距离。$\sigma^2$和$\epsilon$是来控制矩阵$W$的分布和稀疏性的阈值，我们用了10和0.5。$W$的可视化在图3的右侧。&lt;/p&gt;
&lt;h1 id="代码"&gt;代码
&lt;/h1&gt;&lt;p&gt;&lt;a class="link" href="https://github.com/VeritasYin/STGCN_IJCAI-18" target="_blank" rel="noopener"
&gt;作者代码&lt;/a&gt;，这个是作者提供的代码。&lt;/p&gt;
&lt;p&gt;&lt;a class="link" href="https://github.com/Davidham3/STGCN" target="_blank" rel="noopener"
&gt;仓库地址&lt;/a&gt;，我按照论文结合作者的代码进行了复现与修正。&lt;/p&gt;</description></item><item><title>Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title><link>https://davidham3.github.io/blog/p/spatial-temporal-graph-convolutional-networks-for-skeleton-based-action-recognition/</link><pubDate>Wed, 18 Apr 2018 10:43:36 +0000</pubDate><guid>https://davidham3.github.io/blog/p/spatial-temporal-graph-convolutional-networks-for-skeleton-based-action-recognition/</guid><description>&lt;p&gt;AAAI 2018，以人体关节为图的顶点，构建空间上的图，然后通过时间上的关系，连接连续帧上相同的关节，构成一个三维的时空图。针对每个顶点，对其邻居进行子集划分，每个子集乘以对应的权重向量，得到时空图上的卷积定义。实现时使用Kipf &amp;amp; Welling 2017的方法实现。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1801.07455" target="_blank" rel="noopener"
&gt;Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="摘要"&gt;摘要
&lt;/h1&gt;&lt;p&gt;人体骨骼的动态传递了用于人体动作识别的很多信息。传统方法需要手工和遍历规则，导致表现力的限制和泛化的困难。我们提出了动态骨骼识别的新模型，STGCN，可以从数据中自动学习时空模式。这套理论有很强的表达能力与泛化能力。在两个大型数据集Kinetics和NTU-RGBD上比主流方法表现的更好。&lt;/p&gt;
&lt;h1 id="1-引言"&gt;1 引言
&lt;/h1&gt;&lt;p&gt;动作识别在视频理解中很有用。一般，从多个角度识别人体动作，如外表、景深、光源、骨骼。对骨骼建模受到的关注较外表和光源较少，我们系统的研究了这个模态，目的是研发出一个有效的对动态骨骼建模的方法，服务于动作识别。&lt;/p&gt;
&lt;p&gt;动态骨骼模态很自然地表示成人体关节的时间序列，以2D或3D坐标的形式。人体动作可以通过分析移动规律来识别。早期的工作只用每帧的关节坐标生成特征向量，然后使用空间分析（Wang et al., 2012; Fernando et al., 2015）。这些方法能力受限的原因是他们没有挖掘关节之间的空间信息，但是这些信息对于理解人体动作来说很关键。最近，新的方法尝试利用关节间的自然连接关系(Shahroudy et al., 2016; Du, Wang, and Wang 2015)。这些方法都有提升，表明了连接的重要性。然而，很多显存的方法依赖手工的部分或是分析空间模式的规则。结果导致针对特定问题设计的模型不能泛化。&lt;/p&gt;
&lt;p&gt;为了跨越这些限制，我们需要一个新的方法能自动捕获关节的空间配置与时间动态性中嵌入的模式。这就是深度神经网络的优势了。由于骨骼是图结构，不是2D或3D网格，因此传统的CNN不行，最近GCN已经成功的应用在了一些应用上，如图像分类(Bruna et al., 2014)，文档分类(Defferrard, Bresson, and Vandergheynst 2016)，还有半监督学习(Kipf and Welling 2017)。然而，这些工作都假设一个固定的图作为输入。GCN的应用在大尺度的数据集上对动态图建模，如人体骨骼序列还没有被挖掘过。&lt;/p&gt;
&lt;p&gt;![Fig1]](/blog/images/spatial-temporal-graph-convolutional-networks-for-skeleton-based-action-recognition/Fig1.JPG)
我们将图网络扩展到一个时空图模型来对骨骼序列进行表示后识别动作。图1所示，这个模型基于一个骨骼图序列，每个顶点表示人体的一个关节。有两种类型的边，空间边，连接关节，时间边连接连续时间的同一关节。构建在上面的时空卷积可以同时集成时间和空间上的信息。&lt;/p&gt;
&lt;p&gt;ST-GCN的层次本质消除了手工和遍历部分。不仅有更强的表达能力和更好的表现，也更简单的泛化到其他环境中。在基础的GCN公式基础上，受到图像模型的启发，我们还提出了设计图卷积核新的策略。&lt;/p&gt;
&lt;p&gt;我们的工作有三点贡献：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;提出了ST-GCN，对动态骨骼建模的基于图结构的模型，第一个应用基于图的神经网络到这个任务上。&lt;/li&gt;
&lt;li&gt;设计ST-GCN的卷积核时提出了几个原则使得在骨骼建模时满足特定的需求。&lt;/li&gt;
&lt;li&gt;在两个大尺度数据集上，我们提出的模型效果比之前的手工和遍历规则的方法强。
代码和模型：https://github.com/yysijie/st-gcn&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="2-相关工作"&gt;2 相关工作
&lt;/h1&gt;&lt;p&gt;两类方法&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;谱方法，谱分析中考虑图卷积的局部形式(Henaff, Bruna, and LeCun 2015; Duvenaud et al., 2015; Li et al., 2016; Kipf and Welling., 2017)&lt;/li&gt;
&lt;li&gt;空间方法，卷积核直接在顶点和他们的邻居上做卷积(Bruna et al., 2014; Niepert, Ahmed, and Kutzkov 2016)。我们的工作follow了第二种方法。我们在空间领域构建CNN滤波器，通过限制滤波器到每个顶点的一阶邻居上。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;骨骼动作识别&lt;/strong&gt; 方法可分为手工方法和深度学习方法。第一类是手工设计特征捕获关节移动的动态性。可以是关节轨迹的协方差矩阵(Hussein et al., 2013)，关节的相对位置(Wang et al., 2012)，身体部分的旋转和变换(Vemulapalli, Arrate, and Chellappa 2014)。深度学习的工作使用RNN(Shahroudy et al., 2016; Zhu et al. 2016; Liu et al. 2016; Zhang, Liu and Xiao 2017)和时间CNN(Li et al. 2017; Ke et al. 2017; Kim and Reiter 2017)用端到端的方式学习动作识别模型。这些方法中很多强调了将关节与身体部分建模的重要性。但这些都需要领域知识。我们的ST-GCN是第一个将图卷积用在骨骼动作识别上的。不同于之前的方法，我们的方法可以利用图卷积的局部性和时间动态性学习身体部分的信息。通过消除手工部分标注的需要，模型更容易去设计，而且能学到更好的动作表示。&lt;/p&gt;
&lt;h1 id="3-spatial-temporal-graph-convnet"&gt;3 Spatial Temporal Graph ConvNet
&lt;/h1&gt;&lt;p&gt;人们在活动的时候，关节只在一个范围内活动，这个部分称为body parts。已有的方法已经证明了将body parts融入到模型中是很有效的(Shahroudy et al., 2016; Liu et al., 2016; Zhang, Liu and Xiao 2017)。我们认为提升很有可能是因为parts将关节轨迹限制在了局部区域中。像物体识别这样的任务，层次表示和局部性通常是卷积神经网络潜在就可以获得的(Krizhevsky, Sutskever, and Hinton 2012)，而不是手动分配的。这使得我们在基于骨骼的动作识别中引入CNN的性质。结果就是ST-GCN模型的尝试。&lt;/p&gt;
&lt;h2 id="31-pipeline-overview"&gt;3.1 Pipeline Overview
&lt;/h2&gt;&lt;p&gt;骨骼数据通过动作捕捉设备和动作估计算法即可从视频中获得。通常数据是一系列的帧，每帧有一组关节坐标。给定身体关节2D或3D的坐标序列，我们构建了一个时空图，关节作图的顶点，身体结构或时间作边。ST-GCN的输入因此就是关节坐标向量。可以认为这是基于图片的CNN的近似，后者的输入是2D网格中的像素向量。多层时空图卷积操作加到输入上会生成更高等级的特征。然后使用softmax做费雷。整个模型以端到端的方式进行训练。&lt;/p&gt;
&lt;h2 id="32-骨骼图构建"&gt;3.2 骨骼图构建
&lt;/h2&gt;&lt;p&gt;骨骼序列通常表示成每帧都是人体关节的2D或3D坐标。之前使用卷积来做骨骼动作识别的工作(Kim and Reiter 2017)拼接了在每帧拼接了所有关节的坐标向量来生成一个特征向量。我们的工作中，我们利用时空图来生成骨骼序列的层次表示。特别地，我们构建了无向时空图$G = (V, E)$，$N$个关节，$T$帧描述身体内和帧与帧之间的连接。&lt;/p&gt;
&lt;p&gt;顶点集$V = \lbrace v_{ti} \mid t = 1, &amp;hellip;, T, i = 1, &amp;hellip;, N \rbrace$包含了骨骼序列中所有的关节。ST-GCN的输入，每个顶点的特征向量$F(v_{ti})$由第$t$帧的第$i$个关节的坐标向量组成，还有estimation confidence。构建时空图分为两步，第一步，一帧内的关节通过人体结构连接，如图1所示。然后每个关节在连续的帧之间连接起来。这里不需要人工干预。举个例子，Kinetics数据集，我们使用OpenPose toolbox(Cao et al., 2017b)2D动作估计生成了18个关节，而NTU-RGB+D(Shahroudy et al., 2016)数据集上使用3D关节追踪产生了25个关节。ST-GCN可以在这两种情况下工作，并且提供一致的优越性能。图1就是时空图的例子。
严格来说，边集$E$由两个子集组成，第一个子集描述了每帧骨骼内的连接，表示为$E_S = \lbrace v_{ti}v_{tj} \mid (i, j) \in H \rbrace$，$H$是自然连接的关节的结合。第二个子集是连续帧的相同关节$E_F = \lbrace v_{ti} v_{(t+1)i} \rbrace$，因此$E_F$中所有的边对于关节$i$来说表示的是它随时间变化的轨迹。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/spatial-temporal-graph-convolutional-networks-for-skeleton-based-action-recognition/Fig2.JPG"
loading="lazy"
alt="Fig2"
&gt;&lt;/p&gt;
&lt;h2 id="33-空间图卷积神经网络"&gt;3.3 空间图卷积神经网络
&lt;/h2&gt;&lt;p&gt;时间$\tau$上，$N$个关节顶点$V_t$，骨骼边集$E_S(\tau) = \lbrace v_{ti} v_{tj} \mid t = \tau, (i, j) \in H \rbrace$。图像上的2D卷积的输入和输出都上2D网格，stride设为1时，加上适当的padding，输出的size就可以不变。给定一个$K \times K$的卷积操作，输入特征$f_{in}$的channels数是$c$。在空间位置$\mathbf{x}$的单个通道的输出值可以写成：&lt;/p&gt;
$$\tag{1}
f\_{out}(\mathbf{x}) = \sum^K\_{h=1} \sum^K\_{w=1} f\_{in}(\mathbf{p}(\mathbf{x}, h, w)) \cdot \mathbf{w}(h, w)
$$&lt;p&gt;&lt;strong&gt;采样函数&lt;/strong&gt;$\mathbf{p} : Z^2 \times Z^2 \rightarrow Z^2$对$\mathbf{x}$的邻居遍历。在图像卷积中，也可表示成$\mathbf{p}(\mathbf{x}, h, w) = \mathbf{x} + \mathbf{p}&amp;rsquo;(h, w)$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;权重函数&lt;/strong&gt;$\mathbf{w}: Z^2 \rightarrow \mathbb{R}^c$提供了一个$c$维的权重向量，与采出的$c$维输入特征向量做内积。需要注意的是权重函数与输入位置$\mathbf{x}$无关。因此滤波器权重在输入图像上是共享的。图像领域标准的卷积通过对$\mathbf{p}(x)$中的举行进行编码得到。更多解释和应用可以看(Dai et al., 2017)。&lt;/p&gt;
&lt;p&gt;图上的卷积是对上式的扩展，输入是空间图$V_t$。feature map $f^t_{in}: V_t \rightarrow R^c$在图上的每个顶点有一个向量。下一步扩展是重新定义采样函数$\mathbf{p}$，权重函数是$\mathbf{w}$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;采样函数.&lt;/strong&gt; 图像中，采样函数$\mathbf{p}(h, w)$定义为中心位置$\mathbf{x}$的邻居像素。图中，我们可以定义相似的采样函数在顶点$v_{ti}$的邻居集合上$B(v_{ti}) = \lbrace v_{tj} \mid d(v_{tj}, v_{ti} \leq D \rbrace)$。这里$d(v_{tj}, t_{ti})$表示从$v_{tj}$到$v_{ti}$的任意一条路径中最短的。因此采样函数$\mathbf{p}: B(v_{ti}) \rightarrow V$可以写成：
&lt;/p&gt;
$$\tag{2}
\mathbf{p}(v\_{ti}, v\_{tj}) = v\_{tj}.
$$&lt;p&gt;
我们令$D = 1$，也就是关节的一阶邻居。更高阶的邻居会在未来的工作中实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;权重函数.&lt;/strong&gt; 对比采样函数，权重函数在定义时更巧妙。在2D卷积，网格型自然就围在了中心位置周围。所以像素与其邻居有个固定的顺序。权重函数根据空间顺序通过对维度为$(c, K, K)$的tensor添加索引来实现。对于像我们构造的这种图，没有这种暗含的关系。解决方法由(Niepert, Ahmed, and Kuzkov 2016)提出，顺序是通过根节点周围的邻居节点的标记顺序确定。我们根据这个思路构建我们的权重函数。不再给每个顶点一个标签，我们通过将顶点$v_{ti}$的邻居集合$B(v_{ti})$划分为$K$个子集来简化过程，其中每个子集都有一个数值型标签。因此我们可以得到一个映射$l_{ti}:B(v_{ti}) \rightarrow \lbrace 0,&amp;hellip;,K-1 \rbrace$，这个映射将顶点映射到它的邻居子集的标签上。权重函数$\mathbf{w}(v_{ti}, v_{tj}):B(v_{ti}) \rightarrow R^c$可以通过对维度为$(c, K)$的tensor标记索引或
&lt;/p&gt;
$$\tag{3}
\mathbf{w}(v\_{ti}, v\_{tj}) = \mathbf{w}'(l\_{ti}(v\_{tj})).
$$&lt;p&gt;
我们会在3.4节讨论分区策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;空间图卷积.&lt;/strong&gt; 我们可以将式1重写为：
&lt;/p&gt;
$$\tag{4}
f\_{out}(v\_{ti}) = \sum\_{v\_{tj} \in B(v\_{ti})} \frac{1}{Z\_{ti}(v\_{tj})} f\_{in}(\mathbf{p}(v\_{ti}, v\_{tj})) \cdot \mathbf{w}(v\_{ti}, v\_{tj}),
$$&lt;p&gt;
其中归一化项$Z_{ti}(v_{tj}) = \vert \lbrace v_{tk} \mid l_{ti}(v_{tk}) = l_{ti}(t_{tj}) \rbrace \vert$等于对应子集的基数。这项被加入是来平衡不同子集对输出的贡献。
替换式2和式3，我们可以得到
&lt;/p&gt;
$$\tag{5}
f\_{out}(v\_{ti}) = \sum\_{v\_{tj} \in B(v\_{ti})} \frac{1}{Z\_{ti}(v\_{tj})} f\_{in}(v\_{tj}) \cdot \mathbf{w}(l\_{ti}(v\_{tj})).
$$&lt;p&gt;
这个公式与标准2D卷积相似如果我们将图片看作2D网格。比如，$3 \times 3$卷积核的中心像素周围有9个像素。邻居集合应被分为9个子集，每个子集有一个像素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时空建模.&lt;/strong&gt; 通过对空间图CNN的构建，我们现在可以对骨骼序列的时空动态性进行建模。回想图的构建，图的时间方面是通过在连续帧上连接相同的关节进行构建的。这可以让我们定义一个很简单的策略来扩展空间图CNN到时空领域。我们扩展邻居的概念到包含空间连接的关节：
&lt;/p&gt;
$$\tag{6}
B(v\_{ti}) = \lbrace v\_{qj} \mid d(v\_{tj}, v\_{ti}) \leq K, \vert q - t \vert \leq \lfloor \Gamma / 2 \rfloor \rbrace.
$$&lt;p&gt;
参数$\Gamma$控制被包含到邻居图的时间范围，因此被称为空间核的大小。我们需要采样函数来完成时空图上的卷积操作，与只有空间卷积一样，我们还需要权重函数，具体来说就是映射$l_{ST}$。因为空间轴是有序的，我们直接修改根节点为$v_{ti}$的时空邻居的标签映射$l_{ST}$为：
&lt;/p&gt;
$$\tag{7}
l\_{ST}(v\_{qj}) = l\_{ti}(v\_{tj}) + (q - t + \lfloor \Gamma / 2 \rfloor) \times K,
$$&lt;p&gt;
其中$l_{ti}(v_{tj})$是$v_{ti}$的单帧的标签映射。这样，我们就有了一个定义在时空图上的卷积操作。&lt;/p&gt;
&lt;h2 id="34-分区策略"&gt;3.4 分区策略
&lt;/h2&gt;&lt;p&gt;设计一个实现标记映射的分区策略很重要。我们探索了几种分区策略。简单来说，我们只讨论单帧情况下，因为使用式7就可以很自然的扩展到时空领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Uni-labeling.&lt;/strong&gt; 最简单的分区策略，所有的邻居都是一个集合。每个邻居顶点的特征向量会和同一个权重向量做内积。事实上，这个策略和Kipf and Welling 2017提出的传播规则很像。但是有个很明显的缺陷，在单帧的时候使用这种分区策略就是将邻居的特征向量取平均后和权重向量做内积。在骨骼序列分析中不能达到最优，因为丢失了局部性质。$K = 1$，$l_{ti}(v_{tj}) = 0, \forall{i, j} \in V$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distance partitioning.&lt;/strong&gt; 另一个自然的分区策略是根据顶点到根节点$v_{ti}$的距离$d(\cdot, v_{ti})$来划分。我们设置$D = 1$，邻居集合会被分成两个子集，$d = 0$表示根节点子集，其他的顶点是在$d = 1$的子集中。因此我们有两个不同的权重向量，他们能对局部性质进行建模，比如关节间的相对变换。$K = 2$，$l_{ti}(v_{tj}) = d(v_{tj}, v_{ti})$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Spatial configuration partitioning.&lt;/strong&gt; 因为骨骼是空间局部化的，我们仍然可以利用这个特殊的空间配置来分区。我们将邻居集合分为三部分：1. 根节点自己；2. 中心组：相比根节点更接近骨骼重心的邻居顶点；3. 其他的顶点。其中，中心定义为一帧中骨骼所有的关节的坐标的平均值。这是受到人体的运动大体分为同心运动和偏心运动两类。
&lt;/p&gt;
$$\tag{8}
l\_{ti}(v\_tj) = \begin{cases}
0 &amp; if r\_j = r\_i \\
1 &amp; if r\_j &lt; r\_i \\
2 &amp; if r\_j &gt; r\_i \end{cases}
$$&lt;p&gt;
其中，$r_i$是训练集中所有帧的重心到关节$i$的平均距离。
分区策略如图3所示。我们通过实验检验提出的分区策略在骨骼动作识别上的表现。分区策略越高级，效果应该是越好的。
&lt;img src="https://davidham3.github.io/blog/images/spatial-temporal-graph-convolutional-networks-for-skeleton-based-action-recognition/Fig3.JPG"
loading="lazy"
alt="Fig3"
&gt;&lt;/p&gt;
&lt;h2 id="35-learnable-edge-importance-weighting"&gt;3.5 Learnable edge importance weighting
&lt;/h2&gt;&lt;p&gt;尽管人在做动作时关节是以组的形式移动的，但一个关节可以出现在身体的多个部分。然而，这些表现在建模时应该有不同的重要性。我们在每个时空图卷积层上添加了一个可学习的mask$M$。这个mask会基于$E_S$中每个空间图边上可学习的重要性权重来调整一个顶点的特征对它的邻居顶点的贡献。通过实验我们发现增加这个mask可以提升ST-GCN的性能。使用注意力映射应该也是可行的，这个留到以后再做。&lt;/p&gt;
&lt;h2 id="36-implementation-st-gcn"&gt;3.6 Implementation ST-GCN
&lt;/h2&gt;&lt;p&gt;实现这个图卷积不像实现2D或3D卷积那样简单。我们提供了实现ST-GCN的具体细节。
我们采用了Kipf &amp;amp; Welling 2017的相似的实现方式。单帧内身体内关节的连接表示为一个邻接矩阵$\rm{A}$，单位阵$\rm{I}$表示自连接。在单帧情况下，ST-GCN使用第一种分区策略时可以实现为：
&lt;/p&gt;
$$\tag{9}
\rm
f\_{out} = \Lambda^{-\frac{1}{2}}(A + I) \Lambda^{-\frac{1}{2}} f\_{in}W,
$$&lt;p&gt;
其中，$\Lambda^{ii} = \sum_j (A^{ij} + I^{ij})$。多个输出的权重向量叠在一起形成了权重矩阵$\mathrm{W}$。实际上，在时空情况下，我们可以将输入的feature map表示为维度为$(C, V, T)$的tensor。图卷积通过一个$1 \times \Gamma$实现一个标准的2D卷积，将结果与归一化的邻接矩阵$\rm \Lambda^{-\frac{1}{2}}(A + I)\Lambda^{-\frac{1}{2}}$在第二个维度上相乘。&lt;/p&gt;
&lt;p&gt;对于多个子集的分区策略，我们可以再次利用这种实现。但注意现在邻接矩阵已经分解成了几个矩阵$A_j$，其中$\rm A + I = \sum_j A_j$。举个例子，在距离分区策略中，$\rm A_0 = I$，$\rm A_1 = A$。式9变形为
&lt;/p&gt;
$$\tag{10}
\rm f\_{out} = \sum\_j \Lambda^{-\frac{1}{2}}\_j A\_j \Lambda^{\frac{1}{2}}\_j f\_{in} W\_j
$$&lt;p&gt;
其中，$\rm \Lambda^{ii}_j = \sum_k (A^{ik}_j) + \alpha$。这里我们设$\alpha = 0.001$避免$\rm A_j$中有空行。&lt;/p&gt;
&lt;p&gt;实现可学习的边重要性权重很简单。对于每个邻接矩阵，我们添加一个可学习的权重矩阵$M$，替换式9中的$\rm A + I$和式10中的$\rm A_j$中的$A_j$为$\rm (A + I) \otimes M$和$\rm A_j \otimes M$。这里$\otimes$表示两个矩阵间的element-wise product。mask$M$初始化为一个全一的矩阵。&lt;/p&gt;</description></item><item><title>Identity Mappings in Deep Residual Networks</title><link>https://davidham3.github.io/blog/p/identity-mappings-in-deep-residual-networks/</link><pubDate>Thu, 08 Mar 2018 18:45:45 +0000</pubDate><guid>https://davidham3.github.io/blog/p/identity-mappings-in-deep-residual-networks/</guid><description>&lt;p&gt;ECCV 2016, ResNet v2, 原文链接：&lt;a class="link" href="https://arxiv.org/abs/1603.05027" target="_blank" rel="noopener"
&gt;Identity Mappings in Deep Residual Networks&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="identity-mappings-in-deep-residual-networks"&gt;Identity Mappings in Deep Residual Networks
&lt;/h1&gt;&lt;h2 id="introduction"&gt;Introduction
&lt;/h2&gt;&lt;p&gt;Deep residual network (ResNets) consist of many stacked &amp;ldquo;Residual Units&amp;rdquo;. Each unit (Fig. 1(a)) can be expressed in a general form:
&lt;/p&gt;
$$y\_l = h(x\_l) + \mathcal{F}(x\_l, \mathcal{W\_l})$$&lt;p&gt;
&lt;/p&gt;
$$x\_{l+1}=f(y\_l)$$&lt;p&gt;
where $x_l$ and $x_{l+1}$ are input and output of the $l$-th unit, and $\mathcal{F}$ is a residual function.$h(x_l)=x_l$ is an identity mapping and $f$ is a ReLU function.
The central idea of ResNets is to learn the additive residual function $\mathcal{F}$ with respect to $h(x_l)$, with a key choice of using an identity mapping $h(x_l)=x_l$. This is realized by attaching an identity skip connection (&amp;ldquo;shortcut&amp;rdquo;).
In this paper, we analyze deep residual networks by focusing on creating a &amp;ldquo;direct&amp;rdquo; path for propagating information &amp;ndash; not only within a residual unit, but through the entire network. Our derivations reveal that &lt;em&gt;if both h(x_l) and f(y_l) are identity mappings, the signal could be directly&lt;/em&gt; propagated from one unit to any other units, in both forward and backward passes.
To understand the role of skip connections, we analyse and compare various types of $h(x_l)$. We find that the identity mapping $h(x_l) = x_l$ chosen in achieves the fastest error reduction and lowest training loss among all variants we investigated, whereas skip connections of scaling, gating, and $1 \times 1$ convolutions all lead to higher training loss and error. These experiments suggest that keeping a &amp;ldquo;clean&amp;rdquo; information path (indicated by the grey arrows in Fig. 1,2, and 4) is helpful for easing optimization.
&lt;img src="https://davidham3.github.io/blog/images/identity-mappings-in-deep-residual-networks/Fig1.PNG"
loading="lazy"
alt="Fig1"
&gt;
Figure 1. Left: (a) original Residual Unit in [1]; (b) proposed Residual Unit. The grey arrows indicate the easiest paths for the information to propagate, corresponding to the additive term &amp;ldquo;x_l&amp;rdquo; in Eqn.(4) (forward propagation) and the additive term &amp;ldquo;1&amp;rdquo; in Eqn.(5) (backward propagation). Right: training curves on CIFAR-10 of 1001-layer ResNets. Solid lines denote test error (y-axis on the right), and dashed lines denote training loss (y-axis on the left). The proposed unit makes ResNet-1001 easier to train.&lt;/p&gt;
&lt;p&gt;To construct an identity mapping $f(y_l)=y_l$, we view the activation functions (ReLU and BN) as &amp;ldquo;pre-activation&amp;rdquo; of the weight layers, in constrast to conventional wisdom of &amp;ldquo;post-activation&amp;rdquo;. This point of view leads to a new residual unit design, shown in (Fig. 1(b)). Based on this unit, we present competitive results on CIFAR-10/100 with a 1001-layer ResNet, which is much easier to train and generalizes better than the original ResNet in [1]. We further report improved results on ImageNet using a 200-layer ResNet, for which the counterpart of [1] starts to overfit. These results suggest that there is much room to exploit the dimension of &lt;em&gt;network depth&lt;/em&gt;, a key to the success of modern deep learning.&lt;/p&gt;
&lt;h2 id="analysis-of-deep-residual-networks"&gt;Analysis of Deep Residual Networks
&lt;/h2&gt;&lt;p&gt;The ResNets developed in [1] are &lt;em&gt;modularized&lt;/em&gt; architectures that stack building blocks of the same connecting shape. In this paper we call these blocks &amp;ldquo;Residual Units&amp;rdquo;. The original Residual Unit in [1] performs the following computation:
&lt;/p&gt;
$$y\_l = h(x\_l) + \mathcal{F}(x\_l, \mathcal{W-l})$$&lt;p&gt;
&lt;/p&gt;
$$x\_{l+1}=f(y\_l)$$&lt;p&gt;
Here $x_l$ is the input feature to the $l$-th Residual Unit. $\mathcal{W_l}=\lbrace W_{l,k} \mid 1 \le k \le K\rbrace$ is a set of weights (and biases) associated with the $l$-th Residual Unit, and $K$ is the number of layers in a Residual Unit ($K$ is 2 or 3 in [1]). $\mathcal{F}$ denotes the residual function, &lt;em&gt;e.g.&lt;/em&gt;, a stack of two $3 \times 3$ convolutional layers in [1]. The function $f$ is the operation after element-wise addition, and in [1] $f$ is ReLU. The function $h$ is set as an identity mapping: $h(x_l)=x_l$.
If $f$ is also an identity mapping: $x_{l+1} \equiv y_l$, we can put Eqn.(2) into Eqn.(1) and obtain:
&lt;/p&gt;
$$x\_{l+1}=x\_l+\mathcal{F}(x\_l, \mathcal{W\_l})$$&lt;p&gt;
Recursively $(x_{l+2}=x_{l+1} + \mathcal{F}(x_{l+1}, \mathcal{W_{l+1}}) = x_l + \mathcal{F}(x_l, \mathcal{W_l}) + \mathcal{F}(x_{l+1},\mathcal{W_{l+1}}), etc.)$ we will have:
&lt;/p&gt;
$$x\_L = x\_l + \sum\_{i=1}^{L-1}\mathcal{F}(x\_i, \mathcal{W\_i})$$&lt;p&gt;
for &lt;em&gt;any deeper unit&lt;/em&gt; $L$ and &lt;em&gt;any shallower unit&lt;/em&gt; $l$. Eqn.(4) exhibits some nice properties.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The feature $x_L$ of any deeper unit $L$ can be represented as the feature $x_l$ of any shallower unit $l$ plus a residual function in a form of $\sum_{i=1}^{L-1}\mathcal{F}$, indicating that the model is in a &lt;em&gt;residual&lt;/em&gt; fashion between any units $L$ and $l$.&lt;/li&gt;
&lt;li&gt;The feature $x_L = x_0 + \sum_{i=0}^{L-1}\mathcal{F}(x_i, \mathcal{W_i})$, of any deep unit $L$, is the &lt;em&gt;summation&lt;/em&gt; of the outputs of all preceding residual functions (plus $x_0$). This is in contrast to a &amp;ldquo;plain network&amp;rdquo; where a feature $x_L$ is a series of matrix-vector &lt;em&gt;products&lt;/em&gt;, say, $\prod_{i=0}^{L-1}W_ix_0$ (ignoring BN and ReLU).
Eqn.(4) also leads to nice backward propagation properties. Denoting the loss function as $\varepsilon$, from the chain rule of backpropagation [9] we have:
$$\frac{\partial{\varepsilon}}{\partial{x\_l}}=\frac{\partial{\varepsilon}}{\partial{x\_L}}\frac{\partial{x\_L}}{\partial{x\_l}}=\frac{\partial{\varepsilon}}{\partial{x\_L}}(1+\frac{\partial}{\partial{x\_l}}\sum\_{i=l}^{L-1}\mathcal{F}(x\_i, \mathcal{W\_i}))$$
Eqn.(5) indicates that the gradient $\frac{\partial{\varepsilon}}{\partial{x_i}}$ can be decomposed into two additive terms: a term of $\frac{\partial{\varepsilon}}{\partial{x_L}}$ that propagates information directly without concerning any weight layers, and another term of $\frac{\partial{\varepsilon}}{\partial{x_L}}(\frac{\partial}{\partial{x_l}}\sum_{i=l}^{L-1}\mathcal{F})$ that propagates through the weight layers. The additive term of $\frac{\partial{\varepsilon}}{\partial{x_L}}$ ensures that information is directly propagated back to &lt;em&gt;any shallower unit&lt;/em&gt; $l$. Eqn.(5) also suggests that it is unlikely for the gradient $\frac{\partial{\varepsilon}}{\partial{x_l}}$ to be canceled out for a mini-batch, because in general the term $\frac{\partial}{\partial{x_l}}\sum_{i=l}^{L-1}\mathcal{F}$ cannot be always -1 for all samples in a mini-batch. This implies that the gradient of a layer does not vanish even when the weights are arbitrarily small.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="on-the-importance-of-identity-skip-connections"&gt;On the Importance of Identity Skip Connections
&lt;/h2&gt;&lt;p&gt;Let&amp;rsquo;s consider a simple modification, $h(x_l)=\lambda_lx_l$, to break the identity shortcut:
&lt;/p&gt;
$$x\_{l+1}=\lambda\_lx\_l+\mathcal{F}(x\_l, \mathcal{W\_l})$$&lt;p&gt;
where $\lambda_l$ is a modulating scalar (for simplicity we still assume $f$ is identity).
Recursively applying this forumulation we obtain an equation similar to Eqn. (4): $x_L=(\prod_{i=l}^{L-1}\lambda_i)x_l+\sum_{i=1}^{L-1}(\prod_{j=i+1}^{L-1}\lambda_j)\mathcal{F}(x_i, \mathcal{W_i})$, or simply:
&lt;/p&gt;
$$x\_L = (\prod\_{i=l}^{L-1}\lambda\_i)x\_l+\sum\_{i=l}^{L-1}\hat{\mathcal{F}}(x\_i, \mathcal{W\_i})$$&lt;p&gt;
where the notation $\hat{\mathcal{F}}$ absorbs the scalars into the residual functions. Similar to Eqn.(5), we have backpropagation of the following form:
&lt;/p&gt;
$$\frac{\partial{\varepsilon}}{\partial{x\_l}}=\frac{\partial{\varepsilon}}{\partial{x\_L}}((\prod\_{i=l}^{L-1}\lambda\_i)+\frac{\partial}{\partial{x\_l}}\sum\_{i=l}^{L-1}\hat{\mathcal{F}}(x\_i, \mathcal{W\_i}))$$&lt;p&gt;
For an extremely deep network ($L$ is large), if $\lambda_i &amp;gt; 1$ for all $i$, this factor can be exponentially large; if $\lambda_i &amp;lt; 1$ for all $i$, this factor can be expoentially small and vanish, which blocks the backpropagated signal from the shortcur and forces it to flow through the weighted layers. This results in optimization difficuties as we show by experiments.
If the skip connection $h(x_l)$ represents more complicated transforms (such as gating and $1 \times 1$ convolutions), in Eqn.(8) the first term becomes $\prod_{i=l}^{L-1}h_i&amp;rsquo;$ where $h&amp;rsquo;$ is the derivative of $h$. This product may also impede information propagation and hamper the training procedure as witnessed in the following experiments.&lt;/p&gt;
&lt;h3 id="experiments-on-skip-connections"&gt;Experiments on skip Connections
&lt;/h3&gt;&lt;p&gt;We experiments with the 110-layer ResNet as presented in [1] on CIFAR-10. Though our above analysis is driven by identity $f$, the experiments in this section are all based on $f = ReLU$ as in [1]; we address identity $f$ in the next section. Our baseline ResNet-110 has 6.61% error on the test set. The comparisons of other variants (Fig.2 and Table 1) are summarized as follows:
&lt;strong&gt;Table 1.&lt;/strong&gt; Classification error on the CIFAR-10 test set using ResNet-110 [1], with different types of shortcut connections applied to all Residual Units. We report &amp;ldquo;fail&amp;rdquo; when the test error is higher than 20%.
&lt;img src="https://davidham3.github.io/blog/images/identity-mappings-in-deep-residual-networks/Table1.PNG"
loading="lazy"
alt="Table1"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Constant scaling&lt;/strong&gt;. We set $\lambda = 0.5$ for all shortcuts (Fig. 2(b)). We further study two cases of scaling $\mathcal{F}$:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\mathcal{F}$ is not scaled;&lt;/li&gt;
&lt;li&gt;$\mathcal{F}$ is scaled by a constant scalar of $1-\lambda = 0.5$, which is similar to the highway gating [6,7] but with frozen gates. The former case does not converge well; the latter is able to converge, but the test error (Table 1, 12.35%) is substantially higher than the original ResNet-110. Fig 3(a) shows that the training error is higher than that of the original ResNet-110, suggesting that the optimization has difficulties when the shortcut signal is scaled down.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Exclusive gating&lt;/strong&gt;. Following the Highway Networks [6,7] that adopt a gating mechanism [5], we consider a gating function $g(x)=\sigma(W_gx+b_g)$ where a transform is represented by weights $W_g$ and biases $b_g$ followed by the sigmoid function $\sigma(x)=\frac{1}{1+e^{-x}}$. In a convolutional network $g(x)$ is realized by a $1 \times 1$ convolutional layer. The gating function modulates the signal by element-wise multiplication.
We investigate the &amp;ldquo;exclusive&amp;rdquo; gates as used in [6,7] &amp;ndash; the $\mathcal{F}$ path is scaled by $g(x)$ and the shortcut path is scaled by $1-g(x)$. See Fig 2(c). We find that the initialization of the biases $b_g$ is critical for training gated models, and following the guidelines in [6,7], we conduct hyper-parameter search on the initial value of $b_g$ in the range of 0 to -10 with a decrement step of -1 on the training set by cross-validation. The best value (-6 here) is then used for training on the training set, leading to a test result of 8.70% (Table 1), which still lags far behind the ResNet-110 baseline. Fig 3(b) shows the training curves. Table 1 also reports the results of using other initialized values, noting that the exclusive gating network does not converge to a good solution when $b_g$ is not appropriately initialized.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shortcut-only gating&lt;/strong&gt;. In this case the function $\mathcal{F}$ is not scaled; only the shortcut path is gated by $1-g(x)$. See Fig 2(d). The initialized value of $b_g$ is still essential in this case. When the initialized $b_g$ is 0 (so initially the expectation of $1-g(x)$ is 0.5), the network converges to a poor result of 12.86% (Table 1). This is also caused by higher training error (Fig 3(c)).
When the initialized $b_g$ is very negatively biased (e.g., -6), the value of $1-g(x)$ is closer to 1 and the shortcut connection is nearly an identity mapping. Therefore, the result (6.91%, Table 1) is much closer to the ResNet-110 baseline.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;$1 \times 1$ convolutional shortcut&lt;/strong&gt;. Next we experiment with $1 \times 1$ convolutional shortcut connections that replace the identity. This option has been investigated in [1] (known as option C) on a 34-layer ResNet (16 Residual Units) and shows good results, suggesting that $1 \times 1$ shortcut connections could be useful. But we find that this is not the case when there are many Residual Units. The 110-layer ResNet has a poorer result (12.22%, Table 1) when using $1 \times 1$ convolutional shortcuts. Again, the training error becomes higher (Fig 3(d)). When stacking so many Residual Units (54 for ResNet-110), even the shortest path may still impede signal propagation. We witnessed similar phenomena on ImageNet with ResNet-101 when using $1 \times 1$ convolutional shortcuts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dropout shortcut&lt;/strong&gt;. Last we experiment with dropout [11] (at a ratio of 0.5) which we adopt on the output of the identity shortcut (Fig. 2(f)). The network fails to converge to a good solution. Dropout statistically imposes a scale of $\lambda $ with an expectation of 0.5 on the shortcut, and similar to constant scaling by 0.5, it impedes signal propagation.&lt;/p&gt;
&lt;h2 id="on-the-usage-of-activation-functions"&gt;On the Usage of Activation Functions
&lt;/h2&gt;&lt;p&gt;We want to make $f$ an identity mapping, which is done by re-arranging the activation function (ReLU and/or BN). The original Residual Unit in [1] has a shape in Fig.4(a) &amp;ndash; BN is used after each weight layer, and ReLU is adopted after BN expect that the last ReLU in a Residual Unit is after element-wise addition ($f=ReLU$). Fig.4(b-e) show the laternatives we investigated, explained as following.&lt;/p&gt;
&lt;h2 id="experiments-on-activation"&gt;Experiments on Activation
&lt;/h2&gt;&lt;p&gt;In this section we experiment with ResNet-110 and a 164-layer Bottlenect [1] architecture (denoted as ResNet-164). A bottleneck Residual Unit consist of a $1 \times 1$ layer for reducing dimension, a $3 \times 3$ layer, and a $1 \times 1$ layer for restoring dimension. As designed in [1], its computational complexity is similar to the two-$3 \times 3$ Residual Unit. More details are in the appendix. The baseline ResNet-164 has a competitive result of 5.93% on CIFAR-10 (Table 2).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BN after addition&lt;/strong&gt;. Before turning $f$ into an identity mapping, we go the opposite way by adopting BN after addition (Fig. 4(b)). In this case $f$ involves BN and ReLU. The results become considerably worse than the baseline (Table 2). Unlike the original design, now the BN layer alters the signal that passes through the shortcut and impedes information propagation, as reflected by the difficulties on reducing training loss at the begining of training (Fib. 6 left).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ReLU before addition&lt;/strong&gt;. A naive choice of making $f$ into an identity mapping is to move the ReLU&lt;/p&gt;
&lt;h2 id="implementation"&gt;Implementation
&lt;/h2&gt;&lt;p&gt;使用mxnet实现了一版&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt; 1
&lt;/span&gt;&lt;span class="lnt"&gt; 2
&lt;/span&gt;&lt;span class="lnt"&gt; 3
&lt;/span&gt;&lt;span class="lnt"&gt; 4
&lt;/span&gt;&lt;span class="lnt"&gt; 5
&lt;/span&gt;&lt;span class="lnt"&gt; 6
&lt;/span&gt;&lt;span class="lnt"&gt; 7
&lt;/span&gt;&lt;span class="lnt"&gt; 8
&lt;/span&gt;&lt;span class="lnt"&gt; 9
&lt;/span&gt;&lt;span class="lnt"&gt; 10
&lt;/span&gt;&lt;span class="lnt"&gt; 11
&lt;/span&gt;&lt;span class="lnt"&gt; 12
&lt;/span&gt;&lt;span class="lnt"&gt; 13
&lt;/span&gt;&lt;span class="lnt"&gt; 14
&lt;/span&gt;&lt;span class="lnt"&gt; 15
&lt;/span&gt;&lt;span class="lnt"&gt; 16
&lt;/span&gt;&lt;span class="lnt"&gt; 17
&lt;/span&gt;&lt;span class="lnt"&gt; 18
&lt;/span&gt;&lt;span class="lnt"&gt; 19
&lt;/span&gt;&lt;span class="lnt"&gt; 20
&lt;/span&gt;&lt;span class="lnt"&gt; 21
&lt;/span&gt;&lt;span class="lnt"&gt; 22
&lt;/span&gt;&lt;span class="lnt"&gt; 23
&lt;/span&gt;&lt;span class="lnt"&gt; 24
&lt;/span&gt;&lt;span class="lnt"&gt; 25
&lt;/span&gt;&lt;span class="lnt"&gt; 26
&lt;/span&gt;&lt;span class="lnt"&gt; 27
&lt;/span&gt;&lt;span class="lnt"&gt; 28
&lt;/span&gt;&lt;span class="lnt"&gt; 29
&lt;/span&gt;&lt;span class="lnt"&gt; 30
&lt;/span&gt;&lt;span class="lnt"&gt; 31
&lt;/span&gt;&lt;span class="lnt"&gt; 32
&lt;/span&gt;&lt;span class="lnt"&gt; 33
&lt;/span&gt;&lt;span class="lnt"&gt; 34
&lt;/span&gt;&lt;span class="lnt"&gt; 35
&lt;/span&gt;&lt;span class="lnt"&gt; 36
&lt;/span&gt;&lt;span class="lnt"&gt; 37
&lt;/span&gt;&lt;span class="lnt"&gt; 38
&lt;/span&gt;&lt;span class="lnt"&gt; 39
&lt;/span&gt;&lt;span class="lnt"&gt; 40
&lt;/span&gt;&lt;span class="lnt"&gt; 41
&lt;/span&gt;&lt;span class="lnt"&gt; 42
&lt;/span&gt;&lt;span class="lnt"&gt; 43
&lt;/span&gt;&lt;span class="lnt"&gt; 44
&lt;/span&gt;&lt;span class="lnt"&gt; 45
&lt;/span&gt;&lt;span class="lnt"&gt; 46
&lt;/span&gt;&lt;span class="lnt"&gt; 47
&lt;/span&gt;&lt;span class="lnt"&gt; 48
&lt;/span&gt;&lt;span class="lnt"&gt; 49
&lt;/span&gt;&lt;span class="lnt"&gt; 50
&lt;/span&gt;&lt;span class="lnt"&gt; 51
&lt;/span&gt;&lt;span class="lnt"&gt; 52
&lt;/span&gt;&lt;span class="lnt"&gt; 53
&lt;/span&gt;&lt;span class="lnt"&gt; 54
&lt;/span&gt;&lt;span class="lnt"&gt; 55
&lt;/span&gt;&lt;span class="lnt"&gt; 56
&lt;/span&gt;&lt;span class="lnt"&gt; 57
&lt;/span&gt;&lt;span class="lnt"&gt; 58
&lt;/span&gt;&lt;span class="lnt"&gt; 59
&lt;/span&gt;&lt;span class="lnt"&gt; 60
&lt;/span&gt;&lt;span class="lnt"&gt; 61
&lt;/span&gt;&lt;span class="lnt"&gt; 62
&lt;/span&gt;&lt;span class="lnt"&gt; 63
&lt;/span&gt;&lt;span class="lnt"&gt; 64
&lt;/span&gt;&lt;span class="lnt"&gt; 65
&lt;/span&gt;&lt;span class="lnt"&gt; 66
&lt;/span&gt;&lt;span class="lnt"&gt; 67
&lt;/span&gt;&lt;span class="lnt"&gt; 68
&lt;/span&gt;&lt;span class="lnt"&gt; 69
&lt;/span&gt;&lt;span class="lnt"&gt; 70
&lt;/span&gt;&lt;span class="lnt"&gt; 71
&lt;/span&gt;&lt;span class="lnt"&gt; 72
&lt;/span&gt;&lt;span class="lnt"&gt; 73
&lt;/span&gt;&lt;span class="lnt"&gt; 74
&lt;/span&gt;&lt;span class="lnt"&gt; 75
&lt;/span&gt;&lt;span class="lnt"&gt; 76
&lt;/span&gt;&lt;span class="lnt"&gt; 77
&lt;/span&gt;&lt;span class="lnt"&gt; 78
&lt;/span&gt;&lt;span class="lnt"&gt; 79
&lt;/span&gt;&lt;span class="lnt"&gt; 80
&lt;/span&gt;&lt;span class="lnt"&gt; 81
&lt;/span&gt;&lt;span class="lnt"&gt; 82
&lt;/span&gt;&lt;span class="lnt"&gt; 83
&lt;/span&gt;&lt;span class="lnt"&gt; 84
&lt;/span&gt;&lt;span class="lnt"&gt; 85
&lt;/span&gt;&lt;span class="lnt"&gt; 86
&lt;/span&gt;&lt;span class="lnt"&gt; 87
&lt;/span&gt;&lt;span class="lnt"&gt; 88
&lt;/span&gt;&lt;span class="lnt"&gt; 89
&lt;/span&gt;&lt;span class="lnt"&gt; 90
&lt;/span&gt;&lt;span class="lnt"&gt; 91
&lt;/span&gt;&lt;span class="lnt"&gt; 92
&lt;/span&gt;&lt;span class="lnt"&gt; 93
&lt;/span&gt;&lt;span class="lnt"&gt; 94
&lt;/span&gt;&lt;span class="lnt"&gt; 95
&lt;/span&gt;&lt;span class="lnt"&gt; 96
&lt;/span&gt;&lt;span class="lnt"&gt; 97
&lt;/span&gt;&lt;span class="lnt"&gt; 98
&lt;/span&gt;&lt;span class="lnt"&gt; 99
&lt;/span&gt;&lt;span class="lnt"&gt;100
&lt;/span&gt;&lt;span class="lnt"&gt;101
&lt;/span&gt;&lt;span class="lnt"&gt;102
&lt;/span&gt;&lt;span class="lnt"&gt;103
&lt;/span&gt;&lt;span class="lnt"&gt;104
&lt;/span&gt;&lt;span class="lnt"&gt;105
&lt;/span&gt;&lt;span class="lnt"&gt;106
&lt;/span&gt;&lt;span class="lnt"&gt;107
&lt;/span&gt;&lt;span class="lnt"&gt;108
&lt;/span&gt;&lt;span class="lnt"&gt;109
&lt;/span&gt;&lt;span class="lnt"&gt;110
&lt;/span&gt;&lt;span class="lnt"&gt;111
&lt;/span&gt;&lt;span class="lnt"&gt;112
&lt;/span&gt;&lt;span class="lnt"&gt;113
&lt;/span&gt;&lt;span class="lnt"&gt;114
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mxnet&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;mxnet&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pickle&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mxnet&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;unpickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;fo&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;dicts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pickle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fo&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bytes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;dicts&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;residual_unit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;same_shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;stride&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;same_shape&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BatchNorm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fix_gamma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;_bn1&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;_relu1&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;pad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;_conv1&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BatchNorm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fix_gamma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;_bn2&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;_relu2&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;pad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;_conv2&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;same_shape&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;stride&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;_conv3&amp;#34;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;ResNet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;residual_unit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&lt;/span&gt;&lt;span class="si"&gt;%s%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;residual_unit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&lt;/span&gt;&lt;span class="si"&gt;%s%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BatchNorm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;batch1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;relu1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Pooling&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pool_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;avg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;pool1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;flat1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FullyConnected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_hidden&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SoftmaxOutput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;all_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unpickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;../data/cifar-10-batches-py/data_batch_&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;all_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;labels&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;all_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;trainX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unpickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;../data/cifar-10-batches-py/test_batch&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;testX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;testY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;labels&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# batch_size = 128&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;128&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;train_iter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;NDArrayIter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;test_iter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;NDArrayIter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;testX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;testY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ResNet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;mod&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gpu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# context = mx.gpu(0),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;data_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;label_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;softmax_label&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_shapes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_iter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;provide_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label_shapes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_iter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;provide_label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Xavier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rnd_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gaussian&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;factor_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;in&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;magnitude&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_optimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;nag&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer_params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;learning_rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;wd&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;momentum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# mod.init_optimizer(optimizer=&amp;#39;adam&amp;#39;, optimizer_params=((&amp;#39;learning_rate&amp;#39;, 5e-4),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# (&amp;#39;beta1&amp;#39;, 0.9),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# (&amp;#39;beta2&amp;#39;, 0.99)))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;losses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;metrics&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;metric&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;acc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;metric&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CrossEntropy&lt;/span&gt;&lt;span class="p"&gt;()]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;train_iter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;train_iter&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;is_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update_metric&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update_metric&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_iter&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;acc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;metric&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CrossEntropy&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;losses&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;, Training acc &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;, loss &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;losses&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;, Validation acc &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;, loss &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;losses&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;losses&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;training loss&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;testing loss&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;upper right&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;iteration&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;loss&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;train_acc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_acc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;training acc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_acc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;testing acc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;upper right&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;iteration&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;acc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>Deep Residual Learning for Image Recognition</title><link>https://davidham3.github.io/blog/p/deep-residual-learning-for-image-recognition/</link><pubDate>Sun, 04 Mar 2018 18:59:20 +0000</pubDate><guid>https://davidham3.github.io/blog/p/deep-residual-learning-for-image-recognition/</guid><description>&lt;p&gt;CVPR 2015，ResNet，原文链接：&lt;a class="link" href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener"
&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="deep-residual-learning-for-image-recongnition"&gt;Deep Residual Learning for Image Recongnition
&lt;/h1&gt;&lt;h2 id="problems"&gt;problems
&lt;/h2&gt;&lt;p&gt;When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error, as reported in [11, 42] and thoroughly verified by our experiments. Fig. 1 shows a typical example.
&lt;img src="https://davidham3.github.io/blog/images/deep-residual-learning-for-image-recognition/Fig1.PNG"
loading="lazy"
alt="Fig1"
&gt;&lt;/p&gt;
&lt;p&gt;Figure 1. Training error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer “plain” networks. The deeper network has higher training error, and thus test error. Similar phenomena on ImageNet is presented in Fig. 4.&lt;/p&gt;
&lt;p&gt;The degradation (of training accuracy) indicates that not all systems are similarly easy to optimize. Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution &lt;em&gt;by construction&lt;/em&gt; to the deeper model: the added layers are &lt;em&gt;identity&lt;/em&gt; mapping and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time).&lt;/p&gt;
&lt;h2 id="deep-residual-learning"&gt;Deep Residual Learning
&lt;/h2&gt;&lt;h3 id="residual-learning"&gt;Residual Learning
&lt;/h3&gt;&lt;p&gt;Let us consider $\mathcal{H}(x)$ as an underlying mapping to be fit by a few stacked layers (not necessarily the entire net), with $x$ denoting the inputs to the first of these layers. If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, $i.e.$, $\mathcal{H}(x)-x$ (assuming that the input and output are of the same dimensions). So rather than expect stacked layers to approximate $\mathcal{H}(x)$, we explicitly let these layers approximate a residual function $\mathcal{F}(x):=\mathcal{H}(x)-x$. The original function thus becomes $\mathcal{F}(x)+x$. Although both forms should be able to asymptotically approximate the desired functions (as hypothesized), the ease of learning might be different.&lt;/p&gt;
&lt;h3 id="identity-mapping-by-shortcuts"&gt;Identity Mapping by Shortcuts
&lt;/h3&gt;$$y = \mathcal{F}(x, {W\_i})+x$$&lt;p&gt;
Here $x$ and $y$ are the input and output vectors of the layers considered. The function $\mathcal{F}(x, W_i)$ represents the residual mapping to be learned. For the example in Fig. 2 that has two layers, $\mathcal{F} = W_2\sigma (W_1x)$ in which $\sigma $ denotes ReLU and the bias are omitting for simplifying notations. The operation $\mathcal{F}+x$ is performed by a shortcut connection and element-wise addition. We adopt the second nonlinearity after the addtion (&lt;em&gt;i.e.&lt;/em&gt;, $\sigma(y)$, see Fig.2).
&lt;img src="https://davidham3.github.io/blog/images/deep-residual-learning-for-image-recognition/Fig2.PNG"
loading="lazy"
alt="Fig2"
&gt;&lt;/p&gt;
&lt;p&gt;Figure2. Residual learning: a building block.&lt;/p&gt;
&lt;p&gt;The dimensions of $x$ and $\mathcal{F}$ must be equal in Eqn.(1). If this is not the case (&lt;em&gt;e.g.&lt;/em&gt;, when changing the input/output channels), we can perform a linear projection $W_s$ by the shortcut connections to match the dimensions:
&lt;/p&gt;
$$y = \mathcal{F}(x, {W\_i}) + W\_sx$$&lt;p&gt;
We can also use a square matrix $W_s$ in Eqn.(1). But we will show by experiments that the identity mapping is sufficient for addressing the degradation problem and is economical, and thus $W_s$ is only used when matching dimensions.
We also note that although the above notations are about fully-connected layers for simplicity, they are applicable to convolutional layers. The function $\mathcal{F}(x, {W_i})$ can represent multiple convolutional layers. The element-wise addition is performed on two feature maps, channel by channel.&lt;/p&gt;
&lt;h2 id="residual-network"&gt;Residual Network
&lt;/h2&gt;&lt;p&gt;The identity shortcuts can be directly used when the input and output are of the same dimensions (solid line shortcuts in Fig.3). When the dimensions increase (dotted line shortcuts in Fig.3), we consider two options: (A) The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. This option introduces no extra parameter; (B) The projection shortcut in Eqn.(2) is used to match dimensions (done by $1 \times 1$ convolutions). For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2.
&lt;img src="https://davidham3.github.io/blog/images/deep-residual-learning-for-image-recognition/Fig3.PNG"
loading="lazy"
alt="Fig3"
&gt;&lt;/p&gt;
&lt;p&gt;Figure3. Example network architectures for ImageNet. &lt;b&gt;Left&lt;/b&gt;: the VGG-19 model. &lt;b&gt;Middle&lt;/b&gt;: a plain network with 34-parameter layers. &lt;strong&gt;Right&lt;/strong&gt;: a residual network with 34 parameter layers. The dotted shortcuts increase dimensions. &lt;b&gt;Table 1&lt;/b&gt; shows more details and other variants.&lt;/p&gt;
&lt;h2 id="implementation"&gt;Implementation
&lt;/h2&gt;&lt;p&gt;Our implementation for ImageNet follows the practice in &lt;em&gt;[Imagenet classification
with deep convolutional neural networks]&lt;/em&gt; and &lt;em&gt;[Very deep convolutional networks for large-scale image recognition]&lt;/em&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The Image is resized with its shorter side randomly sampled in $[256, 480]$ for scale agumentation.&lt;/li&gt;
&lt;li&gt;A $224 \times 224$ crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted.&lt;/li&gt;
&lt;li&gt;The standard color augmentation in &lt;em&gt;Imagenet classification
with deep convolutional neural networks&lt;/em&gt; is used.&lt;/li&gt;
&lt;li&gt;We adopt batch normalization (BN) right after each convolution and before activation.&lt;/li&gt;
&lt;li&gt;We initialize the weights as in &lt;em&gt;Delving deep into rectifiers:
Surpassing human-level performance on imagenet classification&lt;/em&gt; and train all plain/residual nets from scratch.&lt;/li&gt;
&lt;li&gt;We use SGD with a mini-batch size of 256.&lt;/li&gt;
&lt;li&gt;The learning rate starts from 0.1 and is divided by 10 when the error plateaus, and the models are trained from up to $60 \times 10^4$ iterations.&lt;/li&gt;
&lt;li&gt;We use a weight decay of 0.0001 and a momentum of 0.9.&lt;/li&gt;
&lt;li&gt;We do not use dropout, following the practice in &lt;em&gt;Batch normalization: Accelerating deep
network training by reducing internal covariate shift&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;In testing, for comparison studies we adopt the standard 10-crop testing.[&lt;em&gt;Imagenet classification
with deep convolutional neural networks&lt;/em&gt;]&lt;/li&gt;
&lt;li&gt;For best results, we adopt the fully-convolutional form as in &lt;em&gt;Very deep convolutional networks for large-scale image recognition&lt;/em&gt; and &lt;em&gt;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification&lt;/em&gt;, and average the scores at multiple scales (images are resized such that the shorter side is in $\lbrace 224, 256, 384, 480, 640\rbrace $.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="imagenet-classification"&gt;ImageNet classification
&lt;/h2&gt;&lt;h3 id="deeper-bottleneck-architecture"&gt;Deeper Bottleneck Architecture
&lt;/h3&gt;&lt;p&gt;Next we describe our deeper nets for ImageNet. Because of concerns on the training time that we can afford, we modify the building block as a &lt;em&gt;bottleneck&lt;/em&gt; design. For each residual function $\mathcal{F}$, we use a stack of 3 layers instead of 2 (Fig. 5). The three layers are $1 \times 1$, $3 \times 3$, and $1 \times 1$ convolutions, where the $1 \times 1$ layers are responsible for reducing and then increasing (restoring) dimensions, leaving the $3 \times 3$ layer a bottleneck with smaller input/output dimensions. Fig. 5 shows an example, where both designs have similar time complexity.
The parameter-free indentity shortcuts are particularly important for the bottleneck architectures. If the identity&lt;/p&gt;
&lt;h2 id="cifar-10-and-analysis"&gt;CIFAR-10 and Analysis
&lt;/h2&gt;&lt;p&gt;The plain/residual architectures follow the form in Fig.3(middle/right). The network inputs are $32 \times 32$ images, with the per-pixel mean subtracted. The first layer is $3 \times 3$ convolutions. Then we use a stack of $6n$ layers with $3 \times 3$ convolutions on the feature maps of sizes $\lbrace 32, 16, 8\rbrace $ respectively, with $2n$ layers for each feature map size. The numbers of filters are $\lbrace 16, 32, 64\rbrace $ respectively, with $2n$ layers for each feature map size. The subsampling is performed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax.
When shortcut connections are used, they are connected to the pairs of $3 \times 3$ layers(totally $3n$ shortcuts). On this dataset we use identity shortcuts in all cases (&lt;em&gt;i.e.&lt;/em&gt;, option A), so our residual models have exactly the same depth, width and number of parameters as the plain counterparts.
We use a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization in &lt;em&gt;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification&lt;/em&gt; and BN in &lt;em&gt;Accelerating deep network training by reducing internal covariate shift&lt;/em&gt; but with no dropout. These models are trained with a mini-batch size of 128 on two GPUs. We start with a learning rate of 0.1, divide it by 10 at 32k and 48k iterations, and terminate training at 64k iterations, which is determined on a 45k/5k train/val split. We follow the simple data augmentation in &lt;em&gt;Deeply-supervised nets&lt;/em&gt; for training: 4 pixels are padded on each side, and a $32 \times 32$ crop is randomly sampled from the padded image or its horizontal flip. For testing, we only evaluate the single view of the original $32 \times 32$ image.
We compare $n=\lbrace 3, 5, 7, 9\rbrace $, leading to 20, 32, 44, and 56-layer networks.&lt;/p&gt;</description></item><item><title>Image Super-Resolution Using Deep Convolutional Networks</title><link>https://davidham3.github.io/blog/p/image-super-resolution-using-deep-convolutional-networks/</link><pubDate>Fri, 02 Mar 2018 11:19:50 +0000</pubDate><guid>https://davidham3.github.io/blog/p/image-super-resolution-using-deep-convolutional-networks/</guid><description>&lt;p&gt;PAMI 2016，大体思路：把训练集中的所有样本模糊化，扔到三层的卷积神经网络中，把输出和原始图片做一个loss，训练模型即可。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1501.00092" target="_blank" rel="noopener"
&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;首先是ill-posed problem，图像的不适定问题
法国数学家阿达马早在19世纪就提出了不适定问题的概念:称一个数学物理定解问题的解存在、唯一并且稳定的则称该问题是适定的（Well Posed）.如果不满足适定性概念中的上述判据中的一条或几条，称该问题是不适定的。&lt;/p&gt;
&lt;h1 id="convolutional-neural-networks-for-super-resolution"&gt;Convolutional Neural Networks For Super-Resolution
&lt;/h1&gt;&lt;h2 id="formulation"&gt;Formulation
&lt;/h2&gt;&lt;p&gt;We first upscale a single low-resolution image to the desired size using bicubic interpolation. Let us denote the interpolated image as $Y$. Our goal is to recover from $Y$ an image $F(Y)$ that is as similar as possible to the ground truth high-resolution image $X$. For the ease of presentation, we still call $Y$ a &amp;ldquo;low-resolution&amp;rdquo; image, although it has the same size as $X$. We wish to learning a mapping $F$, which conceptually consists of three operations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Patch extraction and representation. this operation extracts (overlapping) patches from the low-resolution image $Y$ and represents each patch as a high-dimensional vector. These vectors comprise a set of feature maps, of which the number equals to the dimensionality of the vectors.&lt;/li&gt;
&lt;li&gt;Non-linear mapping. this operation nonlinearly maps each high-dimensional vector onto another high-dimensional vector. Each mapped vector is conceptually the representation of a high-resolution patch. These vectors comprise another set of feature maps.&lt;/li&gt;
&lt;li&gt;Reconstruction. this operation aggregates the above high-resolution patch-wise representations to generate the final high-resolution image. This image is expected to be similar to the ground truth $X$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="patch-extraction-and-representation"&gt;Patch Extraction and Representation
&lt;/h3&gt;&lt;p&gt;A popular strategy in image restoration is to densely extract patches and then represent them by a set of pre-trained bases such as PCA, DCT, Haar, etc. This is equivalent to convolving the image by a set of filters, each of which is a basis. In our formulation, we involve the optimization of these bases into the optimization of the network. Formally, our first layer is expressed as an operation $F_1$:
&lt;/p&gt;
$$F\_1(Y)=max(0, W\_1 * Y + B\_1)$$&lt;p&gt;
where $W_1$ and $B_1$ represent the filters and biases respectively, and $*$ denotes the convolution operation. Here, $W_1$ corresponds to $n_1$ filters of support $c \times f_1 \times f_1$, where $c$ is the number of channels in the input image, $f_1$ is the spatial size of a filter. Intuitively, $W_1$ applies $n_1$ convolutions on the image, and each convolution has a kernel size $c \times f_1 \times f_1$. The output is composed of $n_1$ feature maps. $B_1$ is an $n_1$-dimensional vector, whose each element is associated with a filter. We apply the ReLU on the filter responses.&lt;/p&gt;
&lt;h3 id="non-linear-mapping"&gt;Non-Linear Mapping
&lt;/h3&gt;&lt;p&gt;The first layer extracts an $n_1$-dimensional feature for each patch. In the second operation, we map each of these $n_1$-dimensional vectors into an $n_2$-dimensional one. This is equivalent to applying $n_2$ filters which have a trivial spatial support $1 \times 1$. This interpretation is only valid for $1 \times 1$ filters. But it is easy to generalize to larger filters like $3 \times 3$ or $5 \times 5$. In that case, the non-linear mapping is not on a patch of the input image; instead, it is on a $3 \times 3$ or $5 \times 5$ &amp;ldquo;patch&amp;rdquo; of the feature map. The operation of the second layer is:
&lt;/p&gt;
$$F\_2(Y) = max(0, W\_2 * F\_1(Y) + B\_2)$$&lt;p&gt;
Here $W_2$ contains $n_2$ filters of size $n_1 \times f_2 \times f_2$, and $B_2$ is $n_2$-dimensional. Each of the output $n_2$-dimensional vectors is conceptually a representation of a high-resolution patch that will be used for reconstruction.&lt;/p&gt;
&lt;h3 id="reconstruction"&gt;Reconstruction
&lt;/h3&gt;&lt;p&gt;In the traditional methods, the predicted overlapping high-resolution patches are often averaged to produce the final full image. The averaging can be considered as a pre-defined filter on a set of feature maps (where each position is the &amp;ldquo;flattened&amp;rdquo; vector form of a high-resolution patch). Motivated by this, we define a convolutional layer to produce the final high-resolution image:
&lt;/p&gt;
$$F(Y)=W\_3 * F\_2(Y) + B\_3$$&lt;p&gt;
Here W_3 corresponds to $c$ filters of a size $n_2 \times f_3 \times f_3$, and $B_3$ is a $c$-dimensional vector.&lt;/p&gt;
&lt;h2 id="training"&gt;Training
&lt;/h2&gt;&lt;p&gt;Loss function: given a set of high-resolution images ${X_i}$ and their corresponding low-resolution images ${Y_i}$, we use mean squared error (MSE) as the loss function:
&lt;/p&gt;
$$L(\Theta ) = \frac{1}{n}\sum^n\_{i=1}\Vert F(Y\_i;\Theta)-X\_i\Vert ^2$$&lt;p&gt;
where $n$ is the number of training samples. Using MSE as the loss function favors a high PSNR. The PSNR is widely-used metric for quantitatively evaluating image restoration quality, and is at least partially related to the perceptual quality. Despite that the proposed model is trained favoring a high PSNR, we still observe satisfactory performance when the model is evaluated using alternative evaluation metrics, e.g., SSIM, MSSIM.
PSNR: Peak Signal to Noise Ratio. 是一种评价图像的客观标准。
&lt;/p&gt;
$$PSNR = 10 \times \log\_{10}(\frac{(2^n-1)^2}{MSE})$$&lt;p&gt;
其中，MSE是原图像和处理图像之间的均方误差，n是每个采样值的比特数，单位是dB。
The loss is minimized using stochastic gradient descent with the standard backpropagation. In particular, the weight matrices are updated as
&lt;/p&gt;
$$\Delta\_{i+1}=0.9 \cdot \Delta\_i + \eta \cdot \frac{\partial{L}}{\partial{W^\ell\_i}}, W^\ell\_{i+1}=W^\ell\_{i}+\Delta\_{i+1}$$&lt;p&gt;
where $\ell \in {1,2,3}$ and $i$ are the indices of layers and iterations, $\eta$ is the learning rate, and $\frac{\partial{L}}{\partial{W^\ell_i}}$ is the derivative. The filter weights of each layer are initialized by drawing randomly from a Gaussian distribution with zero mean and standard deviation 0.0001 (and 0 for biases). The learning rate is $10^{-4}$ for the first two layers, and $10^{-5}$ for the last layer. We empirically find that a smaller learning rate in the last layer is important for the network to converge (similar to the denoising case).
In the training phase, the ground truth images ${X_i}$ are prepared as $f_{sub} \times f_{sub} \times c$-pixel sub-images randomly cropped from the training images. By &amp;ldquo;sub-images&amp;rdquo; we mean these samples are treated as small &amp;ldquo;images&amp;rdquo; rather than &amp;ldquo;patches&amp;rdquo;, in the sense that &amp;ldquo;patches&amp;rdquo; are overlapping and require some averaging as post-processing but &amp;ldquo;sub-images&amp;rdquo; need not. To synthesize the low-resolution samples ${Y_i}$, we blur a sub-image by a Gaussian kernel, sub-sample it by the upscaling factor, and upscale it by the same factor via bicubic interpolation.
To avoid border effects during training, all the convolutional layers have no padding, and the network produces a smaller output $((f_{sub}-f_1-f_2-f_3+3)^2 \times c)$. The MSE loss function is evaluated only by the difference between the contral pixels of $X_i$ and the network output. Although we use a fixed image size in training, the convolutional nerual network can be applied on images of arbitrary sizes during testing.&lt;/p&gt;</description></item><item><title>Perceptual Losses for Real-Time Style Transfer and Super-Resolution</title><link>https://davidham3.github.io/blog/p/perceptual-losses-for-real-time-style-transfer-and-super-resolution/</link><pubDate>Thu, 01 Mar 2018 19:01:46 +0000</pubDate><guid>https://davidham3.github.io/blog/p/perceptual-losses-for-real-time-style-transfer-and-super-resolution/</guid><description>&lt;p&gt;ECCV 2016，实时风格迁移与超分辨率化的感知损失，这篇论文是在cs231n里面看到的，正好最近在研究风格迁移。一作是Justin Johnson，2017春的cs231n的主讲之一。这篇论文的主要内容是对Gatys等人的风格迁移在优化过程中进行了优化，大幅提升了性能。
主要原理就是，之前Gatys等人的论文是利用已经训练好的VGG19，求loss并利用VGG的结构反向求导更新图片。由于VGG结构复杂，这样反向更新速度很慢，改进方法是再另外设计一个神经网络，将内容图片作为输入，输出扔到VGG中做两个loss，然后反向传播更新当前这个神经网络的参数，这样训练出来的神经网络就可能将任意的内容图片扔进去，输出为风格迁移后的图片，这也就解决了速度的问题。这也就是将Feed-forward image transformation与style transfer结合在一起。原文链接：&lt;a class="link" href="https://arxiv.org/abs/1603.08155" target="_blank" rel="noopener"
&gt;Perceptual Losses for Real-Time Style Transfer and Super-Resolution&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="image-transformation-network"&gt;Image Transformation Network
&lt;/h1&gt;&lt;h2 id="architecture"&gt;Architecture
&lt;/h2&gt;&lt;p&gt;We do not use any pooling layers, instead using strided and fractionally strided convolutions for in-network downsampling and upsampling. Our network body consists of five residual blocks using the architecture of http://torch. ch/blog/2016/02/04/resnets.html. All non-residual convolutional layers are followed by spatial batch normalization and ReLU nonlinearities with the exception of the output layer, which instead uses a scaled tanh to ensure that the output image has pixels in the range [0, 255]. Other than the first and last layers with use $9 \times 9$ kernels, all convolutional layers use $3 \times 3$ kernels. The exact architectures of all our networks can be found in the supplementary material.&lt;/p&gt;
&lt;h2 id="inputs-and-outputs"&gt;Inputs and Outputs
&lt;/h2&gt;&lt;p&gt;For style transfer the input and output are both color images of shape #3 \times 256 \times 256#.
For super-resolution with an upsampling factor of $f$, the output is a high-resolution patch of shape $3 \times 288/f \times 288/f$. Since the image transformation networks are fully-convolutional, at test-time they can be applied to images of any resolution.&lt;/p&gt;
&lt;h2 id="downsampling-and-upsampling"&gt;Downsampling and Upsampling
&lt;/h2&gt;&lt;p&gt;For super-resolution with an upsampling factor of $f$, we use several residual blocks followed by $log_2f$ convolutional layers with stride $1/2$. This is different from [1] who use bicubic interpolation to upsample the low-resolution input before passing it to the network.&lt;/p&gt;
&lt;p&gt;Our style transfer networks use the architecture shown in Table 1 and our super-resolution networks use the architecture shown in Table 2. In these tables &amp;ldquo;$C \times H \times W$ conv&amp;rdquo; denotes a convolutional layer with $C$ filters size $H \times W$ which is immeidately followed by spatial batch normalization [1] and a ReLU nonlinearity.
Our residual blocks each contain two $3 \times 3$ convolutional layers with the same number of filters on both layer. We use the residual block design of Gross and Wilber [2] (shown in Figure 1), which differs from that of He &lt;em&gt;et al&lt;/em&gt; [3] in that the ReLU nonlinearity following the addition is removed; this modified design was found in [2] to perform slightly better for image classification.
For style transfer, we found that standard zero-padded convolutions resulted in severe artifacts around the borders of the generated image. We therefore remove padding from the convolutions in residual blocks. A $3 \times 3$ convolution with no padding reduces the size of a feature map by 1 pixel on each side, so in this case the identity connection of the residual block performs a center crop on the input feature map. We also add spatial reflection padding to the beginning of the network so that the input and output of the network have the same size.&lt;/p&gt;
&lt;h1 id="perceptual-loss-functions"&gt;Perceptual Loss Functions
&lt;/h1&gt;&lt;p&gt;We define two &lt;em&gt;perceptual loss functions&lt;/em&gt; that measure high-level perceptual and semantic differences between images. They make use of a loss &lt;em&gt;network&lt;/em&gt; $\phi$ pretrained for image classification, meaning that these perceptual loss functions are themselves deep convolutional neural networks. In all our experiments $\phi$ is the 16-layer VGG network pretrained on the ImageNet dataset.&lt;/p&gt;
&lt;h2 id="featue-reconstruction-loss"&gt;Featue Reconstruction Loss
&lt;/h2&gt;&lt;p&gt;Rather than encouraging the pixels of the output image $\hat{y} = f_W(x)$ to exactly match the pixels of the target image $y$, we instead encourage them to have similar feature representations as computed by the loss network $\phi$. Let $\phi_j(x)$ be the activations of the &lt;em&gt;j&lt;/em&gt;th layer of the network $\phi$ when processing the image $x$; if $j$ is a convolutional layer then $\phi_j(x)$ will be a feature map of shape $C_j \times H_j \times W_j$. The &lt;em&gt;feature reconstruction loss&lt;/em&gt; is the (squared, normalized) Euclidean distance between feature representations:
&lt;/p&gt;
$$\ell\_{feat}^{\phi,j}(\hat{y}, y)=\frac{1}{C\_jH\_jW\_j}\Vert \phi\_j(\hat{y})-\phi\_j(y)\Vert\_2^2$$&lt;p&gt;
As demonstrated in [6] and reproduced in Figure 3, finding an image $\hat{y}$ that minimizes the feature reconstruction loss for early layers tends to produce images that are visually indistinguishable from $y$.&lt;/p&gt;
&lt;h2 id="style-reconstruction-loss"&gt;Style Reconstruction Loss
&lt;/h2&gt;&lt;p&gt;The feature reconstruction loss penalizes the output image $\hat{y}$ when it deviates in content from the target $y$. We also wish to penalize differences in style: colors, textures, common patterns, etc. To achieve this effect, Gatys &lt;em&gt;et al&lt;/em&gt; propose the following &lt;em&gt;style reconstruction loss&lt;/em&gt;.
As above, let $\phi_j(x)$ be the activations at the $j$th layer of the network $\phi$ for the input $x$, which is a feature map of shape $C_j \times H_j \times W_j$. Define the &lt;em&gt;Gram matrix&lt;/em&gt; $G^\phi_j(x)$ to be the $C_j \times C_j$ matrix whose elements are given by
&lt;/p&gt;
$$G^\phi\_j(x)\_{c,c'}=\frac{1}{C\_jH\_jW\_j}\sum^{H\_j}\_{h=1}\sum\_{w=1}^{W\_j}\phi\_j(x)\_{h,w,c}\phi\_j(x)\_{h,w,c'}$$&lt;h1 id="experiments"&gt;Experiments
&lt;/h1&gt;&lt;h2 id="style-transfer"&gt;Style Transfer
&lt;/h2&gt;&lt;h2 id="single-image-super_resolution"&gt;Single-Image Super_Resolution
&lt;/h2&gt;&lt;p&gt;This is an inherently ill-posed problem, since for each low-resolution image there exist multiple high-resolution images that could have generated it. The ambiguity becomes more extreme as the super-resolution factor grows; for larger factors ($\times 4$, $\times 8$), fine details of the high-resolution image may have little or no evidence in its low-resolution version.
To overcome this problem, we train super-resolution networks not with the per-pixel loss typically used [1] but instead with a feature reconstruction loss to allow transer of semantic knowledge from the pretrained loss network to the super-resolution network. We focus on $\times 4$ and $\times 8$ super-resolution since larger factors require more semantic reasoning about the input.
The traditional metrics used to evaluate super-resolution are PSNR and SSIM, both of which have been found to correlate poorly with human assessment of visual quality. PSNR and SSIM rely only on low-level differences between pixels and operate under the assumption of additive Gasussian noise, which may be invalid for super-resolution. In addition, PSNR is equivalent to the per-pixel loss $\mathcal{l_{pixle}}$, so as measured by PSNR a model trained to minimize feature reconstruction loss should always outperform a model trained to minimize feature reconstruction loss. We therefore emphasize that the goal of these experiments is not to achieve state-of-the art PSNR or SSIM results, but instead to showcase the qualitative difference between models trained with per-pixel and feature reconstruction losses.&lt;/p&gt;
&lt;h1 id="code"&gt;code
&lt;/h1&gt;&lt;p&gt;我用gluon实现了一个2x的超分辨率网络，训练后感觉效果一般，只有一次loss降到了40附近，那次效果挺好，但是颜色并不是很好
以下是代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt; 1
&lt;/span&gt;&lt;span class="lnt"&gt; 2
&lt;/span&gt;&lt;span class="lnt"&gt; 3
&lt;/span&gt;&lt;span class="lnt"&gt; 4
&lt;/span&gt;&lt;span class="lnt"&gt; 5
&lt;/span&gt;&lt;span class="lnt"&gt; 6
&lt;/span&gt;&lt;span class="lnt"&gt; 7
&lt;/span&gt;&lt;span class="lnt"&gt; 8
&lt;/span&gt;&lt;span class="lnt"&gt; 9
&lt;/span&gt;&lt;span class="lnt"&gt; 10
&lt;/span&gt;&lt;span class="lnt"&gt; 11
&lt;/span&gt;&lt;span class="lnt"&gt; 12
&lt;/span&gt;&lt;span class="lnt"&gt; 13
&lt;/span&gt;&lt;span class="lnt"&gt; 14
&lt;/span&gt;&lt;span class="lnt"&gt; 15
&lt;/span&gt;&lt;span class="lnt"&gt; 16
&lt;/span&gt;&lt;span class="lnt"&gt; 17
&lt;/span&gt;&lt;span class="lnt"&gt; 18
&lt;/span&gt;&lt;span class="lnt"&gt; 19
&lt;/span&gt;&lt;span class="lnt"&gt; 20
&lt;/span&gt;&lt;span class="lnt"&gt; 21
&lt;/span&gt;&lt;span class="lnt"&gt; 22
&lt;/span&gt;&lt;span class="lnt"&gt; 23
&lt;/span&gt;&lt;span class="lnt"&gt; 24
&lt;/span&gt;&lt;span class="lnt"&gt; 25
&lt;/span&gt;&lt;span class="lnt"&gt; 26
&lt;/span&gt;&lt;span class="lnt"&gt; 27
&lt;/span&gt;&lt;span class="lnt"&gt; 28
&lt;/span&gt;&lt;span class="lnt"&gt; 29
&lt;/span&gt;&lt;span class="lnt"&gt; 30
&lt;/span&gt;&lt;span class="lnt"&gt; 31
&lt;/span&gt;&lt;span class="lnt"&gt; 32
&lt;/span&gt;&lt;span class="lnt"&gt; 33
&lt;/span&gt;&lt;span class="lnt"&gt; 34
&lt;/span&gt;&lt;span class="lnt"&gt; 35
&lt;/span&gt;&lt;span class="lnt"&gt; 36
&lt;/span&gt;&lt;span class="lnt"&gt; 37
&lt;/span&gt;&lt;span class="lnt"&gt; 38
&lt;/span&gt;&lt;span class="lnt"&gt; 39
&lt;/span&gt;&lt;span class="lnt"&gt; 40
&lt;/span&gt;&lt;span class="lnt"&gt; 41
&lt;/span&gt;&lt;span class="lnt"&gt; 42
&lt;/span&gt;&lt;span class="lnt"&gt; 43
&lt;/span&gt;&lt;span class="lnt"&gt; 44
&lt;/span&gt;&lt;span class="lnt"&gt; 45
&lt;/span&gt;&lt;span class="lnt"&gt; 46
&lt;/span&gt;&lt;span class="lnt"&gt; 47
&lt;/span&gt;&lt;span class="lnt"&gt; 48
&lt;/span&gt;&lt;span class="lnt"&gt; 49
&lt;/span&gt;&lt;span class="lnt"&gt; 50
&lt;/span&gt;&lt;span class="lnt"&gt; 51
&lt;/span&gt;&lt;span class="lnt"&gt; 52
&lt;/span&gt;&lt;span class="lnt"&gt; 53
&lt;/span&gt;&lt;span class="lnt"&gt; 54
&lt;/span&gt;&lt;span class="lnt"&gt; 55
&lt;/span&gt;&lt;span class="lnt"&gt; 56
&lt;/span&gt;&lt;span class="lnt"&gt; 57
&lt;/span&gt;&lt;span class="lnt"&gt; 58
&lt;/span&gt;&lt;span class="lnt"&gt; 59
&lt;/span&gt;&lt;span class="lnt"&gt; 60
&lt;/span&gt;&lt;span class="lnt"&gt; 61
&lt;/span&gt;&lt;span class="lnt"&gt; 62
&lt;/span&gt;&lt;span class="lnt"&gt; 63
&lt;/span&gt;&lt;span class="lnt"&gt; 64
&lt;/span&gt;&lt;span class="lnt"&gt; 65
&lt;/span&gt;&lt;span class="lnt"&gt; 66
&lt;/span&gt;&lt;span class="lnt"&gt; 67
&lt;/span&gt;&lt;span class="lnt"&gt; 68
&lt;/span&gt;&lt;span class="lnt"&gt; 69
&lt;/span&gt;&lt;span class="lnt"&gt; 70
&lt;/span&gt;&lt;span class="lnt"&gt; 71
&lt;/span&gt;&lt;span class="lnt"&gt; 72
&lt;/span&gt;&lt;span class="lnt"&gt; 73
&lt;/span&gt;&lt;span class="lnt"&gt; 74
&lt;/span&gt;&lt;span class="lnt"&gt; 75
&lt;/span&gt;&lt;span class="lnt"&gt; 76
&lt;/span&gt;&lt;span class="lnt"&gt; 77
&lt;/span&gt;&lt;span class="lnt"&gt; 78
&lt;/span&gt;&lt;span class="lnt"&gt; 79
&lt;/span&gt;&lt;span class="lnt"&gt; 80
&lt;/span&gt;&lt;span class="lnt"&gt; 81
&lt;/span&gt;&lt;span class="lnt"&gt; 82
&lt;/span&gt;&lt;span class="lnt"&gt; 83
&lt;/span&gt;&lt;span class="lnt"&gt; 84
&lt;/span&gt;&lt;span class="lnt"&gt; 85
&lt;/span&gt;&lt;span class="lnt"&gt; 86
&lt;/span&gt;&lt;span class="lnt"&gt; 87
&lt;/span&gt;&lt;span class="lnt"&gt; 88
&lt;/span&gt;&lt;span class="lnt"&gt; 89
&lt;/span&gt;&lt;span class="lnt"&gt; 90
&lt;/span&gt;&lt;span class="lnt"&gt; 91
&lt;/span&gt;&lt;span class="lnt"&gt; 92
&lt;/span&gt;&lt;span class="lnt"&gt; 93
&lt;/span&gt;&lt;span class="lnt"&gt; 94
&lt;/span&gt;&lt;span class="lnt"&gt; 95
&lt;/span&gt;&lt;span class="lnt"&gt; 96
&lt;/span&gt;&lt;span class="lnt"&gt; 97
&lt;/span&gt;&lt;span class="lnt"&gt; 98
&lt;/span&gt;&lt;span class="lnt"&gt; 99
&lt;/span&gt;&lt;span class="lnt"&gt;100
&lt;/span&gt;&lt;span class="lnt"&gt;101
&lt;/span&gt;&lt;span class="lnt"&gt;102
&lt;/span&gt;&lt;span class="lnt"&gt;103
&lt;/span&gt;&lt;span class="lnt"&gt;104
&lt;/span&gt;&lt;span class="lnt"&gt;105
&lt;/span&gt;&lt;span class="lnt"&gt;106
&lt;/span&gt;&lt;span class="lnt"&gt;107
&lt;/span&gt;&lt;span class="lnt"&gt;108
&lt;/span&gt;&lt;span class="lnt"&gt;109
&lt;/span&gt;&lt;span class="lnt"&gt;110
&lt;/span&gt;&lt;span class="lnt"&gt;111
&lt;/span&gt;&lt;span class="lnt"&gt;112
&lt;/span&gt;&lt;span class="lnt"&gt;113
&lt;/span&gt;&lt;span class="lnt"&gt;114
&lt;/span&gt;&lt;span class="lnt"&gt;115
&lt;/span&gt;&lt;span class="lnt"&gt;116
&lt;/span&gt;&lt;span class="lnt"&gt;117
&lt;/span&gt;&lt;span class="lnt"&gt;118
&lt;/span&gt;&lt;span class="lnt"&gt;119
&lt;/span&gt;&lt;span class="lnt"&gt;120
&lt;/span&gt;&lt;span class="lnt"&gt;121
&lt;/span&gt;&lt;span class="lnt"&gt;122
&lt;/span&gt;&lt;span class="lnt"&gt;123
&lt;/span&gt;&lt;span class="lnt"&gt;124
&lt;/span&gt;&lt;span class="lnt"&gt;125
&lt;/span&gt;&lt;span class="lnt"&gt;126
&lt;/span&gt;&lt;span class="lnt"&gt;127
&lt;/span&gt;&lt;span class="lnt"&gt;128
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;mxnet&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mxnet&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mxnet&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;autograd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mxnet&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;gluon&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mxnet.gluon&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mxnet&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;init&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mxnet.gluon.model_zoo&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;vision&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# get_ipython().run_line_magic(&amp;#39;matplotlib&amp;#39;, &amp;#39;inline&amp;#39;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;logging&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;logger&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getLogger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;data_filenames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;trainx_&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;.params&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;target_filenames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;trainy_&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;.params&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;load_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;ratio&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e-5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;start_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;end_index&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;start_index&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;ctx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gpu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;load_params&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;training.log&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;residual_unit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gluon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HybridBlock&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;residual_unit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;channels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bn1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BatchNorm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;act1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;channels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bn2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BatchNorm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;act2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;channels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;act3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hybrid_forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;act1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bn1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;act2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bn2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;act3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;plsr_network&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gluon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HybridBlock&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plsr_network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name_scope&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;residual_sequential&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HybridSequential&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;residual_sequential&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;residual_unit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;deconv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2DTranspose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HybridSequential&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;residual_sequential&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;deconv1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hybrid_forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;tv_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;data1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;data2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;data2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_vgg_loss_net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pretrained_net&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HybridSequential&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pretrained_net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vgg_loss_net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratio&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vgg_loss_net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ratio&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tv_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;rgb_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.485&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.456&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.406&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;rgb_std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.229&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.224&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.225&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;72&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;72&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;72&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;72&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;total_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_filenames&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_filenames&lt;/span&gt;&lt;span class="p"&gt;))[:&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;total_size&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;total_size&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;total_size&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plsr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plsr_network&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;load_params&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;plsr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;plsr.params&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;plsr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Xavier&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plsr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hybridize&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;pretrained_net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vgg16&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pretrained&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;vgg_loss_net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_vgg_loss_net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pretrained_net&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;vgg_loss_net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;collect_params&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_ctx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;trainer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gluon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Trainer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plsr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;collect_params&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;adam&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;learning_rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s1"&gt;&amp;#39;beta1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s1"&gt;&amp;#39;beta2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.99&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;dataloader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gluon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gluon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ArrayDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;training_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dataloader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;data_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gluon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split_and_load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;target_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gluon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split_and_load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;autograd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;record&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;losses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_list&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;losses&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;get_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vgg_loss_net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plsr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;rgb_mean&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copyto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;rgb_std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copyto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratio&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;losses&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;training_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asscalar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;l&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;losses&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training_loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;training.log&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;plsr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;plsr.params&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;在实现的时候，超分辨率后需要一个后处理——直方图匹配，这里参考的是&lt;a class="link" href="https://github.com/mapbox/rio-hist/blob/master/rio_hist/match.py" target="_blank" rel="noopener"
&gt;rio-hist&lt;/a&gt;。
实验数据最开始用的是Microsoft的coco2017，将每张图随机截取$144 \times 144$像素的大小，然后使用宽度为1的高斯核进行模糊处理后，downsampling了一下，得到了$72 \times 72$的图片，作为网络的输入。后来发现效果不是很好，就打算向waifu2x一样，只训练动漫图片，上konachan上爬了一万张图，做同样的处理。此时的loss降到了31.
&lt;img src="https://davidham3.github.io/blog/images/perceptual-losses-for-real-time-style-transfer-and-super-resolution/400_0.1_31.png"
loading="lazy"
alt="Fig1"
&gt;
这是训练的最好的一次，最左侧是输入的模糊图片，第二列是网络的输出，第三列是做了直方图匹配得到的图片，第四列是ground truth。可以看到有很多小点点，我分析是tv loss占比太小的原因，当前tv loss乘以了0.1。于是将tv loss乘以0.5后又训练了一次，loss降到了58，结果如下：
&lt;img src="https://davidham3.github.io/blog/images/perceptual-losses-for-real-time-style-transfer-and-super-resolution/400_0.5_58.png"
loading="lazy"
alt="Fig2"
&gt;
感觉没法看了。。。&lt;/p&gt;</description></item><item><title>Image Style Transfer Using Convolutional Neural Networks</title><link>https://davidham3.github.io/blog/p/image-style-transfer-using-convolutional-neural-networks/</link><pubDate>Sat, 24 Feb 2018 23:51:39 +0000</pubDate><guid>https://davidham3.github.io/blog/p/image-style-transfer-using-convolutional-neural-networks/</guid><description>&lt;p&gt;CVPR 2016，大体原理：选择两张图片，一张作为风格图片，一张作为内容图片，任务是将风格图片中的风格，迁移到内容图片上。方法也比较简单，利用在ImageNet上训练好的VGG19，因为这种深层次的卷积神经网络的卷积核可以有效的捕捉一些特征，越靠近输入的卷积层捕捉到的信息层次越低，而越靠近输出的卷积层捕捉到的信息层次越高，因此可以用高层次的卷积层捕捉到的信息作为对风格图片风格的捕捉。而低层次的卷积层用来捕捉内容图片中的内容。所以实际的操作就是，将内容图片扔到训练好的VGG19中，取出低层次的卷积层的输出，保存起来，然后再把风格图片放到VGG19中，取出高层次的卷积层的输出，保存起来。然后随机生成一张图片，扔到VGG19中，将刚才保存下来的卷积层的输出的那些卷积层的结果拿出来，和那些保存的结果做个loss，然后对输入的随机生成的图片进行优化即可。原文链接：&lt;a class="link" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" target="_blank" rel="noopener"
&gt;Image Style Transfer Using Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="image-style-transfer-using-convolutional-neural-networks"&gt;Image Style Transfer Using Convolutional Neural Networks
&lt;/h1&gt;&lt;h1 id="大体原理"&gt;大体原理
&lt;/h1&gt;&lt;p&gt;选择两张图片，一张作为风格图片，一张作为内容图片，任务是将风格图片中的风格，迁移到内容图片上。方法也比较简单，利用在ImageNet上训练好的VGG19，因为这种深层次的卷积神经网络的卷积核可以有效的捕捉一些特征，越靠近输入的卷积层捕捉到的信息层次越低，而越靠近输出的卷积层捕捉到的信息层次越高，因此可以用高层次的卷积层捕捉到的信息作为对风格图片风格的捕捉。而低层次的卷积层用来捕捉内容图片中的内容。所以实际的操作就是，将内容图片扔到训练好的VGG19中，取出低层次的卷积层的输出，保存起来，然后再把风格图片放到VGG19中，取出高层次的卷积层的输出，保存起来。然后随机生成一张图片，扔到VGG19中，将刚才保存下来的卷积层的输出的那些卷积层的结果拿出来，和那些保存的结果做个loss，然后对输入的随机生成的图片进行优化即可。(Fig2)
&lt;img src="https://davidham3.github.io/blog/images/image-style-transfer-using-convolutional-neural-networks/Fig2.PNG"
loading="lazy"
alt="Fig2"
&gt;&lt;/p&gt;
&lt;p&gt;Figure 2. Style transfer algorithm. First content and style features are extracted and stored. The style image $\vec{a}$ is passed through the network and its style representation $A^l$ on all layers included are computed and stored(left). The content image $\vec{p}$ is passed through the network and the content representation $P^l$ in one layer is stored(right). Then a random white noise image $\vec{x}$ is passed through the network and its style features $G^l$ and content features $F^l$ are computed. On each layer included in the style representation, the element-wise mean squared difference between $G^l$ and $A^l$ is computed to give the style loss $\mathcal{L}_{style}$(left). Also the mean squared difference between $F^l$ and $P^l$ is computed to give the content loss $\mathcal{L}_{content}(right)$. The total loss $\mathcal{L}_{total}$ is then a linear combination between the content and the style loss. Its derivative with respect to the pixel values can be computed using error back-propagation(middle). This gradient is used to iteratively update the image $\vec{x}$ until it simultaneously matches the style features of the style image $\vec{a}$ and the content features of the content image $\vec{p}$(middle, bottom).&lt;/p&gt;
&lt;h1 id="deep-image-representations"&gt;Deep image representations
&lt;/h1&gt;&lt;p&gt;We used the feature space provided by a normalized version of the 16 convolutional and 5 pooling layers of the 19-layer VGG network. We normalized the network by scaling the weights such that the mean activation of each convolutional filter over images and positions is equal to one. Such re-scaling can be done for the VGG network without changing its output, because it contains only rectifying linear activation functions and no normalization or pooling over feature maps.
其实这里我不是很明白为什么不会影响输出。&lt;/p&gt;
&lt;h2 id="content-representation"&gt;content representation
&lt;/h2&gt;&lt;p&gt;A layer with $N_l$ distinct filters has $N_l$ feature maps each of size $M_l$, where $M_l$ is the height times the width of the feature map. So the responses in a layer $l$ can be stored in a matrix $F^l \in \mathcal{R}^{N_l \times M_l}$ where $F^l_{ij}$ is the activation of the $i^{th}$ filter at position $j$ in layer $l$.
Let $\vec{p}$ and $\vec{x}$ be the original image and the image that is generated, and $P^l$ and $F^l$ their respective feature representation in layer $l$.
We then define the squared-error loss between the two feature representations
&lt;/p&gt;
$$\mathcal{L}\_{content}(\vec{p}, \vec{x}, l) = \frac{1}{2}\sum\_{i, j}(F^l\_{ij}-P^l\_{ij})^2$$&lt;p&gt;
The derivative of this loss with respect to the activations in layer $l$ equals&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial{\mathcal{L}&lt;em&gt;{content}}}{\partial{F^l&lt;/em&gt;{ij}}}=\left{
\begin{aligned}
&amp;amp; (F^l - P^l)&lt;em&gt;{ij} &amp;amp; if \ F^l&lt;/em&gt;{ij} &amp;gt; 0 \
&amp;amp; 0 &amp;amp; if \ F^l_{ij} &amp;lt; 0
\end{aligned}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;from which the gradient with respect to the image $\vec{x}$ can be computed using standard error back-propagation.&lt;/p&gt;
&lt;p&gt;When Convolutional Neural Networks are trained on object recongnition, they develop a representation of the image that makes object information increasingly explicit along the processing hierarchy. Higher layers in the network capture the high-level &lt;em&gt;content&lt;/em&gt; in terms of objects and their arrangement in the input image but do not constrain the exact pixel values of the reconstruction very much. We therefore refer to the feature responses in higher layers of the network as the &lt;em&gt;content representation&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id="style-representation"&gt;style representation
&lt;/h2&gt;&lt;p&gt;To obtain a representation of the style of an input image, we use a feature space designed to capture texture information. This feature space can be built on top of the filter responses in any layer of the network. It consists of the correlations between the different filter responses, where the expecation is taken over the spatial extent of the feature maps. These feature correlations are given by the Gram matrix $G^l \in \mathcal{R}^{N_l \times N_l}$, where $G^l_{ij}$ is the inner product between the vecotrized feature maps $i$ and $j$ in layer $l$:
&lt;/p&gt;
$$G^l\_{ij}=\sum\_kF^l\_{ik}F^l\_{jk}.$$&lt;p&gt;
By inducing the feature corelations of multiple layers, we obtain a stationary, multi-scale representation of the input image, which captures its texture information but not the global arrangement. Again, we can visualise the information captured by these style feature spaces built on different layers of the network by constructing an image that matches the style representation of a given input image. This is done by using gradient descent from a white noise image to minimise the mean-squared distance between the entries of the Gram matrices from the original image and the Gram matrices of the image to be generated.
Let $\vec{a}$ and $\vec{x}$ be the original image and the image that is generated, and $A^l$ and $G^l$ their respective style representation in layer $l$. The contribution of layer $l$ to the toal loss is then
&lt;/p&gt;
$$E\_l = \frac{1}{4N^2\_lM^2\_l}\sum\_{i,j}(G^l\_{ij} - A^l\_{ij})^2$$&lt;p&gt;
and the total style loss is
&lt;/p&gt;
$$\mathcal{L}\_{style}(\vec{a}, \vec{x})=\sum^L\_{l=0}w\_lE\_l,$$&lt;p&gt;
where $w_L$ are weighting factors of the contribution of each layer to the total loss (see below for specific values of $w_l$ in our results). The derivative of $E_l$ with respect to the activations in layer $l$ can be computed analytically:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial{E_l}}{\partial{F^l_{ij}}}=\left{
\begin{aligned}
&amp;amp; \frac{1}{N^2_lM^2_l}((F^l)^T(G^l-A^l))&lt;em&gt;{ji} &amp;amp; if \ F^l&lt;/em&gt;{ij} &amp;gt; 0 \
&amp;amp; 0 &amp;amp; if \ F^l_{ij} &amp;lt; 0
\end{aligned}
\right.
\end{equation}
The gradient of $E_l$ with respect to the pixel values $\vec{x}$ can be readily computed using standard error back-propagation.&lt;/p&gt;
&lt;h2 id="style-transfer"&gt;style transfer
&lt;/h2&gt;&lt;p&gt;To transfer the style of an artwork $\vec{a}$ onto a photograph $\vec{p}$ we synthesise a new image that simultaneously matches the content representation of $\vec{p}$ and the style representation of $\vec{a}$. Thus we jointly minimise the distance of the feature representations of a white noise image fron the content representation of the photograph in one layer and the style representation of the painting defined on a numebr of layers of the Convolutional Neural Network. The loss function we minimise is
&lt;/p&gt;
$$\mathcal{L}\_{total}(\vec{p}, \vec{a}, \vec{x})=\alpha \mathcal{L}\_{content}(\vec{p}, \vec{x}) + \beta \mathcal{L}\_{style}(\vec{a}, \vec{x})$$&lt;p&gt;
where $\alpha$ and $\beta$ are the weighting factors for content and style reconstruction, respectively. The gradient with respect to the pixel values $\frac{\partial{\mathcal{L}_{total}}}{\partial{\vec{x}}}$ can be used as input for some numerical optimisation strategy. Here we use &lt;strong&gt;L-BFGS&lt;/strong&gt;, which we found to work best for image synthesis. To extract image information on comparable scales, we always resized the style image to the same size as the content image before computing its feature representations.&lt;/p&gt;
&lt;h1 id="results"&gt;Results
&lt;/h1&gt;&lt;h2 id="trade-off-between-content-and-style-matching"&gt;Trade-off between content and style matching
&lt;/h2&gt;&lt;p&gt;Since the loss function we minimise during image synthesis is a linear combination between the loss functions for content and style respectively, we can smoothly regulate the emphasis on either reconstructing the content or the style(Fig4).
&lt;img src="https://davidham3.github.io/blog/images/image-style-transfer-using-convolutional-neural-networks/Fig4.PNG"
loading="lazy"
alt="Fig4"
&gt;
Figure 4. Relative weighting of matching content and style of the respective source images. The ratio $\alpha / \beta$ between matching the content and matching the style increases from top left to bottom right. A high emphasis on the style effectively produces a texturised version of the style image(top left). A high emphasis on the content produces an image with only little stylisation(bottom right). In practice one can smoothly interpolate between the two extremes.&lt;/p&gt;
&lt;h2 id="effect-of-different-layers-of-the-convolutional-neural-network"&gt;Effect of different layers of the Convolutional Neural Network
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/image-style-transfer-using-convolutional-neural-networks/Fig5.PNG"
loading="lazy"
alt="Fig5"
&gt;
Figure 5. The effect of matching the content representation in different layers of the network. Matching the content on layer &amp;lsquo;conv2_2&amp;rsquo; preserves much of the fine structure of the original photograph and the synthesised image looks as if the texture of the painting is simply blended over the photograph(middle). When matching the content on layer &amp;lsquo;conv4_2&amp;rsquo; the texture of the painting and the content of the photograph merge together such that the content of photograph is displayed in the style of the painting(bottom). Both images were generated with the same choice of parameters($\alpha / \beta = 1 \times 10^{-3}$). The painting that served as the style image is shown in the bottom left corner and is name &lt;i&gt;Jesuiten Ⅲ&lt;/i&gt; by Lyonel Feininger, 1915.
Another important factor in the image synthesis process is the choice of layers to match the content and style representation on. As outlined above, the style representation is a multi-scale representation that includes multiple layers of the neural network. The number and position of these layers determines the local scale on which the style is matched, leading to different visual experiences. We find that matching the style representations up to higher layers in the network preserves local images structures an increasingly large scale, leading to a smoother and more continuous visual experience. Thus, the visually most appealing images are usually created by matching the style representation up to high layers in the network, which is why for all images shown we match the style features in layers &amp;lsquo;conv1_1&amp;rsquo;, &amp;lsquo;conv2_1&amp;rsquo;, &amp;lsquo;conv3_1&amp;rsquo;, &amp;lsquo;conv4_1&amp;rsquo;and &amp;lsquo;conv5_1&amp;rsquo; of the network.
To analyse the effect of using different layers to match the content features, we present a style transfer result obtained by stylising a photograph with the same artwork and parameter configuration ($\alpha / \beta = 1 \times 10^{-3}$), but in one matching the content features on layer &amp;lsquo;conv2_2&amp;rsquo; and in the other on layer &amp;lsquo;conv4_2&amp;rsquo;(Fig5). When matching the content on a lower layer of the network, the algorithm matches much of the detailed pixel information in the photograph and the generated image appears as if the texture of the artwork is merely blended over the photograph(Fig5, middle). In contrast, when matching the content features on a higher layer of the network, deatiled pixel information of the photograph is not as strongly constraint and the texture of the artwork and the content of the photograph are properly merged. That is, the fine structure of the image, for example the edges and colour map, is altered such that it agrees with the style of the artwork while displaying the content of the photograph(Fig5, bottom).&lt;/p&gt;
&lt;h2 id="initialisation-of-gradient-descent"&gt;Initialisation of gradient descent
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://davidham3.github.io/blog/images/image-style-transfer-using-convolutional-neural-networks/Fig6.PNG"
loading="lazy"
alt="Fig6"
&gt;
Figure 6. Initialisation of the gradient descent. &lt;b&gt;A&lt;/b&gt; Initialised from the content image. &lt;b&gt;B&lt;/b&gt; Initialised from the style image. &lt;b&gt;C&lt;/b&gt; Four samples of images initialised from different white noise images. For all images the ratio $\alpha / \beta$ was equal to $1 \times 10^{-3}$
We have initialised all images shown so far with white noise. However, one could also initialise the image synthesis with either the content image or the style image. We explored these two alternatives(Fig6 A, B): although they bias the final image somewhat towards the spatial structure of the initialisation, the different intialisation do not seem to have a strong effect on the outcome of the synthesis procedure. It should be noted that only initialising with noise allows to generate an arbitrary number of new images(Fig6 C). Initialising with a fixed image always deterministically leads to the same outcome (up to stochasticity in the gradient descent procedure).&lt;/p&gt;
&lt;h1 id="implementation"&gt;implementation
&lt;/h1&gt;&lt;p&gt;关于实现的部分，我自己用mxnet实现了一下，但是发现和mxnet的example里面给的非常不一样。在他们的实现里面提到了Total variation denoising。而且，论文中的loss function是sum of square，而图2中给出是MSE，取了个平均值。我实现是时候没有取平均，导致loss很大，但是也可以训练。但是自己实现的梯度下降很难收敛，需要对梯度进行归一化，后来使用MXNet的gluon的Trainer训练会比原来好很多。&lt;/p&gt;
&lt;h2 id="total-variation-denoising"&gt;Total variation denoising
&lt;/h2&gt;&lt;p&gt;In signal processing, total variation denoising, also known as total variation regularization, is a process, most often used in digital image processing, that has applications in noise removal.
&lt;img src="https://davidham3.github.io/blog/images/image-style-transfer-using-convolutional-neural-networks/ROF_Denoising_Example.png"
loading="lazy"
alt="“Total variation denoising”"
&gt;
Example of application of the Rudin et al.[1] total variation denoising technique to an image corrupted by Gaussian noise. This example created using demo_tv.m by Guy Gilboa, see external links.
It is based on the principle that signals with excessive and possibly spurious detail have high total variation, that is, the integral of the absolute gradient of the signla is high. According to this principle, reducing the total variation of the signal subject to it being a close match to the original signal, removes unwanted detail whilst preserving important details such as edges. The concept was pioneered by Rudin, Osher, and Fatemi in 1992 and so is today known as the ROF model.
This noise removal technique has advantages over simple techniques such as linear smoothing or median filtering which reduce noise but at the same time smooth away edges to a greater or lesser degree. By contrast, total variation denoising is remarkably effective at simultaneously preserving edges whilst smoothing away noise in flat regions, even at low signal-to-noise ratios.&lt;/p&gt;
&lt;h3 id="1d-signal-series"&gt;1D signal series
&lt;/h3&gt;&lt;p&gt;For a digital signal $y_n$, we can, for example, define the total variation as:
&lt;/p&gt;
$$V(y)=\sum\_n\vert y\_{n+1}-y\_n\vert$$&lt;p&gt;
Given an input signal $x_n$, the goal of total variation denoising is to find an approximation, call it $y_n$, that has smaller total variation than $x_n$ but is &amp;ldquo;close&amp;rdquo; to $x_n$. One measure of closeness is the sum of square errors:
&lt;/p&gt;
$$E(x, y)=\frac{1}{2}\sum\_n(x\_n - y\_n)^2$$&lt;p&gt;
So the total variation denoising problem amounts to minimizing the following discrete functional over the signal $y_n$:
&lt;/p&gt;
$$E(x, y) + \lambda V(y)$$&lt;p&gt;
By differentiating this functional with respect to $y_n$, we can derive a corresponding Euler-lagrange equation, that can be numerically integrated with the original signal $x_n$ as initial condition. This was the original approach. Alternatively, since this is a convex functional, techniques from convex optimization can be used to minimize it and find the solution $y_n$.&lt;/p&gt;
&lt;h3 id="regularization-properties"&gt;Regularization properties
&lt;/h3&gt;&lt;p&gt;The regularization parameter $\lambda $ plays a critical role in the denoising process. When $\lambda = 0$, there is no smoothing and the result is the same as minimizing the sum of squares. As $\lambda \to \infty $, however, the total variation term plays an increasingly strong role, which forces the result to have smaller total variation, at the expanse of being less like the input (noisy) signal. Thus, the choice of regularization parameter is critical to achieving just the right amount of noise removal.&lt;/p&gt;
&lt;h3 id="2d-signal-images"&gt;2D signal images
&lt;/h3&gt;&lt;p&gt;We now consider 2D signals $y$, such as images. The total variation norm proposed by the 1992 paper is
&lt;/p&gt;
$$V(y) = \sum\_{i,j}\sqrt{\vert y\_{i+1,j}-y\_{i,j}\vert ^2 + \vert y\_{i, j+1} - y\_{i, j}\vert ^2}$$&lt;p&gt;
and is isotropic and not differentiable. A variation that is sometimes used, since it may sometimes be easier to minimize, is an anisotropic version
&lt;/p&gt;
$$V\_{aniso}(y) = \sum\_{i,j}\sqrt{\vert y\_{i+1,j}-y\_{i,j}\vert ^2} + \sqrt{\vert y\_{i,j+1} - y\_{i,j}\vert ^2} = \sum\_{i,j}\vert y\_{i+1,j}-y\_{i,j}\vert + \vert y\_{i,j+1}-y\_{i,j}\vert $$&lt;p&gt;
The standard total variation denoising problem is still of the form
&lt;/p&gt;
$$\min\_yE(x,y)+\lambda V(y)$$&lt;p&gt;
where $E$ is the 2D L2 norm. In contrast to the 1D case, solving this denoising is non-trivial. A recent algorithm that solves this is known as the primal dual method.
Due in part to much research in compressed sensing in the mid-2000s, there are many algorithms, such as the split-Bregman method, that solve variants of this problem.&lt;/p&gt;
&lt;p&gt;不过我个人在实现的时候，实现了两个版本，一个是增加了total variation denoising，另一个是没增加total variation denoising的a。
代码如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt; 1
&lt;/span&gt;&lt;span class="lnt"&gt; 2
&lt;/span&gt;&lt;span class="lnt"&gt; 3
&lt;/span&gt;&lt;span class="lnt"&gt; 4
&lt;/span&gt;&lt;span class="lnt"&gt; 5
&lt;/span&gt;&lt;span class="lnt"&gt; 6
&lt;/span&gt;&lt;span class="lnt"&gt; 7
&lt;/span&gt;&lt;span class="lnt"&gt; 8
&lt;/span&gt;&lt;span class="lnt"&gt; 9
&lt;/span&gt;&lt;span class="lnt"&gt; 10
&lt;/span&gt;&lt;span class="lnt"&gt; 11
&lt;/span&gt;&lt;span class="lnt"&gt; 12
&lt;/span&gt;&lt;span class="lnt"&gt; 13
&lt;/span&gt;&lt;span class="lnt"&gt; 14
&lt;/span&gt;&lt;span class="lnt"&gt; 15
&lt;/span&gt;&lt;span class="lnt"&gt; 16
&lt;/span&gt;&lt;span class="lnt"&gt; 17
&lt;/span&gt;&lt;span class="lnt"&gt; 18
&lt;/span&gt;&lt;span class="lnt"&gt; 19
&lt;/span&gt;&lt;span class="lnt"&gt; 20
&lt;/span&gt;&lt;span class="lnt"&gt; 21
&lt;/span&gt;&lt;span class="lnt"&gt; 22
&lt;/span&gt;&lt;span class="lnt"&gt; 23
&lt;/span&gt;&lt;span class="lnt"&gt; 24
&lt;/span&gt;&lt;span class="lnt"&gt; 25
&lt;/span&gt;&lt;span class="lnt"&gt; 26
&lt;/span&gt;&lt;span class="lnt"&gt; 27
&lt;/span&gt;&lt;span class="lnt"&gt; 28
&lt;/span&gt;&lt;span class="lnt"&gt; 29
&lt;/span&gt;&lt;span class="lnt"&gt; 30
&lt;/span&gt;&lt;span class="lnt"&gt; 31
&lt;/span&gt;&lt;span class="lnt"&gt; 32
&lt;/span&gt;&lt;span class="lnt"&gt; 33
&lt;/span&gt;&lt;span class="lnt"&gt; 34
&lt;/span&gt;&lt;span class="lnt"&gt; 35
&lt;/span&gt;&lt;span class="lnt"&gt; 36
&lt;/span&gt;&lt;span class="lnt"&gt; 37
&lt;/span&gt;&lt;span class="lnt"&gt; 38
&lt;/span&gt;&lt;span class="lnt"&gt; 39
&lt;/span&gt;&lt;span class="lnt"&gt; 40
&lt;/span&gt;&lt;span class="lnt"&gt; 41
&lt;/span&gt;&lt;span class="lnt"&gt; 42
&lt;/span&gt;&lt;span class="lnt"&gt; 43
&lt;/span&gt;&lt;span class="lnt"&gt; 44
&lt;/span&gt;&lt;span class="lnt"&gt; 45
&lt;/span&gt;&lt;span class="lnt"&gt; 46
&lt;/span&gt;&lt;span class="lnt"&gt; 47
&lt;/span&gt;&lt;span class="lnt"&gt; 48
&lt;/span&gt;&lt;span class="lnt"&gt; 49
&lt;/span&gt;&lt;span class="lnt"&gt; 50
&lt;/span&gt;&lt;span class="lnt"&gt; 51
&lt;/span&gt;&lt;span class="lnt"&gt; 52
&lt;/span&gt;&lt;span class="lnt"&gt; 53
&lt;/span&gt;&lt;span class="lnt"&gt; 54
&lt;/span&gt;&lt;span class="lnt"&gt; 55
&lt;/span&gt;&lt;span class="lnt"&gt; 56
&lt;/span&gt;&lt;span class="lnt"&gt; 57
&lt;/span&gt;&lt;span class="lnt"&gt; 58
&lt;/span&gt;&lt;span class="lnt"&gt; 59
&lt;/span&gt;&lt;span class="lnt"&gt; 60
&lt;/span&gt;&lt;span class="lnt"&gt; 61
&lt;/span&gt;&lt;span class="lnt"&gt; 62
&lt;/span&gt;&lt;span class="lnt"&gt; 63
&lt;/span&gt;&lt;span class="lnt"&gt; 64
&lt;/span&gt;&lt;span class="lnt"&gt; 65
&lt;/span&gt;&lt;span class="lnt"&gt; 66
&lt;/span&gt;&lt;span class="lnt"&gt; 67
&lt;/span&gt;&lt;span class="lnt"&gt; 68
&lt;/span&gt;&lt;span class="lnt"&gt; 69
&lt;/span&gt;&lt;span class="lnt"&gt; 70
&lt;/span&gt;&lt;span class="lnt"&gt; 71
&lt;/span&gt;&lt;span class="lnt"&gt; 72
&lt;/span&gt;&lt;span class="lnt"&gt; 73
&lt;/span&gt;&lt;span class="lnt"&gt; 74
&lt;/span&gt;&lt;span class="lnt"&gt; 75
&lt;/span&gt;&lt;span class="lnt"&gt; 76
&lt;/span&gt;&lt;span class="lnt"&gt; 77
&lt;/span&gt;&lt;span class="lnt"&gt; 78
&lt;/span&gt;&lt;span class="lnt"&gt; 79
&lt;/span&gt;&lt;span class="lnt"&gt; 80
&lt;/span&gt;&lt;span class="lnt"&gt; 81
&lt;/span&gt;&lt;span class="lnt"&gt; 82
&lt;/span&gt;&lt;span class="lnt"&gt; 83
&lt;/span&gt;&lt;span class="lnt"&gt; 84
&lt;/span&gt;&lt;span class="lnt"&gt; 85
&lt;/span&gt;&lt;span class="lnt"&gt; 86
&lt;/span&gt;&lt;span class="lnt"&gt; 87
&lt;/span&gt;&lt;span class="lnt"&gt; 88
&lt;/span&gt;&lt;span class="lnt"&gt; 89
&lt;/span&gt;&lt;span class="lnt"&gt; 90
&lt;/span&gt;&lt;span class="lnt"&gt; 91
&lt;/span&gt;&lt;span class="lnt"&gt; 92
&lt;/span&gt;&lt;span class="lnt"&gt; 93
&lt;/span&gt;&lt;span class="lnt"&gt; 94
&lt;/span&gt;&lt;span class="lnt"&gt; 95
&lt;/span&gt;&lt;span class="lnt"&gt; 96
&lt;/span&gt;&lt;span class="lnt"&gt; 97
&lt;/span&gt;&lt;span class="lnt"&gt; 98
&lt;/span&gt;&lt;span class="lnt"&gt; 99
&lt;/span&gt;&lt;span class="lnt"&gt;100
&lt;/span&gt;&lt;span class="lnt"&gt;101
&lt;/span&gt;&lt;span class="lnt"&gt;102
&lt;/span&gt;&lt;span class="lnt"&gt;103
&lt;/span&gt;&lt;span class="lnt"&gt;104
&lt;/span&gt;&lt;span class="lnt"&gt;105
&lt;/span&gt;&lt;span class="lnt"&gt;106
&lt;/span&gt;&lt;span class="lnt"&gt;107
&lt;/span&gt;&lt;span class="lnt"&gt;108
&lt;/span&gt;&lt;span class="lnt"&gt;109
&lt;/span&gt;&lt;span class="lnt"&gt;110
&lt;/span&gt;&lt;span class="lnt"&gt;111
&lt;/span&gt;&lt;span class="lnt"&gt;112
&lt;/span&gt;&lt;span class="lnt"&gt;113
&lt;/span&gt;&lt;span class="lnt"&gt;114
&lt;/span&gt;&lt;span class="lnt"&gt;115
&lt;/span&gt;&lt;span class="lnt"&gt;116
&lt;/span&gt;&lt;span class="lnt"&gt;117
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;mxnet&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;skimage&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;io&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;skimage&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mxnet&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mxnet.gluon.model_zoo&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;vision&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mxnet.gluon&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mxnet&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;autograd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mxnet.gluon&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Trainer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mxnet.gluon&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;warnings&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;warnings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filterwarnings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;ignore&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;content_image_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;../../gluon-tutorial-zh/img/pine-tree.jpg&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;style_image_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;the_starry_night.jpg&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;rgb_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.485&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.456&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.406&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;rgb_std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.229&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.224&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.225&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;preprocessing&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;image_shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu&lt;/span&gt;&lt;span class="p"&gt;()):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;newImage&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;image_shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;newImage&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;newImage&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;newImage&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;newImage&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;rgb_mean&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;rgb_std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;newImage&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;postprocessing&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;newImage&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asnumpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;rgb_std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;rgb_mean&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;newImage&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style_layers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content_layers&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HybridSequential&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style_layers&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;content_layers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pretrained_net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hybridize&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;extract_features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content_layers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style_layers&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;content_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;style_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;content_layers&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;content_results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;style_layers&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;style_results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;content_results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style_results&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;content_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content_results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content_target&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;losses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content_results&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;losses&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;content_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;content_target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_n&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;losses&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature_map&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;feature_map&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;M&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature_map&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;new_feature_map&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;feature_map&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_feature_map&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;new_feature_map&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;style_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style_results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style_target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;losses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style_results&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;l&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;style_target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; \
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;losses&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_n&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;losses&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content_loss_result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style_loss_result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratio&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;content_loss_result&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ratio&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;style_loss_result&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;style_layers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;34&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 这里与论文不同，我选的层比论文给出的更深，为了捕捉到更抽象的style&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;content_layers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style_layers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content_layers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;content_image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content_image_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;style_image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style_image_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;pretrained_net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vgg19&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pretrained&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;ctx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gpu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;collect_params&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_ctx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;content_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preprocessing&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content_image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;300&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;style_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preprocessing&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style_image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;300&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;output&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;content_img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# output.set_data(nd.random_normal(shape = content_img.shape).abs())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content_img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;content_img_result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content_layers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style_layers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style_img_result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content_layers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style_layers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;content_results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;content_layers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style_layers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;style_target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;gram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;style_img_result&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;trainer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Trainer&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;adam&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;learning_rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;beta1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;beta2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.99&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;autograd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;record&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;content_results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;content_layers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style_layers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content_results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content_img_result&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;style_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style_results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style_target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="mf"&gt;1e-4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asscalar&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;postprocessing&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;这里在实现的时候，使用了这个2D图像的total variation denoising，也就是，每个像素应尽可能的与左侧和上方的像素相近。所以最后的优化目标是三部分组成，第一部分是content loss，第二部分是style loss，第三部分是total variation loss。
研究一下mxnet给出的example
model_vgg19.py&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt; 1
&lt;/span&gt;&lt;span class="lnt"&gt; 2
&lt;/span&gt;&lt;span class="lnt"&gt; 3
&lt;/span&gt;&lt;span class="lnt"&gt; 4
&lt;/span&gt;&lt;span class="lnt"&gt; 5
&lt;/span&gt;&lt;span class="lnt"&gt; 6
&lt;/span&gt;&lt;span class="lnt"&gt; 7
&lt;/span&gt;&lt;span class="lnt"&gt; 8
&lt;/span&gt;&lt;span class="lnt"&gt; 9
&lt;/span&gt;&lt;span class="lnt"&gt;10
&lt;/span&gt;&lt;span class="lnt"&gt;11
&lt;/span&gt;&lt;span class="lnt"&gt;12
&lt;/span&gt;&lt;span class="lnt"&gt;13
&lt;/span&gt;&lt;span class="lnt"&gt;14
&lt;/span&gt;&lt;span class="lnt"&gt;15
&lt;/span&gt;&lt;span class="lnt"&gt;16
&lt;/span&gt;&lt;span class="lnt"&gt;17
&lt;/span&gt;&lt;span class="lnt"&gt;18
&lt;/span&gt;&lt;span class="lnt"&gt;19
&lt;/span&gt;&lt;span class="lnt"&gt;20
&lt;/span&gt;&lt;span class="lnt"&gt;21
&lt;/span&gt;&lt;span class="lnt"&gt;22
&lt;/span&gt;&lt;span class="lnt"&gt;23
&lt;/span&gt;&lt;span class="lnt"&gt;24
&lt;/span&gt;&lt;span class="lnt"&gt;25
&lt;/span&gt;&lt;span class="lnt"&gt;26
&lt;/span&gt;&lt;span class="lnt"&gt;27
&lt;/span&gt;&lt;span class="lnt"&gt;28
&lt;/span&gt;&lt;span class="lnt"&gt;29
&lt;/span&gt;&lt;span class="lnt"&gt;30
&lt;/span&gt;&lt;span class="lnt"&gt;31
&lt;/span&gt;&lt;span class="lnt"&gt;32
&lt;/span&gt;&lt;span class="lnt"&gt;33
&lt;/span&gt;&lt;span class="lnt"&gt;34
&lt;/span&gt;&lt;span class="lnt"&gt;35
&lt;/span&gt;&lt;span class="lnt"&gt;36
&lt;/span&gt;&lt;span class="lnt"&gt;37
&lt;/span&gt;&lt;span class="lnt"&gt;38
&lt;/span&gt;&lt;span class="lnt"&gt;39
&lt;/span&gt;&lt;span class="lnt"&gt;40
&lt;/span&gt;&lt;span class="lnt"&gt;41
&lt;/span&gt;&lt;span class="lnt"&gt;42
&lt;/span&gt;&lt;span class="lnt"&gt;43
&lt;/span&gt;&lt;span class="lnt"&gt;44
&lt;/span&gt;&lt;span class="lnt"&gt;45
&lt;/span&gt;&lt;span class="lnt"&gt;46
&lt;/span&gt;&lt;span class="lnt"&gt;47
&lt;/span&gt;&lt;span class="lnt"&gt;48
&lt;/span&gt;&lt;span class="lnt"&gt;49
&lt;/span&gt;&lt;span class="lnt"&gt;50
&lt;/span&gt;&lt;span class="lnt"&gt;51
&lt;/span&gt;&lt;span class="lnt"&gt;52
&lt;/span&gt;&lt;span class="lnt"&gt;53
&lt;/span&gt;&lt;span class="lnt"&gt;54
&lt;/span&gt;&lt;span class="lnt"&gt;55
&lt;/span&gt;&lt;span class="lnt"&gt;56
&lt;/span&gt;&lt;span class="lnt"&gt;57
&lt;/span&gt;&lt;span class="lnt"&gt;58
&lt;/span&gt;&lt;span class="lnt"&gt;59
&lt;/span&gt;&lt;span class="lnt"&gt;60
&lt;/span&gt;&lt;span class="lnt"&gt;61
&lt;/span&gt;&lt;span class="lnt"&gt;62
&lt;/span&gt;&lt;span class="lnt"&gt;63
&lt;/span&gt;&lt;span class="lnt"&gt;64
&lt;/span&gt;&lt;span class="lnt"&gt;65
&lt;/span&gt;&lt;span class="lnt"&gt;66
&lt;/span&gt;&lt;span class="lnt"&gt;67
&lt;/span&gt;&lt;span class="lnt"&gt;68
&lt;/span&gt;&lt;span class="lnt"&gt;69
&lt;/span&gt;&lt;span class="lnt"&gt;70
&lt;/span&gt;&lt;span class="lnt"&gt;71
&lt;/span&gt;&lt;span class="lnt"&gt;72
&lt;/span&gt;&lt;span class="lnt"&gt;73
&lt;/span&gt;&lt;span class="lnt"&gt;74
&lt;/span&gt;&lt;span class="lnt"&gt;75
&lt;/span&gt;&lt;span class="lnt"&gt;76
&lt;/span&gt;&lt;span class="lnt"&gt;77
&lt;/span&gt;&lt;span class="lnt"&gt;78
&lt;/span&gt;&lt;span class="lnt"&gt;79
&lt;/span&gt;&lt;span class="lnt"&gt;80
&lt;/span&gt;&lt;span class="lnt"&gt;81
&lt;/span&gt;&lt;span class="lnt"&gt;82
&lt;/span&gt;&lt;span class="lnt"&gt;83
&lt;/span&gt;&lt;span class="lnt"&gt;84
&lt;/span&gt;&lt;span class="lnt"&gt;85
&lt;/span&gt;&lt;span class="lnt"&gt;86
&lt;/span&gt;&lt;span class="lnt"&gt;87
&lt;/span&gt;&lt;span class="lnt"&gt;88
&lt;/span&gt;&lt;span class="lnt"&gt;89
&lt;/span&gt;&lt;span class="lnt"&gt;90
&lt;/span&gt;&lt;span class="lnt"&gt;91
&lt;/span&gt;&lt;span class="lnt"&gt;92
&lt;/span&gt;&lt;span class="lnt"&gt;93
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Licensed to the Apache Software Foundation (ASF) under one&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# or more contributor license agreements. See the NOTICE file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# distributed with this work for additional information&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# regarding copyright ownership. The ASF licenses this file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# to you under the Apache License, Version 2.0 (the&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# &amp;#34;License&amp;#34;); you may not use this file except in compliance&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# with the License. You may obtain a copy of the License at&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# http://www.apache.org/licenses/LICENSE-2.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Unless required by applicable law or agreed to in writing,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# software distributed under the License is distributed on an&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# &amp;#34;AS IS&amp;#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# KIND, either express or implied. See the License for the&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# specific language governing permissions and limitations&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# under the License.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;find_mxnet&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;mxnet&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;namedtuple&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;ConvExecutor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;namedtuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ConvExecutor&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;executor&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;data_grad&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;style&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;arg_dict&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_symbol&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# declare symbol&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;data&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv1_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;relu1_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu1_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conv1_1&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv1_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1_2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;relu1_1&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;relu1_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu1_2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conv1_2&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;pool1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Pooling&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pool1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;relu1_2&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;pool_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;avg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv2_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv2_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pool1&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;relu2_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu2_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conv2_1&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv2_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv2_2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;relu2_1&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;relu2_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu2_2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conv2_2&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;pool2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Pooling&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pool2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;relu2_2&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;pool_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;avg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv3_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pool2&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;relu3_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu3_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conv3_1&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv3_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3_2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;relu3_1&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;relu3_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu3_2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conv3_2&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv3_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3_3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;relu3_2&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;relu3_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu3_3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conv3_3&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv3_4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3_4&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;relu3_3&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;relu3_4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu3_4&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conv3_4&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;pool3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Pooling&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pool3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;relu3_4&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;pool_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;avg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv4_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv4_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pool3&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;relu4_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu4_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conv4_1&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv4_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv4_2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;relu4_1&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;relu4_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu4_2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conv4_2&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv4_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv4_3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;relu4_2&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;relu4_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu4_3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conv4_3&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv4_4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv4_4&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;relu4_3&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;relu4_4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu4_4&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conv4_4&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;pool4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Pooling&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pool4&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;relu4_4&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;pool_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;avg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;conv5_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv5_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pool4&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;relu5_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu5_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conv5_1&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;act_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# style and content layers&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;style&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Group&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;relu1_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;relu2_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;relu3_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;relu4_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;relu5_1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;content&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Group&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;relu4_2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_executor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Group&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# make executor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;arg_shapes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_shapes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;aux_shapes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;infer_shape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;arg_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;list_arguments&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;arg_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arg_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;arg_shapes&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;grad_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;data&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;arg_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;data&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copyto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# init with pretrained weight&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;pretrained&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;./model/vgg19.params&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;arg_names&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;data&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;continue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;arg:&amp;#34;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;pretrained&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;pretrained&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copyto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arg_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;Skip argument &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;executor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;arg_dict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args_grad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;grad_dict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;grad_req&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;write&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ConvExecutor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;executor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;executor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;arg_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;data&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;data_grad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;grad_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;data&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;arg_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;arg_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_symbol&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;get_executor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;nstyle.py&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt; 1
&lt;/span&gt;&lt;span class="lnt"&gt; 2
&lt;/span&gt;&lt;span class="lnt"&gt; 3
&lt;/span&gt;&lt;span class="lnt"&gt; 4
&lt;/span&gt;&lt;span class="lnt"&gt; 5
&lt;/span&gt;&lt;span class="lnt"&gt; 6
&lt;/span&gt;&lt;span class="lnt"&gt; 7
&lt;/span&gt;&lt;span class="lnt"&gt; 8
&lt;/span&gt;&lt;span class="lnt"&gt; 9
&lt;/span&gt;&lt;span class="lnt"&gt; 10
&lt;/span&gt;&lt;span class="lnt"&gt; 11
&lt;/span&gt;&lt;span class="lnt"&gt; 12
&lt;/span&gt;&lt;span class="lnt"&gt; 13
&lt;/span&gt;&lt;span class="lnt"&gt; 14
&lt;/span&gt;&lt;span class="lnt"&gt; 15
&lt;/span&gt;&lt;span class="lnt"&gt; 16
&lt;/span&gt;&lt;span class="lnt"&gt; 17
&lt;/span&gt;&lt;span class="lnt"&gt; 18
&lt;/span&gt;&lt;span class="lnt"&gt; 19
&lt;/span&gt;&lt;span class="lnt"&gt; 20
&lt;/span&gt;&lt;span class="lnt"&gt; 21
&lt;/span&gt;&lt;span class="lnt"&gt; 22
&lt;/span&gt;&lt;span class="lnt"&gt; 23
&lt;/span&gt;&lt;span class="lnt"&gt; 24
&lt;/span&gt;&lt;span class="lnt"&gt; 25
&lt;/span&gt;&lt;span class="lnt"&gt; 26
&lt;/span&gt;&lt;span class="lnt"&gt; 27
&lt;/span&gt;&lt;span class="lnt"&gt; 28
&lt;/span&gt;&lt;span class="lnt"&gt; 29
&lt;/span&gt;&lt;span class="lnt"&gt; 30
&lt;/span&gt;&lt;span class="lnt"&gt; 31
&lt;/span&gt;&lt;span class="lnt"&gt; 32
&lt;/span&gt;&lt;span class="lnt"&gt; 33
&lt;/span&gt;&lt;span class="lnt"&gt; 34
&lt;/span&gt;&lt;span class="lnt"&gt; 35
&lt;/span&gt;&lt;span class="lnt"&gt; 36
&lt;/span&gt;&lt;span class="lnt"&gt; 37
&lt;/span&gt;&lt;span class="lnt"&gt; 38
&lt;/span&gt;&lt;span class="lnt"&gt; 39
&lt;/span&gt;&lt;span class="lnt"&gt; 40
&lt;/span&gt;&lt;span class="lnt"&gt; 41
&lt;/span&gt;&lt;span class="lnt"&gt; 42
&lt;/span&gt;&lt;span class="lnt"&gt; 43
&lt;/span&gt;&lt;span class="lnt"&gt; 44
&lt;/span&gt;&lt;span class="lnt"&gt; 45
&lt;/span&gt;&lt;span class="lnt"&gt; 46
&lt;/span&gt;&lt;span class="lnt"&gt; 47
&lt;/span&gt;&lt;span class="lnt"&gt; 48
&lt;/span&gt;&lt;span class="lnt"&gt; 49
&lt;/span&gt;&lt;span class="lnt"&gt; 50
&lt;/span&gt;&lt;span class="lnt"&gt; 51
&lt;/span&gt;&lt;span class="lnt"&gt; 52
&lt;/span&gt;&lt;span class="lnt"&gt; 53
&lt;/span&gt;&lt;span class="lnt"&gt; 54
&lt;/span&gt;&lt;span class="lnt"&gt; 55
&lt;/span&gt;&lt;span class="lnt"&gt; 56
&lt;/span&gt;&lt;span class="lnt"&gt; 57
&lt;/span&gt;&lt;span class="lnt"&gt; 58
&lt;/span&gt;&lt;span class="lnt"&gt; 59
&lt;/span&gt;&lt;span class="lnt"&gt; 60
&lt;/span&gt;&lt;span class="lnt"&gt; 61
&lt;/span&gt;&lt;span class="lnt"&gt; 62
&lt;/span&gt;&lt;span class="lnt"&gt; 63
&lt;/span&gt;&lt;span class="lnt"&gt; 64
&lt;/span&gt;&lt;span class="lnt"&gt; 65
&lt;/span&gt;&lt;span class="lnt"&gt; 66
&lt;/span&gt;&lt;span class="lnt"&gt; 67
&lt;/span&gt;&lt;span class="lnt"&gt; 68
&lt;/span&gt;&lt;span class="lnt"&gt; 69
&lt;/span&gt;&lt;span class="lnt"&gt; 70
&lt;/span&gt;&lt;span class="lnt"&gt; 71
&lt;/span&gt;&lt;span class="lnt"&gt; 72
&lt;/span&gt;&lt;span class="lnt"&gt; 73
&lt;/span&gt;&lt;span class="lnt"&gt; 74
&lt;/span&gt;&lt;span class="lnt"&gt; 75
&lt;/span&gt;&lt;span class="lnt"&gt; 76
&lt;/span&gt;&lt;span class="lnt"&gt; 77
&lt;/span&gt;&lt;span class="lnt"&gt; 78
&lt;/span&gt;&lt;span class="lnt"&gt; 79
&lt;/span&gt;&lt;span class="lnt"&gt; 80
&lt;/span&gt;&lt;span class="lnt"&gt; 81
&lt;/span&gt;&lt;span class="lnt"&gt; 82
&lt;/span&gt;&lt;span class="lnt"&gt; 83
&lt;/span&gt;&lt;span class="lnt"&gt; 84
&lt;/span&gt;&lt;span class="lnt"&gt; 85
&lt;/span&gt;&lt;span class="lnt"&gt; 86
&lt;/span&gt;&lt;span class="lnt"&gt; 87
&lt;/span&gt;&lt;span class="lnt"&gt; 88
&lt;/span&gt;&lt;span class="lnt"&gt; 89
&lt;/span&gt;&lt;span class="lnt"&gt; 90
&lt;/span&gt;&lt;span class="lnt"&gt; 91
&lt;/span&gt;&lt;span class="lnt"&gt; 92
&lt;/span&gt;&lt;span class="lnt"&gt; 93
&lt;/span&gt;&lt;span class="lnt"&gt; 94
&lt;/span&gt;&lt;span class="lnt"&gt; 95
&lt;/span&gt;&lt;span class="lnt"&gt; 96
&lt;/span&gt;&lt;span class="lnt"&gt; 97
&lt;/span&gt;&lt;span class="lnt"&gt; 98
&lt;/span&gt;&lt;span class="lnt"&gt; 99
&lt;/span&gt;&lt;span class="lnt"&gt;100
&lt;/span&gt;&lt;span class="lnt"&gt;101
&lt;/span&gt;&lt;span class="lnt"&gt;102
&lt;/span&gt;&lt;span class="lnt"&gt;103
&lt;/span&gt;&lt;span class="lnt"&gt;104
&lt;/span&gt;&lt;span class="lnt"&gt;105
&lt;/span&gt;&lt;span class="lnt"&gt;106
&lt;/span&gt;&lt;span class="lnt"&gt;107
&lt;/span&gt;&lt;span class="lnt"&gt;108
&lt;/span&gt;&lt;span class="lnt"&gt;109
&lt;/span&gt;&lt;span class="lnt"&gt;110
&lt;/span&gt;&lt;span class="lnt"&gt;111
&lt;/span&gt;&lt;span class="lnt"&gt;112
&lt;/span&gt;&lt;span class="lnt"&gt;113
&lt;/span&gt;&lt;span class="lnt"&gt;114
&lt;/span&gt;&lt;span class="lnt"&gt;115
&lt;/span&gt;&lt;span class="lnt"&gt;116
&lt;/span&gt;&lt;span class="lnt"&gt;117
&lt;/span&gt;&lt;span class="lnt"&gt;118
&lt;/span&gt;&lt;span class="lnt"&gt;119
&lt;/span&gt;&lt;span class="lnt"&gt;120
&lt;/span&gt;&lt;span class="lnt"&gt;121
&lt;/span&gt;&lt;span class="lnt"&gt;122
&lt;/span&gt;&lt;span class="lnt"&gt;123
&lt;/span&gt;&lt;span class="lnt"&gt;124
&lt;/span&gt;&lt;span class="lnt"&gt;125
&lt;/span&gt;&lt;span class="lnt"&gt;126
&lt;/span&gt;&lt;span class="lnt"&gt;127
&lt;/span&gt;&lt;span class="lnt"&gt;128
&lt;/span&gt;&lt;span class="lnt"&gt;129
&lt;/span&gt;&lt;span class="lnt"&gt;130
&lt;/span&gt;&lt;span class="lnt"&gt;131
&lt;/span&gt;&lt;span class="lnt"&gt;132
&lt;/span&gt;&lt;span class="lnt"&gt;133
&lt;/span&gt;&lt;span class="lnt"&gt;134
&lt;/span&gt;&lt;span class="lnt"&gt;135
&lt;/span&gt;&lt;span class="lnt"&gt;136
&lt;/span&gt;&lt;span class="lnt"&gt;137
&lt;/span&gt;&lt;span class="lnt"&gt;138
&lt;/span&gt;&lt;span class="lnt"&gt;139
&lt;/span&gt;&lt;span class="lnt"&gt;140
&lt;/span&gt;&lt;span class="lnt"&gt;141
&lt;/span&gt;&lt;span class="lnt"&gt;142
&lt;/span&gt;&lt;span class="lnt"&gt;143
&lt;/span&gt;&lt;span class="lnt"&gt;144
&lt;/span&gt;&lt;span class="lnt"&gt;145
&lt;/span&gt;&lt;span class="lnt"&gt;146
&lt;/span&gt;&lt;span class="lnt"&gt;147
&lt;/span&gt;&lt;span class="lnt"&gt;148
&lt;/span&gt;&lt;span class="lnt"&gt;149
&lt;/span&gt;&lt;span class="lnt"&gt;150
&lt;/span&gt;&lt;span class="lnt"&gt;151
&lt;/span&gt;&lt;span class="lnt"&gt;152
&lt;/span&gt;&lt;span class="lnt"&gt;153
&lt;/span&gt;&lt;span class="lnt"&gt;154
&lt;/span&gt;&lt;span class="lnt"&gt;155
&lt;/span&gt;&lt;span class="lnt"&gt;156
&lt;/span&gt;&lt;span class="lnt"&gt;157
&lt;/span&gt;&lt;span class="lnt"&gt;158
&lt;/span&gt;&lt;span class="lnt"&gt;159
&lt;/span&gt;&lt;span class="lnt"&gt;160
&lt;/span&gt;&lt;span class="lnt"&gt;161
&lt;/span&gt;&lt;span class="lnt"&gt;162
&lt;/span&gt;&lt;span class="lnt"&gt;163
&lt;/span&gt;&lt;span class="lnt"&gt;164
&lt;/span&gt;&lt;span class="lnt"&gt;165
&lt;/span&gt;&lt;span class="lnt"&gt;166
&lt;/span&gt;&lt;span class="lnt"&gt;167
&lt;/span&gt;&lt;span class="lnt"&gt;168
&lt;/span&gt;&lt;span class="lnt"&gt;169
&lt;/span&gt;&lt;span class="lnt"&gt;170
&lt;/span&gt;&lt;span class="lnt"&gt;171
&lt;/span&gt;&lt;span class="lnt"&gt;172
&lt;/span&gt;&lt;span class="lnt"&gt;173
&lt;/span&gt;&lt;span class="lnt"&gt;174
&lt;/span&gt;&lt;span class="lnt"&gt;175
&lt;/span&gt;&lt;span class="lnt"&gt;176
&lt;/span&gt;&lt;span class="lnt"&gt;177
&lt;/span&gt;&lt;span class="lnt"&gt;178
&lt;/span&gt;&lt;span class="lnt"&gt;179
&lt;/span&gt;&lt;span class="lnt"&gt;180
&lt;/span&gt;&lt;span class="lnt"&gt;181
&lt;/span&gt;&lt;span class="lnt"&gt;182
&lt;/span&gt;&lt;span class="lnt"&gt;183
&lt;/span&gt;&lt;span class="lnt"&gt;184
&lt;/span&gt;&lt;span class="lnt"&gt;185
&lt;/span&gt;&lt;span class="lnt"&gt;186
&lt;/span&gt;&lt;span class="lnt"&gt;187
&lt;/span&gt;&lt;span class="lnt"&gt;188
&lt;/span&gt;&lt;span class="lnt"&gt;189
&lt;/span&gt;&lt;span class="lnt"&gt;190
&lt;/span&gt;&lt;span class="lnt"&gt;191
&lt;/span&gt;&lt;span class="lnt"&gt;192
&lt;/span&gt;&lt;span class="lnt"&gt;193
&lt;/span&gt;&lt;span class="lnt"&gt;194
&lt;/span&gt;&lt;span class="lnt"&gt;195
&lt;/span&gt;&lt;span class="lnt"&gt;196
&lt;/span&gt;&lt;span class="lnt"&gt;197
&lt;/span&gt;&lt;span class="lnt"&gt;198
&lt;/span&gt;&lt;span class="lnt"&gt;199
&lt;/span&gt;&lt;span class="lnt"&gt;200
&lt;/span&gt;&lt;span class="lnt"&gt;201
&lt;/span&gt;&lt;span class="lnt"&gt;202
&lt;/span&gt;&lt;span class="lnt"&gt;203
&lt;/span&gt;&lt;span class="lnt"&gt;204
&lt;/span&gt;&lt;span class="lnt"&gt;205
&lt;/span&gt;&lt;span class="lnt"&gt;206
&lt;/span&gt;&lt;span class="lnt"&gt;207
&lt;/span&gt;&lt;span class="lnt"&gt;208
&lt;/span&gt;&lt;span class="lnt"&gt;209
&lt;/span&gt;&lt;span class="lnt"&gt;210
&lt;/span&gt;&lt;span class="lnt"&gt;211
&lt;/span&gt;&lt;span class="lnt"&gt;212
&lt;/span&gt;&lt;span class="lnt"&gt;213
&lt;/span&gt;&lt;span class="lnt"&gt;214
&lt;/span&gt;&lt;span class="lnt"&gt;215
&lt;/span&gt;&lt;span class="lnt"&gt;216
&lt;/span&gt;&lt;span class="lnt"&gt;217
&lt;/span&gt;&lt;span class="lnt"&gt;218
&lt;/span&gt;&lt;span class="lnt"&gt;219
&lt;/span&gt;&lt;span class="lnt"&gt;220
&lt;/span&gt;&lt;span class="lnt"&gt;221
&lt;/span&gt;&lt;span class="lnt"&gt;222
&lt;/span&gt;&lt;span class="lnt"&gt;223
&lt;/span&gt;&lt;span class="lnt"&gt;224
&lt;/span&gt;&lt;span class="lnt"&gt;225
&lt;/span&gt;&lt;span class="lnt"&gt;226
&lt;/span&gt;&lt;span class="lnt"&gt;227
&lt;/span&gt;&lt;span class="lnt"&gt;228
&lt;/span&gt;&lt;span class="lnt"&gt;229
&lt;/span&gt;&lt;span class="lnt"&gt;230
&lt;/span&gt;&lt;span class="lnt"&gt;231
&lt;/span&gt;&lt;span class="lnt"&gt;232
&lt;/span&gt;&lt;span class="lnt"&gt;233
&lt;/span&gt;&lt;span class="lnt"&gt;234
&lt;/span&gt;&lt;span class="lnt"&gt;235
&lt;/span&gt;&lt;span class="lnt"&gt;236
&lt;/span&gt;&lt;span class="lnt"&gt;237
&lt;/span&gt;&lt;span class="lnt"&gt;238
&lt;/span&gt;&lt;span class="lnt"&gt;239
&lt;/span&gt;&lt;span class="lnt"&gt;240
&lt;/span&gt;&lt;span class="lnt"&gt;241
&lt;/span&gt;&lt;span class="lnt"&gt;242
&lt;/span&gt;&lt;span class="lnt"&gt;243
&lt;/span&gt;&lt;span class="lnt"&gt;244
&lt;/span&gt;&lt;span class="lnt"&gt;245
&lt;/span&gt;&lt;span class="lnt"&gt;246
&lt;/span&gt;&lt;span class="lnt"&gt;247
&lt;/span&gt;&lt;span class="lnt"&gt;248
&lt;/span&gt;&lt;span class="lnt"&gt;249
&lt;/span&gt;&lt;span class="lnt"&gt;250
&lt;/span&gt;&lt;span class="lnt"&gt;251
&lt;/span&gt;&lt;span class="lnt"&gt;252
&lt;/span&gt;&lt;span class="lnt"&gt;253
&lt;/span&gt;&lt;span class="lnt"&gt;254
&lt;/span&gt;&lt;span class="lnt"&gt;255
&lt;/span&gt;&lt;span class="lnt"&gt;256
&lt;/span&gt;&lt;span class="lnt"&gt;257
&lt;/span&gt;&lt;span class="lnt"&gt;258
&lt;/span&gt;&lt;span class="lnt"&gt;259
&lt;/span&gt;&lt;span class="lnt"&gt;260
&lt;/span&gt;&lt;span class="lnt"&gt;261
&lt;/span&gt;&lt;span class="lnt"&gt;262
&lt;/span&gt;&lt;span class="lnt"&gt;263
&lt;/span&gt;&lt;span class="lnt"&gt;264
&lt;/span&gt;&lt;span class="lnt"&gt;265
&lt;/span&gt;&lt;span class="lnt"&gt;266
&lt;/span&gt;&lt;span class="lnt"&gt;267
&lt;/span&gt;&lt;span class="lnt"&gt;268
&lt;/span&gt;&lt;span class="lnt"&gt;269
&lt;/span&gt;&lt;span class="lnt"&gt;270
&lt;/span&gt;&lt;span class="lnt"&gt;271
&lt;/span&gt;&lt;span class="lnt"&gt;272
&lt;/span&gt;&lt;span class="lnt"&gt;273
&lt;/span&gt;&lt;span class="lnt"&gt;274
&lt;/span&gt;&lt;span class="lnt"&gt;275
&lt;/span&gt;&lt;span class="lnt"&gt;276
&lt;/span&gt;&lt;span class="lnt"&gt;277
&lt;/span&gt;&lt;span class="lnt"&gt;278
&lt;/span&gt;&lt;span class="lnt"&gt;279
&lt;/span&gt;&lt;span class="lnt"&gt;280
&lt;/span&gt;&lt;span class="lnt"&gt;281
&lt;/span&gt;&lt;span class="lnt"&gt;282
&lt;/span&gt;&lt;span class="lnt"&gt;283
&lt;/span&gt;&lt;span class="lnt"&gt;284
&lt;/span&gt;&lt;span class="lnt"&gt;285
&lt;/span&gt;&lt;span class="lnt"&gt;286
&lt;/span&gt;&lt;span class="lnt"&gt;287
&lt;/span&gt;&lt;span class="lnt"&gt;288
&lt;/span&gt;&lt;span class="lnt"&gt;289
&lt;/span&gt;&lt;span class="lnt"&gt;290
&lt;/span&gt;&lt;span class="lnt"&gt;291
&lt;/span&gt;&lt;span class="lnt"&gt;292
&lt;/span&gt;&lt;span class="lnt"&gt;293
&lt;/span&gt;&lt;span class="lnt"&gt;294
&lt;/span&gt;&lt;span class="lnt"&gt;295
&lt;/span&gt;&lt;span class="lnt"&gt;296
&lt;/span&gt;&lt;span class="lnt"&gt;297
&lt;/span&gt;&lt;span class="lnt"&gt;298
&lt;/span&gt;&lt;span class="lnt"&gt;299
&lt;/span&gt;&lt;span class="lnt"&gt;300
&lt;/span&gt;&lt;span class="lnt"&gt;301
&lt;/span&gt;&lt;span class="lnt"&gt;302
&lt;/span&gt;&lt;span class="lnt"&gt;303
&lt;/span&gt;&lt;span class="lnt"&gt;304
&lt;/span&gt;&lt;span class="lnt"&gt;305
&lt;/span&gt;&lt;span class="lnt"&gt;306
&lt;/span&gt;&lt;span class="lnt"&gt;307
&lt;/span&gt;&lt;span class="lnt"&gt;308
&lt;/span&gt;&lt;span class="lnt"&gt;309
&lt;/span&gt;&lt;span class="lnt"&gt;310
&lt;/span&gt;&lt;span class="lnt"&gt;311
&lt;/span&gt;&lt;span class="lnt"&gt;312
&lt;/span&gt;&lt;span class="lnt"&gt;313
&lt;/span&gt;&lt;span class="lnt"&gt;314
&lt;/span&gt;&lt;span class="lnt"&gt;315
&lt;/span&gt;&lt;span class="lnt"&gt;316
&lt;/span&gt;&lt;span class="lnt"&gt;317
&lt;/span&gt;&lt;span class="lnt"&gt;318
&lt;/span&gt;&lt;span class="lnt"&gt;319
&lt;/span&gt;&lt;span class="lnt"&gt;320
&lt;/span&gt;&lt;span class="lnt"&gt;321
&lt;/span&gt;&lt;span class="lnt"&gt;322
&lt;/span&gt;&lt;span class="lnt"&gt;323
&lt;/span&gt;&lt;span class="lnt"&gt;324
&lt;/span&gt;&lt;span class="lnt"&gt;325
&lt;/span&gt;&lt;span class="lnt"&gt;326
&lt;/span&gt;&lt;span class="lnt"&gt;327
&lt;/span&gt;&lt;span class="lnt"&gt;328
&lt;/span&gt;&lt;span class="lnt"&gt;329
&lt;/span&gt;&lt;span class="lnt"&gt;330
&lt;/span&gt;&lt;span class="lnt"&gt;331
&lt;/span&gt;&lt;span class="lnt"&gt;332
&lt;/span&gt;&lt;span class="lnt"&gt;333
&lt;/span&gt;&lt;span class="lnt"&gt;334
&lt;/span&gt;&lt;span class="lnt"&gt;335
&lt;/span&gt;&lt;span class="lnt"&gt;336
&lt;/span&gt;&lt;span class="lnt"&gt;337
&lt;/span&gt;&lt;span class="lnt"&gt;338
&lt;/span&gt;&lt;span class="lnt"&gt;339
&lt;/span&gt;&lt;span class="lnt"&gt;340
&lt;/span&gt;&lt;span class="lnt"&gt;341
&lt;/span&gt;&lt;span class="lnt"&gt;342
&lt;/span&gt;&lt;span class="lnt"&gt;343
&lt;/span&gt;&lt;span class="lnt"&gt;344
&lt;/span&gt;&lt;span class="lnt"&gt;345
&lt;/span&gt;&lt;span class="lnt"&gt;346
&lt;/span&gt;&lt;span class="lnt"&gt;347
&lt;/span&gt;&lt;span class="lnt"&gt;348
&lt;/span&gt;&lt;span class="lnt"&gt;349
&lt;/span&gt;&lt;span class="lnt"&gt;350
&lt;/span&gt;&lt;span class="lnt"&gt;351
&lt;/span&gt;&lt;span class="lnt"&gt;352
&lt;/span&gt;&lt;span class="lnt"&gt;353
&lt;/span&gt;&lt;span class="lnt"&gt;354
&lt;/span&gt;&lt;span class="lnt"&gt;355
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Licensed to the Apache Software Foundation (ASF) under one&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# or more contributor license agreements. See the NOTICE file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# distributed with this work for additional information&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# regarding copyright ownership. The ASF licenses this file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# to you under the Apache License, Version 2.0 (the&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# &amp;#34;License&amp;#34;); you may not use this file except in compliance&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# with the License. You may obtain a copy of the License at&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# http://www.apache.org/licenses/LICENSE-2.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Unless required by applicable law or agreed to in writing,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# software distributed under the License is distributed on an&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# &amp;#34;AS IS&amp;#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# KIND, either express or implied. See the License for the&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# specific language governing permissions and limitations&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# under the License.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;find_mxnet&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;mxnet&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;importlib&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;logging&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;basicConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DEBUG&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;argparse&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;namedtuple&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;skimage&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;skimage.restoration&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;denoise_tv_chambolle&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;CallbackData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;namedtuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;CallbackData&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;field_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;eps&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;epoch&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;img&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;filename&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_args&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arglist&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;argparse&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ArgumentParser&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;neural style&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 选择模型，默认是VGG19&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--model&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;vgg19&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;choices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;vgg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;the pretrained model to use&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 内容图片的路径&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--content-image&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;input/IMG_4343.jpg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;the content image&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 风格图片的路径&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--style-image&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;input/starry_night.jpg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;the style image&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 停止迭代的阈值，若relative change小于这个数就停止迭代&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--stop-eps&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.005&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;stop if the relative chanage is less than eps&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 内容图片在loss上的权重&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--content-weight&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;the weight for the content image&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 风格图片在loss上的权重&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--style-weight&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;the weight for the style image&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# total variation在loss上的权重&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--tv-weight&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;the magtitute on TV loss&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 最大迭代次数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--max-num-epochs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;the maximal number of training epochs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--max-long-edge&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;600&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;resize the content image&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 初始的学习率&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--lr&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;the initial learning rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 使用哪块GPU&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--gpu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;which gpu card to use, -1 means using cpu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 输出图像的路径&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--output_dir&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;output/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;the output image&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 每多少轮保存一次当前的输出结果&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--save-epochs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;save the output every n epochs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--remove-noise&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.02&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;the magtitute to remove noise&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 每迭代多少轮减小一下学习率&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--lr-sched-delay&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;75&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;how many epochs between decreasing learning rate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 学习率衰减因子&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--lr-sched-factor&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;factor to decrease learning rate on schedule&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;arglist&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_args&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_args&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arglist&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;PreprocessContentImage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;long_edge&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; 内容图片预处理
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; Parameter: path, str, 图片路径
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; long_edge, int, float, str(float), 图像被缩放后长边的长度
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 读取图片，使用skimage.io.imread，返回numpy.ndarray&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# img.shape前两个数分别是多少行和多少列，第三个数是channel数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;load the content image, size = &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# resize一下图片，resize后的范围在0到1内&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;long_edge&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;new_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;factor&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;factor&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;resized_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;new_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 乘以256恢复到原来的区间&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;resized_img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# swap axes to make image from (224, 224, 3) to (3, 224, 224)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;swapaxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;swapaxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# sub mean，这里的均值应该是ImageNet数据集在RGB三通道上的均值&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mf"&gt;123.68&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mf"&gt;116.779&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mf"&gt;103.939&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;resize the content image to &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;new_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;PreprocessStyleImage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; 对风格图片的预处理
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; Parameter: path, str, 图像路径
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; shape, tuple, 长度为4的tuple，第三个元素和第四个元素是content image的size
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;resized_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;resized_img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;swapaxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;swapaxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mf"&gt;123.68&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mf"&gt;116.779&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mf"&gt;103.939&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;PostprocessImage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; 对图像的后处理
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; Parameter: img, numpy.ndarray
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mf"&gt;123.68&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mf"&gt;116.779&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mf"&gt;103.939&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;swapaxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;swapaxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# clip函数是用来砍掉小于下界和大于上届的数的&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;uint8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;SaveImage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;remove_noise&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; 保存图片
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; Parameter: img, numpy.ndarray
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; filename, str
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; remove_noise, float, default=0.,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;save output to &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PostprocessImage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;remove_noise&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;denoise_tv_chambolle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;remove_noise&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;multichannel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imsave&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;style_gram_symbol&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; Parameter: input_size, tuple, length=2, 表示content image的size
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; style, mx.sym.Group，里面是style对应的层
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s1"&gt; &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_shapes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;infer_shape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;gram_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;grad_scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;list_outputs&lt;/span&gt;&lt;span class="p"&gt;())):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_shapes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;target_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:]))))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# use fully connected to quickly do dot(x, x^T)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;gram&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FullyConnected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;gram_list&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gram&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# grad_scale c*h*w*c&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;grad_scale&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Group&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gram_list&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;grad_scale&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gram&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;gram_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gram&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;list_outputs&lt;/span&gt;&lt;span class="p"&gt;())):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;gvar&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;target_gram_&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;gram_loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gvar&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;gram&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;cvar&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;target_content&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;content_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cvar&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Group&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gram_loss&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;content_loss&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_tv_grad_executor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tv_weight&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;&amp;#34;&amp;#34;create TV gradient executor with input binded on img
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;tv_weight&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;nchannel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;simg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;img&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;skernel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;kernel&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;channels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SliceChannel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;simg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;nchannel&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Concat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;skernel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;num_filter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;no_bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nchannel&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;kernel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;8.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tv_weight&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;img&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;kernel&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_nstyle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;callback&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;&amp;#34;&amp;#34;Train a neural style network.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; Args are from argparse and control input, output, hyper-parameters.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; callback allows for display of training progress.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# input&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;dev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gpu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gpu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gpu&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;content_np&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PreprocessContentImage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content_image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_long_edge&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;style_np&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PreprocessStyleImage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;style_image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;content_np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# size是内容图片的尺寸&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;content_np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# model&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;Executor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;namedtuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Executor&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;executor&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;data_grad&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 导入&amp;#39;model_vgg19.py&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;model_module&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;importlib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;import_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;model_&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 获取到style和content两个mx.sym.Group，里面装着style和content层&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_symbol&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 获取到所有style层的gram矩阵和grad scale&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;gram&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gscale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;style_gram_symbol&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;model_executor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_executor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gram&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;style_np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;style_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;style_array&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copyto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;content_np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;content_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copyto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# delete the executor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;model_executor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;style_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gram&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;model_executor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_executor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;style_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;grad_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style_array&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;style_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copyto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arg_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;target_gram_&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;grad_array&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;style_weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;gscale&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;grad_array&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content_weight&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asscalar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;grad_array&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;content_array&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copyto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arg_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;target_content&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# train&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# initialize img with random noise&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content_np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;[:]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rnd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;lr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lr_scheduler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FactorScheduler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lr_sched_delay&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;factor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lr_sched_factor&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;NAG&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;wd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.95&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;lr_scheduler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;optim_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create_state&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;start training arguments &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;old_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copyto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;clip_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;tv_grad_executor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_tv_grad_executor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tv_weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_num_epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copyto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grad_array&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;gnorm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_grad&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asscalar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;gnorm&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;clip_norm&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_grad&lt;/span&gt;&lt;span class="p"&gt;[:]&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;clip_norm&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;gnorm&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;tv_grad_executor&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;tv_grad_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_grad&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tv_grad_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;optim_state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data_grad&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optim_state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;new_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;eps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;old_img&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;new_img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;mx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_img&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asscalar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;old_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copyto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;epoch &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;, relative change &lt;/span&gt;&lt;span class="si"&gt;%f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stop_eps&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;eps &amp;lt; args.stop_eps, training finished&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;callback&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;cbdata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s1"&gt;&amp;#39;eps&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s1"&gt;&amp;#39;epoch&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save_epochs&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;outfn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_dir&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;e_&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;.jpg&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;npimg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asnumpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;SaveImage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;npimg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;outfn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;remove_noise&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;callback&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;cbdata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;filename&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outfn&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;cbdata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;img&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;npimg&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;callback&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;callback&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cbdata&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;final_fn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_dir&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/final.jpg&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;SaveImage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asnumpy&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;final_fn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;__main__&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_args&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;train_nstyle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>训练神经网络时归一化的目的</title><link>https://davidham3.github.io/blog/p/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%97%B6%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E7%9B%AE%E7%9A%84/</link><pubDate>Wed, 21 Feb 2018 13:39:35 +0000</pubDate><guid>https://davidham3.github.io/blog/p/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%97%B6%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E7%9B%AE%E7%9A%84/</guid><description>&lt;p&gt;在训练神经网络的时候，normalization是必不可少的，原因是如果不进行normalization，在更新参数的时候会出现zig zag的现象。&lt;/p&gt;
&lt;p&gt;在训练神经网络的时候，归一化是必不可少的。之前一直不理解为什么非要归一化，直到看了cs231n这门课才知道归一化的目的。
事实上这个问题主要是针对激活函数来说，如果不归一化的话，那么激活函数在反向传播的时候就会出问题。
&lt;img src="https://davidham3.github.io/blog/images/%e8%ae%ad%e7%bb%83%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%97%b6%e5%bd%92%e4%b8%80%e5%8c%96%e7%9a%84%e7%9b%ae%e7%9a%84/1.png"
loading="lazy"
alt="“图1 图片来源于cs231n”"
&gt;
图1 左侧是原始数据，中间是中心化后的，右侧是归一化后的 图片来源于cs231n
事实上归一化分为两个步骤，第一步是将数据变为以0为中心，第二部是缩小数据的范围。所以归一化的公式为：
&lt;/p&gt;
$$\frac{X-\bar{X}}{std(X)}$$&lt;p&gt;
其中，X为原始样本，$\bar{X}$为样本均值，$std(X)$为样本标准差。
在这里，真正影响反向传播的是第一步，zero-centered。如果没有将数据以0为中心中心化的话，就会影响反向传播的效果。
以逻辑回归(Logistic Regression)为例，逻辑回归的模型可写为
&lt;/p&gt;
$$\hat{y} = sigmoid(W \cdot X+b)$$&lt;p&gt;
其中$W$和$b$是参数，X是样本，$sigmoid$表示sigmoid激活函数，设损失函数为
&lt;/p&gt;
$$L = Loss(y, \hat{y})$$&lt;p&gt;
其中，$y$为样本的标签或标注值。在反向传播的时候，需要对$W$和$b$求偏导数，即求损失函数在当前样本点的梯度，这里我们设$Z = W \cdot X + b$，则
&lt;/p&gt;
$$\frac{\partial{L}}{\partial{W}} = \frac{\partial{L}}{\partial{\hat{y}}}\frac{\partial{\hat{y}}}{\partial{Z}}\frac{\partial{Z}}{\partial{W}} = \frac{\partial{L}}{\partial{\hat{y}}}\frac{\partial{\hat{y}}}{\partial{Z}}X^T$$&lt;p&gt;
同理可以求出$b$的偏导数。
在这里就可以看出问题，假设我们的输入是图像那样的样本，像素值都是大于0的，那这里$\frac{\partial{L}}{\partial{W}}$就会大于0。
使用梯度下降的更新规则来更新参数时
&lt;/p&gt;
$$W := W - \alpha \frac{\partial{L}}{\partial{W}}$$&lt;p&gt;
W就会一直减小，这显然是有问题的。
&lt;img src="https://davidham3.github.io/blog/images/%e8%ae%ad%e7%bb%83%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%97%b6%e5%bd%92%e4%b8%80%e5%8c%96%e7%9a%84%e7%9b%ae%e7%9a%84/2.png"
loading="lazy"
alt="“图2 图片来源于cs231n”"
&gt;
图2 右图展示了只有两个方向允许更新梯度后实际的参数更新路线(红线) 图片来源于cs231n
如图2所示，可以发现如果我们的输入变成了要么都是大于0，要么都是小于0的数，那么允许梯度更新的两个方向在二维空间中就只能落在第一和第三象限中，扩展到高维空间中也是相对的两个卦限。这样在更新的过程中就会产生这种红线所示的路径zig zag path。以上是不进行中心化的后果。
而不进行特征缩放的后果则是，如果每个特征的量级不同，假设一个特征是数值范围在$[-10, 10]$，另一个特征在$[-10^9, 10^9]$，那么在计算梯度后，使用梯度下降更新时，也会造成上面所述的zig zag现象。&lt;/p&gt;</description></item></channel></rss>