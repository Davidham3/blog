<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="IJCAI 2019. 原文链接：[STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger  Demand Forecasting](https://arxiv.org/abs/1905.10069.pdf)"><title>STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting</title>
<link rel=canonical href=https://davidham3.github.io/blog/p/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/><link rel=stylesheet href=/blog/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting"><meta property='og:description' content="IJCAI 2019. 原文链接：[STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger  Demand Forecasting](https://arxiv.org/abs/1905.10069.pdf)"><meta property='og:url' content='https://davidham3.github.io/blog/p/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/'><meta property='og:site_name' content='Davidham的博客'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Graph'><meta property='article:tag' content='graph convolutional network'><meta property='article:tag' content='Spatial-temporal'><meta property='article:tag' content='deep learning'><meta property='article:published_time' content='2019-07-12T19:57:39+00:00'><meta property='article:modified_time' content='2019-07-12T19:57:39+00:00'><meta name=twitter:title content="STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting"><meta name=twitter:description content="IJCAI 2019. 原文链接：[STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger  Demand Forecasting](https://arxiv.org/abs/1905.10069.pdf)"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/><img src=/blog/img/avatar_hu_a92d3b55c5d43e55.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🚀</span></figure><div class=site-meta><h1 class=site-name><a href=/blog>Davidham的博客</a></h1><h2 class=site-description>随便写写</h2></div></header><ol class=menu-social><li><a href=https://github.com/Davidham3 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/blog/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#31-passenger-demand-on-graph>3.1 Passenger Demand on Graph</a></li><li><a href=#32-long-term-and-short-term-encoders>3.2 Long-term and Short-term Encoders</a></li><li><a href=#33-gated-graph-convolutional-module>3.3 Gated Graph Convolutional Module</a></li><li><a href=#34-attention-based-output-module>3.4 Attention-based Output Module</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/>论文阅读笔记</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/>STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting</a></h2><h3 class=article-subtitle>IJCAI 2019. 原文链接：[STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting](https://arxiv.org/abs/1905.10069.pdf)</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 12, 2019</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 7 分钟</time></div></footer></div></header><section class=article-content><p>IJCAI 2019. 原文链接：<a class=link href=https://arxiv.org/abs/1905.10069.pdf target=_blank rel=noopener>STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger
Demand Forecasting</a></p><h1 id=abstract>Abstract</h1><p>多步乘客需求预测对于按需车辆共享服务来说是个重要的任务。然而，预测多个时刻的乘客需求由于时空依赖的非线性和动态性很有挑战。我们提出了基于图的城市范围的旅客需求预测模型，使用一个层次图卷积同时捕获空间和时间关联性。我们的模型有三部分：1) 长期编码器对历史旅客需求编码；2) 短期编码器推导下一步预测结果来生成多步预测；3) 使用一个基于注意力的输出模块对动态的时间和各通道信息建模。实验在三个数据集上表明我们的方法比很多方法好。</p><h1 id=1-introduction>1. Introduction</h1><h1 id=2-notations-and-problem-statement>2. Notations and Problem Statement</h1><p>假设一个城市分成 $N$ 个小的区域，不考虑是分成网格还是路网。我们将区域的集合表示为 $\lbrace r_1, r_2, \dots, r_i, \dots r_N \rbrace$。在每个时间步 $t$，一个二维矩阵 $\boldsymbol{D_t} \in \mathbb{R}^{N \times d_{in}}$ 表示所有区域在时间 $t$ 的旅客需求。另一个向量 $\boldsymbol{E_t} \in \mathbb{R}^{d_e}$ 表示时间步 $t$ 的时间特征，包含了几点、星期几以及节假日的信息。</p><p>给定城市范围的历史旅客需求序列 $\lbrace \bm{D_0}, \bm{D_1}, \dots, \bm{D_t} \rbrace$ 和时间特征 $\lbrace \bm{E_0}, \bm{E_1}, \dots, \bm{E_{t+\tau}} \rbrace$，目标是学习一个预测函数 $\Gamma(\cdot)$ 来预测接下来的 $\tau$ 个时间步上城市范围的旅客需求序列。我们只使用历史 $h$ 个时间步的需求序列作为输入 $\lbrace \bm{D_{t-h+1}, \bm{D_{t-h+2}}, \dots, \bm{D_t}} \rbrace$。我们的任务描述为：</p>$$\tag{1}
(\bm{D\_{t+1}}, \bm{D\_{t+2}}, \dots, \bm{D\_{t+\tau}}) = \Gamma(\bm{D\_{t-h+1}}, \bm{D\_{t-h+2}}, \dots, \bm{D\_t}; \bm{E\_0}, \bm{E\_1}, \dots, \bm{E\_{t+\tau}})
$$<h1 id=3-methodology>3. Methodology</h1><p>STG2Seq 的架构有三个组件：1. 长期编码器，2. 短期编码器，3.基于注意力的输出模块。长期和短期编码器由多个序列时空门控图卷积模块 (GGCM) 组成，通过在时间维度使用 GCN 可以同时捕获时间和空间相关性。</p><p><img src=/blog/images/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/Fig2.JPG loading=lazy alt=Figure2></p><h2 id=31-passenger-demand-on-graph>3.1 Passenger Demand on Graph</h2><p>我们先介绍如何将城市范围的旅客需求在图上描述出来。之前的工作假设一个区域的旅客需求会被近邻的区域影响。然而，我们认为空间关系并不是仅依赖空间位置。如果遥远的区域和当前区域有相似的地方，比如具有相似的 POI，那么也可能拥有相同的旅客需求模式。因此，我们将城市看作一个图 $G = (v, \xi, A)$，$v$ 是区域的集合 $v = \lbrace r_i \mid i=1,2,\dots,N \rbrace$，$\xi$ 表示边的集合，$A$ 是邻接矩阵。我们根据区域间旅客需求模式的相似性定义图的边。</p>$$\tag{2}
A\_{i, j} = \begin{cases}
1, \quad \text{if} \quad Similarity\_{r\_i, r\_j} > \epsilon\\
0, \quad \text{otherwise}
\end{cases}
$$<p>其中 $\epsilon$ 是阈值，控制 $A$ 的稀疏程度。为了定量区域间的旅客需求模式的相似性，我们使用皮尔逊相关系数。$D_{0\text{\textasciitilde}t}(r_i)$ 表示时间从 0 到 $t$ 的区域 $r_i$ 历史旅客需求序列。$r_i$ 和 $r_j$ 之间的相似度可以定义为：</p>$$\tag{3}
Similarity\_{r\_i, r\_j} = Pearson(D\_{0\text{\textasciitilde}t}(r\_i), D\_{0\text{\textasciitilde}t}(r\_j))
$$<h2 id=32-long-term-and-short-term-encoders>3.2 Long-term and Short-term Encoders</h2><p>很多之前的工作只考虑下一步预测，即预测下一时间步的旅客需求。在训练过程中通过减少下一时间步预测值的误差而不考虑后续时间步的误差来优化模型。因此，这些方法在多步预测的问题上会退化。仅有一些工作考虑了多步预测的问题 [Xingjian et al., 2015; Li et al., 2018]。这些工作采用了基于 RNN 的编码解码器的架构，或是它的变体，比如 ConvLSTM 这样的作为编码解码器。这些方法有两个劣势：1. 链状结构的 RNN 在编码的时候需要遍历输入的时间步。因此他们需要与输入序列等长的 RNN 单元个数（序列多长，RNN单元就有多少个）。在目标需求和前一个需求上的长距离计算会导致一些信息的遗忘。2. 在解码部分，为了预测时间步 $T$ 的需求，RNN 将隐藏状态和前一时间步 $T-1$ 作为输入。因此，前一时间步带来的误差会直接影响到预测，导致未来时间步误差的累积。</p><p>不同于之前所有的工作，我们引入了一个依赖于同时使用长期和短期编码器的架构，不用 RNN 做多步预测。长期编码器取最近的 $h$ 个时间步的城市历史旅客需求序列 $\lbrace \bm{D_{t-h+1}}, \bm{D_{t-h+2}}, \dots, \bm{D_t} \rbrace$ 作为输入来学习历史的时空模式。这 $h$ 步需求合并后组织成三维矩阵，$h \times N \times d_{in}$。长期编码器由一些 GGCM 组成，每个 GCGGM 捕获在所有的 $N$ 个区域上捕获空间关联性，在 $k$ 个时间步上捕获时间关联性。$k$ 是超参数，我们会在 3.3 节讨论。因此，只需要 $\frac{h-1}{k-1}$ 个迭代的步数就可以捕获 $h$ 个时间步上的时间关联性。对比 RNN 结构，我们的基于 GGCM 的长期编码器显著的降低了遍历长度，进一步减少了信息的损失。长期编码器的输出 $Y_h$ 的维数是 $h \times N \times d_{out}$，是输入的编码表示。</p><p>短期编码器用来集成已经预测的需求，用于多步预测。它使用一个长度为 $q$ 的滑动窗来捕获近期的时空关联性。当预测在 $T(T \in [t+1,t+\tau])$ 步的旅客需求时，它取最近的 $q$ 个时间步的旅客需求，即 $\lbrace \bm{D_{T-q}}, \bm{D_{T-q+1}}, \dots, \bm{D_{T-1}} \rbrace$ 作为输入。除了时间步的长度以外，短期编码器和长期编码器一样。短期编码器生成一个维数为 $q \times N \times d_{out}$ 的矩阵 $Y^T_q$ 作为近期趋势表示。和基于 RNN 的解码器不同的是，RNN的解码器只将最后一个时间步的预测结果输入回去。因此，预测误差会被长期编码器小柔，减轻基于 RNN 的解码器会导致误差累积的问题。</p><h2 id=33-gated-graph-convolutional-module>3.3 Gated Graph Convolutional Module</h2><p>门控图卷积模块是长期编码器和短期编码器的核心。每个 GGCM 由几个 GCN 层组成，沿着时间轴并行。为了捕获时空关联性，每个 GCN 在一定长度的时间窗内操作($k$)。它可以提取 $k$ 个时间步内所有区域的空间关联性。通过堆叠多个 GGCM，我们的模型形成了一个层次结构，可以捕获整个输入的时空关联性。图 3 展示了只使用 GCN 捕获时空关联性，为了简化我们忽略了通道维。Yu et al., 2018 的工作和我们的 GGCM 模块很像。他们的工作首先使用 CNN 捕获时间关联性，然后使用 GCN 捕获空间关联性。我们的方法对比他们的方法极大的简化了，因为我们可以同时捕获时空关联性。</p><p><img src=/blog/images/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/Fig3.JPG loading=lazy alt=Figure3></p><p><img src=/blog/images/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/Fig4.JPG loading=lazy alt=Figure4></p><p>GGCM 模块的详细设计如图 4。第 $l$ 个 GGCM 的输入是一个矩阵，维数为 $h \times N \times C^l$。在第一个 GGCM 模块，$C^l$ 是 $d_{in}$ 维的。第 $l$ 个 GGCM 的输出是 $h \times N \times C^{l+1}$。我们先拼接一个 zero padding，维数为 $(k-1) \times N \times C^l$，得到新的输入 $(h+k-1) \times N \times C^l$，确保变换不会减少序列的长度。接下来，GGCM 中的每个 GCN 取 $k$ 个时间步的数据 $k \times N \times C^l$ 作为输入来提取时空关联性，然后 reshape 成一个二维矩阵 $N \times (k \cdot C^l)$。根据 Kipf & Welling 的 GCN，GCN 层可以描述如下：</p>$$\tag{4}
X^{l+1} = (\tilde{P}^{-\frac{1}{2}} \tilde{A} \tilde{P}^{-\frac{1}{2}}) X^l W
$$<p>$\tilde{A} = A + I_n$，$\tilde{P}_{ii} = \sum_j \tilde{A}_{ij}$，$X \in \mathbb{R}^{N \times (k \cdot C^l)}$，$W \in \mathbb{R}^{(k \cdot C^l) \times C^{l+1}}$，$X^{l+1} \in \mathbb{R}^{N \times C^{l+1}}$
。</p><p>除此以外，我们使用了门控机制对旅客需求预测的复杂非线性建模。式 4 重新描述如下：</p>$$\tag{5}
X^{l+1} = ((\tilde{P}^{-\frac{1}{2}} \tilde{A} \tilde{P}^{-\frac{1}{2}}) X^l W\_1 + X^l) \otimes \sigma((\tilde{P}^{-\frac{1}{2}} \tilde{A} \tilde{P}^{-\frac{1}{2}}) X^l W\_2)
$$<p>$\otimes$ 是对应元素相乘，$\sigma$是 sigmoid 激活函数。因此输出是一个非线性门 $\sigma((\tilde{P}^{-\frac{1}{2}} \tilde{A} \tilde{P}^{-\frac{1}{2}}) X^l W_2)$ 控制的线性变换 $((\tilde{P}^{-\frac{1}{2}} \tilde{A} \tilde{P}^{-\frac{1}{2}}) X^l W_1 + X^l)$。非线性门控制线性变换的哪个部分可以通过门影响预测。此外，我们使用残差连接来避免式 5 中的网络退化。</p><p>最后，门控机制产生的 $h$ 个输出沿时间轴合并，生成 GGCM 模块的输出 $h \times N \times C^{l+1}$。</p><h2 id=34-attention-based-output-module>3.4 Attention-based Output Module</h2><p>如 3.2 描述的那样，长期时空依赖和 $T$ 时间步的近期时空依赖通过两个矩阵描述 $Y_h$ 和 $Y^T_q$。我们拼接。我们拼接他们形成联合表示 $Y_{h+q} \in \mathbb{R}^{(h+q) \times N \times d_{out}}$，通过一个基于注意力机制的模块解码获得预测值。这里为了简便忽略 $T$。$Y_{h+q}$ 的三个轴分别是时间、空间、通道。</p><p>我们先引入一个时间注意力机制来解码 $Y_{h+q}$。旅客需求是一个典型的时间序列，前一时刻的需求对后一时刻有影响。然而，之前的每一步对预测目标的影响是不同的，影响随时间变化。我们设计了一个时间注意力机制对每个历史时间步增加注意力分数衡量其影响。分数通过 $Y_{h+q} = [y_1, y_2, \dots, y_{h+q}](y_i \in \mathbb{R}^{N \times d_{out}})$ 和目标时间步的时间特征 $\bm{E}_T$ 生成，这个分数可以自适应地学习之前的时间步随时间的动态影响。我们定义时间注意力分数如下：</p>$$\tag{6}
\bm{\alpha} = softmax(tanh(Y\_{h+q} W^Y\_3 + E\_T W^E\_4 + b\_1))
$$<p>$W^Y_3 \in \mathbb{R}^{(h+q) \times (N \times d_{out}) \times 1}$，$W^E_4 \in \mathbb{R}^{d_e \times (h+q)}$，$b_1 \in \mathbb{R}^{(h+q)}$。联合表示 $Y_{h+q}$ 通过注意力分数 $\bm{\alpha}$ 转换：</p>$$\tag{7}
Y\_{\alpha} = \sum^{h+q}\_{i=1} \alpha^i y\_i \quad \in \mathbb{R}^{N \times d\_{out}}
$$<p>受到 [Chen et al., 2017] 的启发，每个通道的重要性是不同的，我们在时间注意力后面加了一个通道注意力模块来找到 $Y_\alpha = [y_1, y_2, \dots, y_{d_{out}}]$ 中最重要的那个。计算如下：</p>$$\tag{8}
\bm\beta = softmax(tanh(Y\_\alpha W^Y\_5 + E\_T W^E\_6 + b\_2))
$$$$\tag{9}
Y\_{\beta} = \sum^{d\_{out}}\_{i=1} \beta^i y\_i \quad \mathbb{R}^N
$$<p>其中，$W^Y_5 \in \mathbb{R}^{d_{out} \times N \times 1}$，$W^E_6 \in \mathbb{R}^{d_e \times d_{out}}$；$\bm\beta \in \mathbb{R}^{d_{out}}$ 是每个通道的注意力分数。当预测的维度是1时，$Y_\beta$ 就是我们预测的旅客需求 $\bm{D&rsquo;_T}$。当预测维度是 2 时（预测起止需求），我们给每个通道计算注意力分数，将他们拼接起来得到最后的预测值。</p></section><footer class=article-footer><section class=article-tags><a href=/blog/tags/graph/>Graph</a>
<a href=/blog/tags/graph-convolutional-network/>Graph Convolutional Network</a>
<a href=/blog/tags/spatial-temporal/>Spatial-Temporal</a>
<a href=/blog/tags/deep-learning/>Deep Learning</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under Apache License 2.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>最后更新于 Jul 12, 2019 19:57 UTC</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/blog/p/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/><div class=article-details><h2 class=article-title>DeepSTN+: Context-aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis</h2></div></a></article><article><a href=/blog/p/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/><div class=article-details><h2 class=article-title>Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning</h2></div></a></article><article><a href=/blog/p/revisiting-spatial-temporal-similarity-a-deep-learning-framework-for-traffic-prediction/><div class=article-details><h2 class=article-title>Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction</h2></div></a></article><article><a href=/blog/p/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/><div class=article-details><h2 class=article-title>Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction</h2></div></a></article><article><a href=/blog/p/t-gcn-a-temporal-graph-convolutional-network-for-traffic-prediction/><div class=article-details><h2 class=article-title>T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Davidham的博客</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>