<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="IJCAI 2018，看了一部分，还没看完。原文链接：[GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction](https://www.ijcai.org/proceedings/2018/476)"><title>GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction</title>
<link rel=canonical href=https://davidham3.github.io/blog/p/geoman-multi-level-attention-networks-for-geo-sensory-time-series-prediction/><link rel=stylesheet href=/blog/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction"><meta property='og:description' content="IJCAI 2018，看了一部分，还没看完。原文链接：[GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction](https://www.ijcai.org/proceedings/2018/476)"><meta property='og:url' content='https://davidham3.github.io/blog/p/geoman-multi-level-attention-networks-for-geo-sensory-time-series-prediction/'><meta property='og:site_name' content='Davidham的博客'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='deep learning'><meta property='article:tag' content='Spatial-temporal'><meta property='article:tag' content='Attention'><meta property='article:tag' content='Time Series'><meta property='article:published_time' content='2018-07-09T17:03:00+00:00'><meta property='article:modified_time' content='2018-07-09T17:03:00+00:00'><meta name=twitter:title content="GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction"><meta name=twitter:description content="IJCAI 2018，看了一部分，还没看完。原文链接：[GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction](https://www.ijcai.org/proceedings/2018/476)"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/><img src=/blog/img/avatar_hu_a92d3b55c5d43e55.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🚀</span></figure><div class=site-meta><h1 class=site-name><a href=/blog>Davidham的博客</a></h1><h2 class=site-description>随便写写</h2></div></header><ol class=menu-social><li><a href=https://github.com/Davidham3 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/blog/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#21-notations>2.1 Notations</a></li><li><a href=#22-problem-statement>2.2 Problem Statement</a></li></ol><ol><li><a href=#31-spatial-attention>3.1 Spatial Attention</a><ol><li><a href=#local-spatial-attention>Local Spatial Attention</a></li><li><a href=#global-spatial-attention>Global Spatial Attention</a></li></ol></li><li><a href=#32-temporal-attention>3.2 Temporal Attention</a></li><li><a href=#33-external-factor-fusion>3.3 External Factor Fusion</a></li><li><a href=#34-encoder-decoder--model-training>3.4 Encoder-decoder & Model Training</a></li></ol><ol><li><a href=#41-settings>4.1 Settings</a><ol><li><a href=#datasets>Datasets</a></li><li><a href=#evaluation-metrics>Evaluation Metrics</a></li><li><a href=#hyperparameters>Hyperparameters</a></li></ol></li><li><a href=#42-baselines>4.2 Baselines</a></li><li><a href=#43-model-comparsion>4.3 Model Comparsion</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/>论文阅读笔记</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/geoman-multi-level-attention-networks-for-geo-sensory-time-series-prediction/>GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction</a></h2><h3 class=article-subtitle>IJCAI 2018，看了一部分，还没看完。原文链接：[GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction](https://www.ijcai.org/proceedings/2018/476)</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 09, 2018</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 11 分钟</time></div></footer></div></header><section class=article-content><p>IJCAI 2018，看了一部分，还没看完。原文链接：<a class=link href=https://www.ijcai.org/proceedings/2018/476 target=_blank rel=noopener>GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction</a></p><h1 id=abstract>Abstract</h1><p>大量的监测器被部署在各个地方，连续协同地监测周围的环境，如空气质量。这些检测器生成很多时空序列数据，之间有着空间相关性。预测这些时空数据很有挑战，因为预测受很多因素影响，比如动态的时空关联和其他因素。我们在这篇论文中使用多级基于注意力机制的 RNN 模型，结合空间、气象和检测器数据来预测未来的监测数值。我们的模型由两部分组成：</p><ol><li>多级注意力机制对时空依赖关系建模</li><li>一个通用的融合模块对多领域的外部信息进行融合</li></ol><p>实验用了两个真实的数据集，空气质量数据和水质监测数据，结果显示我们的模型比9个baselines都要好。</p><h1 id=1-introduction>1 Introduction</h1><p>现实世界中有大量的检测器，如空气监测站。每个监测站都有一个地理位置，不断地生成时间序列数据。一组检测器不断的监测一个区域的环境，数据间有空间依赖关系。我们成这样的监测数据为 <em>geosensory time series</em>。此外，一个检测器生成多种 geo-sensory 时间序列是很常见的，因为这个检测器同时监测不同的目标。举个例子，图1a，路上的循环检测器实时记录车辆通行情况，也记录他们的速度。图1b 展示了检测器每五分钟生成的关于水质的三个不同的气候指标。除了监测，对于 geo-sensory 时间序列预测还有一个重要的需求就是交通预测。</p><p><img src=/blog/images/geoman-multi-level-attention-networks-for-geo-sensory-time-series-prediction/Fig1.PNG loading=lazy alt=Fig1></p><p>然而，预测 geo-sensory 时间序列很有挑战性，主要受两个因素影响：</p><ol><li>动态时空关系
·检测器间复杂的关系。图1c展示了不同检测器的时间序列间的空间关系是高度动态的，随着时间不断改变。除此以外，geo-sensory时间序列根据地区有非线性的变化。当对动态关系建模时，传统方法（如概率图模型）的计算量会很大，因为他们有很多参数。
·检测器内部的动态关系。首先，一个geo-sensory时间序列通常由一种周期模式（如，图1c中的$S_1$），这种模式一直变化，并且地理上也有改变。其次，检测器记录经常有很大的振动，很快地减少前一个数值的影响。因此，如何选择一个时间间隔来预测也是一个问题。</li><li>外部因素。
检测器数据也被周围的环境影响，比如气象（例如强风），几点（比如早晚高峰）还有土地使用情况等。</li></ol><p>为了解决这些挑战，我们提出了一个多级注意力网络（GeoMAN）来预测一个检测器未来几个小时的数值。我们的贡献有三点：</p><ol><li><em>多级注意力机制</em>。我们研发了一个多级注意力机制对动态时空关系建模。具体来说，第一级，我们提出了一个新的注意力机制，由局部空间注意力和全局空间注意力组成，用来捕获不同检测器时间序列间的复杂空间关系。第二级，时间注意力对一个时间序列中的动态时间关系进行建模。</li><li><em>外部因素融合模型</em>。我们设计了一个通用融合模型融合不同领域的外部因素。学习到的隐含表示会输入至多级注意力网络中来提升这些外部因素的重要性。</li><li><em>真实的评价</em>。我们基于两个真实的数据集评估我们的方法。大量的实验证明了我们的方法相比于baseline的优越性。</li></ol><h1 id=2-preliminary>2 Preliminary</h1><h2 id=21-notations>2.1 Notations</h2><p>假设，有 $N_g$ 个检测器，每个都生成 $N_l$ 种时间序列。我们指定其中的一个为 <em>target series</em> 来预测，其它序列作为特征。时间窗为 $T$，我们使用 $\mathbf{Y} = (\mathbf{y}^1, \mathbf{y}^2, &mldr;, \mathbf{y}^{N_g}) \in \mathbb{R}^{N_g \times T}$ 来表示所有目标序列在过去 $T$ 个小时的监测值，其中 $\mathbf{y}^i \in \mathbb{R}^T$ 属于监测器 $i$。我们使用 $\mathbf{X}^i = (\mathbf{x}^{i, 1}, \mathbf{x}^{i, 2}, &mldr;, \mathbf{x}^{i, N_l})^{\rm T} = (\mathbf{x}^i_1, \mathbf{x}^i_2, &mldr;, \mathbf{x}^i_T) \in \mathbb{R}^{N_l \times T}$ 作为检测器 $i$ 的局部特征，其中 $\mathbf{x}^{i,k} \in \mathbb{R}^T$ 是这个检测器的第 $k$ 个时间序列，$\mathbf{x}^i_t = (x^{i,1}_t, x^{i,2}_t, &mldr;, x^{i,N_l}_t)^{\rm T} \in \mathbb{R}^{N_l}$ 表示检测器 $i$ 在时间 $t$ 的所有时间序列的值。除了检测器 $i$ 的局部特征，由于不同检测器间的空间关系，其他的检测器会共享大量对于预测有用的信息。为了这个目的，我们将每个检测器的局部特征融合到集合 $\mathcal{X}^i = \lbrace \mathbf{X}^1, \mathbf{X}^2, &mldr;, \mathbf{X}^{N_g}\rbrace$ 中作为检测器 $i$ 的全局特征。</p><h2 id=22-problem-statement>2.2 Problem Statement</h2><p>给定每个检测器之前的值和外部因素，预测检测器 $i$ 在未来 $\tau$ 个小时的值，表示为 $\hat{\mathbf{y}}^i = (\hat{y}^i_{T+1}, \hat{y}^i_{T+2}, &mldr;, \hat{y}^i_{T+\tau})^{\rm T} \in \mathbb{R}^{\tau}$.</p><h1 id=3-multi-level-attention-networks>3 Multi-level Attention Networks</h1><p><img src=/blog/images/geoman-multi-level-attention-networks-for-geo-sensory-time-series-prediction/Fig2.PNG loading=lazy alt=Fig2></p><p>图 2 展示了我们方法的框架。基于编码解码架构[Cho et al., 2014b]，我们用两个分开的 LSTM，一个对输入序列编码，也就是对历史的 geo-sensory 时间序列，另一个来预测输出的序列 $\hat{y}^i$。更具体的讲，我们的模型 GeoMAN 有两个主要部分：</p><ol><li>多级注意力机制。包含一个带有两类空间注意力机制的编码器和一个带有时间注意力的解码器。在编码器，我们开发了两种不同的注意力机制，局部空间注意力和全局空间注意力，如图 2 所示，这两种注意力机制通过前几步编码器的隐藏状态、前几步检测器的值和空间信息（检测器网络），可以在每个时间步上捕获检测器间的复杂关系。在解码器，我们使用了一个时间注意力机制来自适应地选择之前的时间段来做预测。</li><li>外部因素融合。这个模块用来处理外部因素的影响，输出会作为解码器的一部分输入。这里，我们使用 $h_t \in \mathbb{R}^m$ 和 $s_t \in \mathbb{R}^m$ 来表示编码器在时间 $t$ 的隐藏状态和细胞状态。$d_t \in \mathbb{R}^n$ 和 $s&rsquo; \in \mathbb{R}^n$ 表示解码器的隐藏状态和细胞状态。</li></ol><h2 id=31-spatial-attention>3.1 Spatial Attention</h2><h3 id=local-spatial-attention>Local Spatial Attention</h3><p>我们先介绍空间局部注意力机制。对应一个监测器，在它的局部时间序列上有复杂的关联性。举个例子，一个空气质量检测站会记录不同的时间序列如 PM2.5，NO 和 SO2。事实上，PM2.5 的浓度通常被其他时间序列影响，包括其他的空气污染物和局部空气质量 [Wang et al., 2005]。为了解决这个问题，给定第 $i$ 个检测器第 $k$ 个局部特征向量 $\mathbf{x}^{i,k}$，我们使用注意力机制自适应地捕获目标序列和每个局部特征间的动态关系：</p>$$\tag{1}
e^k\_t = \mathbf{v}^T\_l \text{tanh} (\mathbf{W}\_l [\mathbf{h}\_{t-1};\mathbf{s}\_{t-1}] + \mathbf{U}\_l \mathbf{x}^{i,k} + \mathbf{b}\_l)
$$$$\tag{2}
\alpha^k\_t = \frac{\text{exp}(e^k\_t)}{\sum^{N\_l}\_{j=1}\text{exp}(e^j\_t)}.
$$<p>其中 $[\cdot;\cdot]$ 是拼接操作（论文这里写的是 concentration，我怎么觉得是concatenation呢。。。）。$\mathbf{v}_l, \mathbf{b}_l \in \mathbb{R}^T, \mathbf{W}_l \in \mathbb{R}^{T \times 2m}, \mathbf{U}_l \in \mathbb{R}^{T \times T}$ 是参数。局部特征的注意力权重通过编码器中输入的局部特征和历史状态共同决定。这个注意力分数语义上表示每个局部特征的重要性，局部空间注意力在时间步 $t$ 的输出向量通过下式计算：</p>$$\tag{3}
\tilde{\mathbf{x}}^{local}\_t = (\alpha^1\_t x^{i,1}\_t, \alpha^2\_t x^{i,2}\_t, \dots, \alpha^{N\_l}\_t x^{i,N\_l}\_t)^{\rm T}.
$$<h3 id=global-spatial-attention>Global Spatial Attention</h3><p>对于一个监测器记录的目标时间序列，其他监测器是时间序列对其有直接影响。然而，影响权重是高度动态地，随时间变化。因为可能有很多不相关的序列，直接使用各种时间序列作为编码器的输入来捕获不同监测器之间的关系会导致很高的计算开销并且降低模型的能力。而且这样的影响权重受其他监测器的局部条件影响。举个例子，当强风从一个遥远地地方吹过来，这个区域的空气质量回比之前受影响的多。受这个现象的启发，我们开发了一个新的注意力机制捕获不同监测器间的动态关系。给定第 $i$ 个监测器作为我们的预测目标，另一个监测器 $l$，我们计算他们之间的注意力分数如下：</p>$$
g^l\_t = \mathbf{v}^{\rm T}\_g \text{tanh} (\mathbf{W}\_g [\mathbf{h}\_{t-1}; \mathbf{s}\_{t-1}] + \mathbf{U}\_g \mathbf{y}^l + \mathbf{W}'\_g \mathbf{X}^l \mathbf{u}\_g + \mathbf{b}\_g),
$$<p>其中 $\mathbf{v}_g, \mathbf{u}_g, \mathbf{b}_g \in \mathbb{R}^T, \mathbf{W}_g \in \mathbb{R}^{T \times 2m}, \mathbf{U}_g \in \mathbb{R}^{T \times T}, \mathbf{W}&rsquo;_g \in \mathbb{R}^{T \times N_l}$ 是参数。通过考虑目标序列和其他检测器的局部特征，这个注意力机制可以自适应地选择相关的监测器来做预测。同时，通过考虑编码器内前一隐藏状态和细胞状态，历史信息会跨时间流动。</p><p>注意，空间因素也会对不同监测器之间的关系做出贡献。一般来说，geo-sensors 通过一个明确的或隐含的网络连接起来。这里，我们使用一个矩阵 $\mathbf{P} \in \mathbb{R}^{N_g \times N_g}$ 来衡量地理空间相似度（如地理距离的倒数），$P_{i,j}$ 表示监测器 $i$ 和 $j$ 之间的相似度。不同于注意力权重，地理相似度可以看作是先验知识。特别的说，如果 $N_g$ 很大，一个方法是使用最近邻或最相近的一组而不是所有的监测器。之后，我们使用一个 softmax 函数，确定所有的注意力权重之和为1，两个方法共同考虑地理相似度如下：</p>$$\tag{4}
\beta^l\_t = \frac{\text{exp}((1-\lambda)g^l\_t + \lambda P\_{i,l})}{\sum^{N\_g}\_{j=1} \text{exp}((1-\lambda)g^j\_t + \lambda P\_{i,j})},
$$<p>其中，$\lambda$ 是一个可调的超惨。如果 $\lambda$ 大，这项会强制注意力权重等于地理相似度。全局注意力的输出向量计算如下：</p>$$\tag{5}
\tilde{\mathbf{x}}^{global}\_t = (\beta^1\_t y^1\_t, \beta^2\_t y^2\_t, \dots, \beta^{N\_g}\_t y^{N\_g}\_t)^{\rm T}.
$$<h2 id=32-temporal-attention>3.2 Temporal Attention</h2><p>因为编码解码结构会随着长度增长会很快的降低性能，一个重要的扩展是增加时间注意力机制，可以自适应地选择编码器相关的隐藏状态来生成输出序列，即模型对目标序列中不同时间间隔的动态时间关系建模。特别来说，为了计算每个输出时间 $t&rsquo;$ 对编码器每个隐藏状态的的注意力向量，我们定义：</p>$$\tag{6}
u^o\_{t'} = \mathbf{v}^{\rm T}\_d \text{tanh} (\mathbf{W}'\_d [\mathbf{d}\_{t'-1}; \mathbf{s}'\_{t'-1}] + \mathbf{W}\_d \mathbf{h}\_o + \mathbf{b}\_d),
$$$$\tag{7}
\gamma^o\_{t'} = \frac{\text{exp} (u^o\_{t'})}{\sum^T\_{o=1} \gamma^o\_{t'} \mathbf{h}\_o},
$$$$\tag{8}
\mathbf{c}\_{t'} = \sum^T\_{o=1} \gamma^o\_{t'} \mathbf{h}\_o,
$$<h2 id=33-external-factor-fusion>3.3 External Factor Fusion</h2><p>Geo-sensory 时间序列和空间因素有强烈的相关性，如 POI 和监测器网络。这些因素一起表示一个区域的功能。而且还有很多时间因素影响监测器的数值，如气象或时间。受之前工作的启发 [Liang et al., 2017; Wang et al., 2018] 专注时空应用中的外部因素的影响，我们设计了一个简单有效的组件来处理这些因素。</p><p>如图 2 所示，我们先将包含时间、气象特征的时间因素和表示目标监测器的监测器ID融合。因为未来的天气条件未知，我们使用天气预报来提升性能。这些因素的大部分都是离散特征，不能直接放到神经网络里面，我们通过将离散特征分开放入不同的嵌入层，将离散特征转换为低维向量。根据空间因素，我们使用不同 POI 类型的密度作为特征。因为监测器的属性依赖实际情况，我们只使用网络的结构特征，如邻居数和交集等。最后，我们将获得的嵌入向量和空间特征拼接作为这个模块的输出，表示为 $\mathbf{ex}_{t&rsquo;} \in \mathbb{R}^{N_e}$，其中 $t&rsquo;$ 是解码器中未来的时间步。</p><h2 id=34-encoder-decoder--model-training>3.4 Encoder-decoder & Model Training</h2><p>编码器中，我们简单地从局部空间注意力和全局空间注意力聚合输出：</p>$$\tag{9}
\tilde{\mathbf{x}}\_t = [\tilde{\mathbf{x}}^{local}\_t; \tilde{\mathbf{x}}^{global}\_t],
$$<p>其中 $\tilde{\mathbf{x}}_t \in \mathbb{R}^{N_l + N_g}$。我们将拼接 $\tilde{\mathbf{x}}_t$ 作为编码器的新输入，使用 $\mathbf{h}_t = f_e(\mathbf{h}_{t-1}, \tilde{\mathbf{x}}_t)$ 更新时间 $t$ 的隐藏状态，$f_e$ 是一个 LSTM 单元。</p><p>解码器中，一旦我们获得了时间 $t&rsquo;$ 的上下文向量 $\mathbf{c}_{t&rsquo;}$ 的带权和，我们将他与外部因素融合模块的输出 $\mathbf{ex}_{t&rsquo;}$ 还有解码器的最后一个输出 $\hat{y}^i_{t&rsquo;-1}$ 融合，用 $\mathbf{d}_{t&rsquo;} = f_d (\mathbf{d}_{t&rsquo;-1}, [\hat{y}^i_{t&rsquo;-1}; \mathbf{ex}_{t&rsquo;}; \mathbf{c}_{t&rsquo;}])$ 更新解码器的隐藏状态，$f_d$ 是解码器中使用的 LSTM 单元。然后，我们讲上下文向量 $\mathbf{c}_{t&rsquo;}$ 和隐藏状态 $\mathbf{d}_{t&rsquo;}$ 拼接，得到新的隐藏状态，然后做最后的预测：</p>$$\tag{10}
\hat{y}^i\_{t'} = \mathbf{v}^{\rm T}\_y (\mathbf{W}\_m [\mathbf{c}\_{t'}; \mathbf{b}\_{t'}] + \mathbf{b}\_m) + b\_y,
$$<p>其中，$\mathbf{W}_m \in \mathbb{R}^{n \times (m + n))}$ 和 $\mathbf{b}_m \in \mathbb{R}^n$ 将 $[\mathbf{c}_{t&rsquo;}; \mathbf{d}_{t&rsquo;}] \in \mathbb{R}^{m + n}$ 映射到解码器隐藏状态的空间。最后，我们用一个线性变换生成最终结果。</p><p>因为我们的方法是平滑且可微的，可以通过反向传播训练。在训练时，我们使用 Adam，最小化 MSE。</p>$$\tag{11}
\mathcal{L}(\theta) = \Vert \hat{\mathbf{y}}^i - \mathbf{y}^i \Vert^2\_2,
$$<h1 id=4-experiments>4 Experiments</h1><h2 id=41-settings>4.1 Settings</h2><h3 id=datasets>Datasets</h3><p>我们在两个数据集中开展了实验，每个数据集包含三个子集：气象数据、POI、监测器网络数据。</p><ul><li><p>水质数据集：中国东南的一个城市的供水系统中的监测器提供了长达三年的每5分钟一个记录的数据，包含了残余氯(RC)、浑浊度和PH值等。我们将 RC 作为目标时间序列，因为它在环境科学中作为常用的水质指标。一共有 14 个监测器，监测 10 个指标，它们之间通过管道网络相连。我们使用 Liu et al., 2016a 提出的指标作为这个数据集的相似度矩阵。</p></li><li><p>空气质量：从一个公开数据集抓取的，这个数据集包含不同污染物的浓度，还有气象数据，北京地区一共 35 个监测器。主要污染物是 PM2.5，因此我们将它作为目标。我们只使用空间距离的倒数表示两个监测器之间的相似度。</p></li></ul><p><img src=/blog/images/geoman-multi-level-attention-networks-for-geo-sensory-time-series-prediction/Table1.JPG loading=lazy alt=Table1></p><p>对于水质数据集，我们将数据分成了不重叠的训练集、验证集和测试集，去年的前一半作为验证机，去年的后半段作为测试集。可惜的是，我们在第二个数据集上没能获得很多的数据，因此我们使用了8：1：1的比例划分。</p><h3 id=evaluation-metrics>Evaluation Metrics</h3><p>我们使用多个标准评价模型，RMSE 和 MAE。</p><h3 id=hyperparameters>Hyperparameters</h3><p>我们令 $\tau = 6$，做短期预测。在训练阶段，batch size 256，学习率 0.001。外部因素融合模块，我使用 $\mathbb{R}^6$ 嵌入监测器 ID，时间特征 $\mathbb{R}^10$。我们的模型一共 4 个超参数，$\lambda$ 根据经验设置，从 0.1 到 0.5。对于窗口长度 $T$，我们设为 $T \in \lbrace 6, 12, 24, 36, 48 \rbrace$。为了简单，我们将编码器和解码器采用同样的隐藏维数，网格搜索 $\lbrace 32, 64, 128, 256\rbrace$。我们堆叠 LSTM 来提高性能，层数记为 $q$。验证集上得到的最好参数是 $q = 2, m = n = 64, \lambda = 2$。</p><h2 id=42-baselines>4.2 Baselines</h2><p>ARIMA, VAR, GBRT, FFA, stMTMVL, stDNN, LSTM, Seq2seq, DA-RNN。</p><p>对于 ARIMA，我们用前六个小时的数据作为输入。stMTMVL 和 FFA，我们使用和作者一样的设置。和 GeoMAN 类似，我们使用前 $T \in \lbrace 6, 12, 24, 36, 48\rbrace$ 个小时的数据作为其他模型的输入。最后，我们测试了不同的超参数，得到了每个模型的最好效果。</p><h2 id=43-model-comparsion>4.3 Model Comparsion</h2><p>我们在两个数据集上比较了模型和 baselines。为了公平，我们在表 2 展示了每个方法的最好性能。</p><p>我们的方法在水质预测上得到了最好的性能。比 state-of-the-art 的 DA-RNN 在两个指标上分别提升了 14.2% 和 13.5%。因为 RC 浓度有一个确定的周期模式，stDNN 和 基于 RNN 的模型比 stMTMVL 和 FFA 获得了更好的效果，因为他们能捕获更长的时间依赖。对比 LSTM 智能预测一个未来的时间步，GeoMAN 和 Seq2seq 由于解码器的存在有很大的提升。GBRT 比大部分方法也要好，体现了集成学习的优势。</p><p>对比数据相对稳定的水质数据集，PM2.5 的浓度有些时候震荡得很厉害，使得很难预测。表 2 展示了北京的空气质量数据集上一个全面的对比。可以看到我们的模型有最好的效果。我们主要讨论下 MAE。我们的方法比这些方法的 MAE 相对低 7.2% 和 63.5%，展示出了比其他方法更好的泛化效果。另一个有趣的现象是 stMTMVL 在水质预测上表现很好，在空气质量上</p></section><footer class=article-footer><section class=article-tags><a href=/blog/tags/deep-learning/>Deep Learning</a>
<a href=/blog/tags/spatial-temporal/>Spatial-Temporal</a>
<a href=/blog/tags/attention/>Attention</a>
<a href=/blog/tags/time-series/>Time Series</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under Apache License 2.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>最后更新于 Jul 09, 2018 17:03 UTC</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/blog/p/multi-range-attentive-bicomponent-graph-convolutional-network-for-traffic-forecasting/><div class=article-details><h2 class=article-title>Multi-Range Attentive Bicomponent Graph Convolutional Network for Traffic Forecasting</h2></div></a></article><article><a href=/blog/p/gman-a-graph-multi-attention-network-for-traffic-prediction/><div class=article-details><h2 class=article-title>GMAN: A Graph Multi-Attention Network for Traffic Prediction</h2></div></a></article><article><a href=/blog/p/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/><div class=article-details><h2 class=article-title>DeepSTN+: Context-aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis</h2></div></a></article><article><a href=/blog/p/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/><div class=article-details><h2 class=article-title>Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning</h2></div></a></article><article><a href=/blog/p/revisiting-spatial-temporal-similarity-a-deep-learning-framework-for-traffic-prediction/><div class=article-details><h2 class=article-title>Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Davidham的博客</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>