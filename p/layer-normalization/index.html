<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Layer Normalization，之前看到一篇论文用了这个LN层，看一下这个怎么实现。原文链接：[Layer Normalization](https://arxiv.org/abs/1607.06450.pdf)"><title>Layer Normalization</title><link rel=canonical href=https://davidham3.github.io/blog/p/layer-normalization/><link rel=stylesheet href=/blog/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css><meta property='og:title' content="Layer Normalization"><meta property='og:description' content="Layer Normalization，之前看到一篇论文用了这个LN层，看一下这个怎么实现。原文链接：[Layer Normalization](https://arxiv.org/abs/1607.06450.pdf)"><meta property='og:url' content='https://davidham3.github.io/blog/p/layer-normalization/'><meta property='og:site_name' content='Davidham的博客'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='deep learning'><meta property='article:tag' content='machine learning'><meta property='article:published_time' content='2018-12-03T15:17:12+00:00'><meta property='article:modified_time' content='2018-12-03T15:17:12+00:00'><meta name=twitter:title content="Layer Normalization"><meta name=twitter:description content="Layer Normalization，之前看到一篇论文用了这个LN层，看一下这个怎么实现。原文链接：[Layer Normalization](https://arxiv.org/abs/1607.06450.pdf)"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/><img src=/blog/img/avatar_hu_a95981f1fc190aef.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🚀</span></figure><div class=site-meta><h1 class=site-name><a href=/blog>Davidham的博客</a></h1><h2 class=site-description>随便写写</h2></div></header><ol class=menu-social><li><a href=https://github.com/Davidham3 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/blog/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#31-layer-normalized-recurrent-neural-networks>3.1 Layer normalized recurrent neural networks</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/>论文阅读笔记</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/layer-normalization/>Layer Normalization</a></h2><h3 class=article-subtitle>Layer Normalization，之前看到一篇论文用了这个LN层，看一下这个怎么实现。原文链接：[Layer Normalization](https://arxiv.org/abs/1607.06450.pdf)</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Dec 03, 2018</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 4 分钟</time></div></footer></div></header><section class=article-content><p>Layer Normalization，之前看到一篇论文用了这个LN层，看一下这个怎么实现。原文链接：<a class=link href=https://arxiv.org/abs/1607.06450.pdf target=_blank rel=noopener>Layer Normalization</a></p><h1 id=abstract>Abstract</h1><p>训练神经网络很费时，一个减少训练时间的方法是对神经元的激活值归一化。最近的一项技术称为小批量归一化，也就是 batch norm，使用一个神经元的输入的分布，在一个批量的样本上计算均值和方差，然后在神经元的每个训练样例上做归一化。这个能极大地缩短训练时间。但是，batch norm 的效果和 batch size 有关，而且还不知道怎么应用在 RNN 上。我们使用一个训练样例，将 BN 转置，计算一个层上面所有神经元的输入的均值和方差来归一化。就像 BN 一样，我们在归一化后激活之前给每个神经元它自己的可适应的bias和gain。不像 BN 的地方是，LN 在训练和测试的时候都有，通过在每个时间步上做归一化的统计，LN 也能应用在 RNN 上。LN 在稳定 RNN 隐藏状态的动态性上面很有效。经验表明，LN 与之前的技术对比能有效地减少训练时间。</p><h1 id=1-introduction>1 Introduction</h1><p>很多深度神经网络要训练好多天。BN 除了提升了收敛速度，从批量统计量得到的随机性在训练的时候还会作为一个正则项。</p><p>尽管 BN 简单，但是它需要输入统计量之和的平均值。在定长的 FNN 中，把每个层的 BN 存起来就行。但是，RNN 的循环单元的输入通常随序列长度而变化，所以将 BN 应用在 RNN 上面，不同时间步需要不同的统计量。此外，BN 不能应用在在线学习等任务上，或是非常大的分布式模型上，因为 minibatch 会很小。</p><h1 id=2-background>2 Background</h1><p>前向神经网络是从输入模式 $\rm{x}$ 映射到输出向量 $y$ 的非线性变换。在深度前向神经网络中的第 $l$ 个隐藏层，$a^l$ 表示这层神经元的输入。汇总后的输入通过一个线性映射计算如下：</p>$$\tag{1}
a^l\_i = {w^l\_i}^\text{T} h^l\\
h^{l+1}\_i = f(a^l\_i + b^l\_i)
$$<p>其中 $f(\cdot)$ 是激活函数，$w^l_i$ 和 $b^l_i$ 分别是第 $l$ 个隐藏层的权重和偏置参数。参数通过基于梯度的学习方法得到。</p><p>深度学习的一个挑战是：某一层权重的梯度和上一层的输出高度相关，尤其是当这些输出以一种高度相关的方式变化的时候。BN 提出来是减少这种不希望的 covariate shift 现象。这种方法在输入样例在每个隐藏单元的输入上做计算。详细来说，对于第 $l$ 层的第 $i$ 个输入，BN 根据他们在数据中的分布，将输入缩放了：</p>$$\tag{2}
\bar{a}^l\_i = \frac{g^l\_i}{\sigma^l\_i}(a^l\_i - \mu^l\_i)\\
\mu^l\_i = \mathbb{E}\_{\mathrm{x} \sim P(\mathrm{x})}[a^l\_i]\\
\sigma^l\_i = \sqrt{\mathbb{E}\_{\mathrm{x} \sim P(\mathrm{x})}[(a^l\_i - \mu^l\_i)^2]}
$$<p>其中 $\bar{a}^l_i$ 是第 $l$ 层第 $i$ 个输入的归一化结果，$g_i$ 是在非线性激活函数之前的一个增益参数，对归一化激活值进行缩放。注意，期望是在所有训练数据上的。事实上计算式2中的期望是不实际的，因为这需要用当前的参数，前向传播过所有的训练集。实际中是用当前的 mini-batch 来估计 $\mu$ 和 $\sigma$。这就给 batch size 增加了限制，而且很难应用到 RNN 上。</p><h1 id=3-layer-normalization>3 Layer normalization</h1><p>层归一化用来克服批量归一化的一些缺点。</p><p>一个层输出的变换倾向于导致下一层的输入之间有着关联度很高的变化，尤其是使用 ReLU 激活后，这些输出的变化很多。这表明 covariate shift 问题可以通过固定每层的输入的均值和方差解决。因此，在同一层中所有隐藏单元的层归一化统计量如下：</p>$$\tag{3}
\mu^l = \frac{1}{H} \sum^H\_{i = 1}a^l\_i\\
\sigma^l = \sqrt{\frac{1}{H} \sum^H\_{i=1} (a^l\_i - \mu^l)^2}
$$<p>其中 $H$ 表示层内的隐藏单元数。式2和式3的区别是在层归一化之下，层内所有隐藏单元共享相同的归一化项 $\mu$ 和 $\sigma$，但是不同的样本有着不同的归一化项。不像 BN，层归一化不会有 batch size 的限制，而且可以使用在 batch size 设为1的时候的在线学习上。</p><h2 id=31-layer-normalized-recurrent-neural-networks>3.1 Layer normalized recurrent neural networks</h2><p>最近的序列到序列模型 [Sutskever et al., 2014] 利用了紧致的 RNN 来解决 NLP 中的序列预测问题。在 NLP 任务中不同的训练样例长度不一致是很常见的。RNN 在每个时间步使用的参数都是相同的。但是在使用 BN 来处理 RNN 时，我们需要计算并存储序列中每个时间步的统计量。如果一个测试的序列比任何训练的序列都长，那就会出问题了。层归一化不会有这样的问题，因为它的归一化项只依赖于当前时间步层的输入。它在所有的时间步上也有一组共享的 gain 和 bias 参数。</p><p>在标准的 RNN 中，循环层的输入通过当前的输入 $\mathrm{x}^t$ 和前一层的隐藏状态 $\mathrm{h}^{t-1}$，得到 $\mathrm{a}^t = W_{hh}h^{t-1} + W_{xh} \mathrm{x}^t$。层归一化后的循环层会将它的激活值使用像式3一样的归一化项缩放到：</p>$$\tag{4}
\mathrm{h}^t = f[\frac{\mathrm{g}}{\sigma^t} \odot (\mathrm{a}^t - \mu^t) + b]\\
\mu^t = \frac{1}{H} \sum^H\_{i=1}a^t\_i\\
\sigma^t = \sqrt{\frac{1}{H} \sum^H\_{i=1}(a^t\_i - \mu^t)^2}
$$<p>其中 $W_{hh}$ 是循环隐藏到隐藏的权重，$W_{xh}$ 是输入到隐藏的权重，$\odot$ 是element-wise multiplication。$\rm b$ 和 $\rm g$是和 $\mathrm{h}^t$ 同维度的 bias 和 gain 参数。</p><p>在标准的 RNN 中，每个时间步的循环单元的输入的数量级倾向于增大或减小，导致梯度的爆炸或消失问题。在一个层归一化的 RNN 里，归一化项使它对一个层的输入的缩放不发生变化，使得隐藏到隐藏动态性更稳定。</p></section><footer class=article-footer><section class=article-tags><a href=/blog/tags/deep-learning/>Deep Learning</a>
<a href=/blog/tags/machine-learning/>Machine Learning</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under Apache License 2.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/blog/p/deeper-insights-into-graph-convolutional-networks-for-semi-supervised-learning/><div class=article-details><h2 class=article-title>Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</h2></div></a></article><article><a href=/blog/p/semi-supervised-classification-with-graph-convolutional-networks/><div class=article-details><h2 class=article-title>Semi-Supervised Classification With Graph Convolutional Networks</h2></div></a></article><article><a href=/blog/p/lattice-lstm-%E4%B8%AD%E6%96%87ner/><div class=article-details><h2 class=article-title>Lattice LSTM 中文NER</h2></div></a></article><article><a href=/blog/p/%E9%97%A8%E6%8E%A7%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E5%BB%BA%E6%A8%A1/><div class=article-details><h2 class=article-title>门控卷积网络语言建模</h2></div></a></article><article><a href=/blog/p/convolutional-sequence-to-sequence-learning/><div class=article-details><h2 class=article-title>Convolutional Sequence to Sequence Learning</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2020 -
2026 Davidham的博客</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>