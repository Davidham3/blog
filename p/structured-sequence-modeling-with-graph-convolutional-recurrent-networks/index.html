<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="ICLR 2017(reject)，两个模型，第一个是将数据扔到Defferrard的图卷积里面，然后将输出扔到LSTM里面。第二个模型是将RNN中的矩阵乘法换成了图卷积操作，最后对动态的mnist进行了识别。原文链接：[Structured Sequence Modeling With Graph Convolutional Recurrent Networks](https://arxiv.org/abs/1612.07659v1)"><title>Structured Sequence Modeling With Graph Convolutional Recurrent Networks</title>
<link rel=canonical href=https://davidham3.github.io/blog/p/structured-sequence-modeling-with-graph-convolutional-recurrent-networks/><link rel=stylesheet href=/blog/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Structured Sequence Modeling With Graph Convolutional Recurrent Networks"><meta property='og:description' content="ICLR 2017(reject)，两个模型，第一个是将数据扔到Defferrard的图卷积里面，然后将输出扔到LSTM里面。第二个模型是将RNN中的矩阵乘法换成了图卷积操作，最后对动态的mnist进行了识别。原文链接：[Structured Sequence Modeling With Graph Convolutional Recurrent Networks](https://arxiv.org/abs/1612.07659v1)"><meta property='og:url' content='https://davidham3.github.io/blog/p/structured-sequence-modeling-with-graph-convolutional-recurrent-networks/'><meta property='og:site_name' content='Davidham的博客'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='deep learning'><meta property='article:tag' content='Graph'><meta property='article:published_time' content='2018-07-23T10:59:15+00:00'><meta property='article:modified_time' content='2018-07-23T10:59:15+00:00'><meta name=twitter:title content="Structured Sequence Modeling With Graph Convolutional Recurrent Networks"><meta name=twitter:description content="ICLR 2017(reject)，两个模型，第一个是将数据扔到Defferrard的图卷积里面，然后将输出扔到LSTM里面。第二个模型是将RNN中的矩阵乘法换成了图卷积操作，最后对动态的mnist进行了识别。原文链接：[Structured Sequence Modeling With Graph Convolutional Recurrent Networks](https://arxiv.org/abs/1612.07659v1)"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/><img src=/blog/img/avatar_hu_a92d3b55c5d43e55.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🚀</span></figure><div class=site-meta><h1 class=site-name><a href=/blog>Davidham的博客</a></h1><h2 class=site-description>随便写写</h2></div></header><ol class=menu-social><li><a href=https://github.com/Davidham3 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/blog/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#21-structured-sequence-modeling>2.1 Structured Sequence Modeling</a></li><li><a href=#22-long-short-term-memory>2.2 Long Short-Term Memory</a></li><li><a href=#23-convolutional-neural-networks-on-graphs>2.3 Convolutional Neural Networks On Graphs</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/>论文阅读笔记</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/structured-sequence-modeling-with-graph-convolutional-recurrent-networks/>Structured Sequence Modeling With Graph Convolutional Recurrent Networks</a></h2><h3 class=article-subtitle>ICLR 2017(reject)，两个模型，第一个是将数据扔到Defferrard的图卷积里面，然后将输出扔到LSTM里面。第二个模型是将RNN中的矩阵乘法换成了图卷积操作，最后对动态的mnist进行了识别。原文链接：[Structured Sequence Modeling With Graph Convolutional Recurrent Networks](https://arxiv.org/abs/1612.07659v1)</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 23, 2018</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 9 分钟</time></div></footer></div></header><section class=article-content><p>ICLR 2017(reject)，两个模型，第一个是将数据扔到Defferrard的图卷积里面，然后将输出扔到LSTM里面。第二个模型是将RNN中的矩阵乘法换成了图卷积操作，最后对动态的mnist进行了识别。原文链接：<a class=link href=https://arxiv.org/abs/1612.07659v1 target=_blank rel=noopener>Structured Sequence Modeling With Graph Convolutional Recurrent Networks</a></p><h1 id=摘要>摘要</h1><p>GCRN(Graph Convolutional Recurrent Network)，一个可以预测结构化序列数据的深度学习模型。GCRN是传统的循环神经网络的在任意的图结构上的一种泛化形式。这样的结构化数据可以表示成视频中的一系列帧，检测器组成的网络监测到的时空监测值，或是用于自然语言建模的词网中的随机游走。我们提出的模型合并了图上的CNN来辨识空间结构，RNN寻找动态模型。我们研究了两种GCRN，对Penn Treebank数据集进行建模。实验显示同时挖掘图的空间与动态信息可以同时提升precision和学习速度。</p><h1 id=1-introduction>1 Introduction</h1><p>很多工作，Donahue et al. 2015; Karpathy & Fei-Fei 2015; Vinyals et al. 2015，利用CNN和RNN的组合来挖掘时空规律性。这些模型那个处理时间变化的视觉输入来做变长的预测。这些网络架构由视觉特征提取的CNN，和一个在CNN后面，用于序列学习的RNN组成。这样的架构成功地用于视频活动识别，图像注释生成以及视频描述。</p><p>最近，大家开始对时空序列建模时融合CNN和RNN感兴趣。受到语言模型的启发，Ranzato et al. 2014提出了通过发现时空相关性的能表示复杂变形和动作模式的模型。他们的实验表明在通过quantizing the image patches获得到的visual words上，使用RNN建模，可以很好的预测视频的下一帧以及中间帧。他们的表现最好的模型是recursive CNN(rCNN)，对输入和状态同时使用卷积。Shi et al. 2015之后提出了卷积LSTM(convLSTM)，一个使用2D卷积利用输入数据的空间相关性，用于时空序列建模的RNN模型。他们成功的对降雨临近预报的雷达回波图的演化进行了预测。</p><p>很多重要问题中，空间结构不是简单的网格状。气象站就不是网格状。而且空间结构不一定是空间上的，如社交网络或生物网络。最后，Mikolov et al. 2013等人认为，句子可以解释成在词网上的随机游走，使得我们转向了分析图结构的句子建模问题。</p><p>我们的工作利用了近期的模型——Defferrard et al. 2016; Ranzato et al. 2014; Shi et al. 2015——来设计GCRN模型对时间变化的图结构数据建模和预测。核心思想是融合图结构上的CNN和RNN来同时辨识空间结构和动态模式。图1给出了GCRN的架构。</p><p><img src=/blog/images/structured-sequence-modeling-with-graph-convolutional-recurrent-networks/Fig1.JPG loading=lazy alt=Figure1></p><h1 id=2-preliminaries>2 Preliminaries</h1><h2 id=21-structured-sequence-modeling>2.1 Structured Sequence Modeling</h2><p>序列建模是给定前$J$个观测值，对未来最可能的长度为$K$的序列进行预测：</p>$$\tag{1}
\hat{x}\_{t+1},...,\hat{x}\_{t+K} = \mathop{\mathrm{argmax}}\limits\_{x\_{t+1},...,x\_{t+K}}P(x\_{t+1},...,x\_{t+K} \mid x\_{t-J+1},...,x\_t),
$$<p>$x_t \in \mathbf{D}$是时间$t$的观测值，$\mathbf{D}$表示观测到的特征的域。原型应用是$n-\mathrm{gram}$模型$(n = J + 1)$，$P(x_{t+1} \mid x_{t-J+1},&mldr;,x_t)$对在句子中给定过去$J$个词时$x_{t+1}$出现的概率进行建模。</p><p>我们感兴趣的是特别的结构化的句子，也就是句子中$x_t$的特征不是相互独立的，而是有着两两相连的关系。这样的关系广义上通过带权图建模。</p><p>$x_t$可以看作是一个图信号，也就是一个定义在无向带权图$\mathcal{G} = ( \mathcal{V}, \Large{\varepsilon}, \normalsize{A )}$，其中$\mathcal{V}$是$\vert \mathcal{V} \vert = n$个顶点的有限集，$\Large{\varepsilon}$是边集，$A \in \mathbb{R}^{n \times n}$是带权邻接矩阵，编码了两个顶点之间的连接权重。定义在图的顶点上的信号$x_t: \mathcal{V} \rightarrow \mathbb{R}^{d_x}$可以当作是一个矩阵$x_t \in \mathbb{R}^{n \times d_x}$，列$i$是$d_x$维向量，表示$x_t$在第$i$个顶点的值。尽管自由变量的数量在长度为$K$的结构化序列中本质上是$\mathcal{O}(n^K{d_x}^K)$，我们仍然试图去挖掘可能的预测结果的空间结构以减少维度，来使这些问题变得容易解决。</p><h2 id=22-long-short-term-memory>2.2 Long Short-Term Memory</h2><p>防止梯度过快消失，由Hochreiter & Schmidhuber 1997发明的一种RNN，LSTM。这个模型已经被证明在各种序列建模任务中，对长期依赖关系是稳定且强劲的模型(Graves, 2013; Srivastava et al., 2015; Sutskever et al., 2014)。全连接LSTM(FC-LSTM)可以看作是一个多变量版本的LSTM，其中$x_t \in \mathbb{R}^d_x$是输入，$h_t \in [-1, 1]^{d_h}$是细胞状态，$c_t \in \mathbb{R}^{d_h}$是隐藏状态，他们都是向量。我们使用Graves 2013的FC-LSTM：</p>$$\tag{2}
i = \sigma(W\_{xi} x\_t + W\_{hi}h\_{t-1} + w\_{ci} \odot c\_{t-1} + b\_i),\\
f = \sigma(W\_{xf} x\_t + W\_{hf} h\_{t-1} + w\_{cf} \odot c\_{t-1} + b\_f),\\
c\_t = f\_t \odot c\_{t-1} + i\_t \odot \mathrm{tanh}(W\_{xc} x\_t + W\_{hc} h\_{t-1} + b\_c),\\
o = \sigma(W\_{xo} x\_t + W\_{ho} h\_{t-1} + w\_{co} \odot c\_t + b\_o),\\
h\_t = o \odot \mathrm{tanh}(c\_t),
$$<p>其中$\odot$表示Hadamard product，$\sigma(\cdot)$表示sigmoid function $\sigma(x) = 1/(1+e^{-x})$，$i,f,o \in [0, 1]^{d_h}$是输入门，遗忘门，输出门。权重$W_{x\cdot} \in \mathbb{R}^{d_h \times d_x}$，$W_{h\cdot} \in \mathbb{R}^{d_h \times d_h}$，$w_{c\cdot} \in \mathbb{R}^{d_h}$，偏置$b_i,b_f,b_c,b_o \in \mathbb{R}^{d_h}$是模型参数。这个模型之所以称为全连接是因为$W_{x\cdot}$和$W_{h\cdot}$与$x$和$h$所有分量进行线性组合。由Gers & Schmidhuber 2000引入可选的peephole connections $w_{c\cdot} \odot c_t$，在某些特定任务上可以提升性能。</p><h2 id=23-convolutional-neural-networks-on-graphs>2.3 Convolutional Neural Networks On Graphs</h2><p>Defferrard et al., 2016选择了谱上的卷积操作：</p>$$\tag{3}
y = g\_\theta \ast\_\mathcal{G} x = g\_\theta (L)x = g\_\theta (U \Lambda U^T)x = U g\_\theta (\Lambda) U^T x \in \mathbb{R}^{n \times d\_x},
$$<p>对于归一化的拉普拉斯矩阵$L = I_n - D^{-1/2} A D^{-1/2} = U \Lambda U^T \in \mathbb{R}^{n \times n}$来说，$U \in \mathbb{R}^{n \times n}$是矩阵的特征向量，$\Lambda \in \mathbb{R}^{n \times n}$是特征值的对角矩阵。式3的时间复杂度很高，因为$U$的乘法的时间复杂度是$\mathcal{O}(n^2)$。此外，计算$L$的特征值分解对于大的图来说很慢。Defferrard et al., 2016使用切比雪夫多项式：</p>$$\tag{4}
g\_\theta(\Lambda) = \sum^{K-1}\_{k=0} \theta\_k T\_k(\tilde{\Lambda}),
$$<p>参数$\theta \in \mathbb{R}^K$是切比雪夫系数的向量，$T_k(\tilde{\Lambda}) \in \mathbb{R}^{n \times n}$是切比雪夫多项式的k阶项在$\tilde{\Lambda} = 2\Lambda/\lambda_{max} - I_n$的值。图卷积操作可以写为：</p>$$\tag{5}
y = g\_\theta \ast\_{\mathcal{G}} x = g\_\theta (L) x = \sum^{K-1}\_{k=0} \theta\_k T\_k (\tilde{L})x,
$$<p>$T_0 = 1$，$T_1 = x$，$T_k(x) = 2xT_{k-1}(x)-T_{k-2}(x)$，时间复杂度是$\mathcal{O}(K \vert \Large{\varepsilon} \normalsize \vert)$，也就是和边数相关。这个图卷积是$K$阶局部化的。</p><h1 id=3-related-works>3 Related Works</h1><p>Shi et al. 2015提出了针对常规网格结构的序列的模型，可以看作是图是图像网格且顶点有序的特殊情况。他们的模型本质上是FC-LSTM，$W$的乘法替换为卷积核$W$：</p>$$\tag{6}
i = \sigma(W\_{xi} \ast x\_t + W\_{hi} \ast h\_{t-1} + w\_{ci} \odot c\_{t-1} + b\_i),\\
f = \sigma(W\_{xf} \ast x\_t + W\_{hf} \ast h\_{t-1} + w\_{cf} \odot c\_{t-1} + b\_f),\\
c\_t = f\_t \odot c\_{t-1} + i\_t \odot \mathrm{tanh}(W\_{xc} \ast x\_t + W\_{hc} \ast h\_{t-1} + b\_c),\\
o = \sigma(W\_{xo} \ast x\_t + W\_{ho} \ast h\_{t-1} + w\_{co} \odot c\_t + b\_o),\\
h\_t = o \odot \mathrm{tanh}(c\_t),
$$<p>$\ast$表示一组卷积核的2D卷积。在他们的设定中$x_t \in \mathbb{R}^{n_r \times n_c \times d_x}$是一个动态系统中，时间$t$的$d_x$的观测值，这个动态系统建立在一个表示为$n_r$行$n_c$列的空间区域上。模型有着空间分布的隐藏核细胞状态，大小是$d_h$，由张量$c_t$体现，$h_t \in \mathbb{R}^{n_r \times n_c \times d_h}$。卷积核$W_{h\cdot} \in \mathbb{R}^{m \times m \times d_h \times d_h}$和$W_{x\cdot} \in \mathbb{R}^{m \times m \times d_h \times d_x}$的尺寸$m$决定了参数的数量，与网格大小$n_r \times n_c$无关。更早一点，Ranzato et al. 2014提出了相似的RNN变体，使用卷积层而不是全连接层。时间$t$的隐藏状态：</p>$$\tag{7}
h\_t = \mathrm{tanh}(\sigma(W\_{x2} \ast \sigma(W\_{x1} \ast x\_t)) + \sigma(W\_h \ast h\_{t-1})),
$$<p>卷积核$W_h \in \mathbb{R}^{d_h \times d_h}$受限到$1 \times 1$的大小。</p><p>观察到自然语言表示出语法性质，自然的将词融入短语中，Tai et al. 2015提出了一个处理树结构的模型，每个LSTM可以获取他们的孩子的状态。他们在semantic relatedness and sentiment classification上获得了state-of-the-art的结果。Liang et al. 2016在之后提出了在图上的变体。他们复杂的网络结构在4个数据集上获得了semantic object parsing的state-of-the-art结果。这些模型中，状态通过一个可训练的权重矩阵的带权加和从邻居上聚集。然而这些权重并不在图上共享，否则需要对顶点排序，就像其他图卷积的空间定义一样。此外，他们的公式受限于当前顶点的一阶邻居，给其他的邻居相同的权重。</p><p>受到如人体动作和物体交互等时空任务的启发，Jain et al 2016提出了一个方法将时空图看作是一个富RNN的混合，本质上是将一个RNN连接到每个顶点与每条边上。同样的，通信受限于直接连接的顶点与边。</p><p>和我们的工作最相关的模型可能是Li et al 2015提出的模型，在program verification上表现出了最好的结果。尽管他们使用Scarselli et al. 2009提出的GNN，以迭代的步骤传播顶点的表示，直到收敛，我们使用的是Defferrard et al. 2016提出的GCN在顶点间扩散信息。尽管他们的动机和我们很不一样，这些模型的关联是使用$K$阶多项式定义的谱滤波器可以实现成一个$K$层的GNN。</p><h1 id=4-proposed-gcrn-models>4 Proposed GCRN Models</h1><p>我们提出了两种GCRN架构</p><p><strong>Model 1.</strong></p>$$\tag{8}
x^{\mathrm{CNN}}\_t = \mathrm{CNN}\_\mathcal{G}(x\_t)\\
i = \sigma(W\_{xi} x^{\mathrm{CNN}}\_t + W\_{hi}h\_{t-1} + w\_{ci} \odot c\_{t-1} + b\_i),\\
f = \sigma(W\_{xf} x^{\mathrm{CNN}}\_t + W\_{hf} h\_{t-1} + w\_{cf} \odot c\_{t-1} + b\_f),\\
c\_t = f\_t \odot c\_{t-1} + i\_t \odot \mathrm{tanh}(W\_{xc} x^{\mathrm{CNN}}\_t + W\_{hc} h\_{t-1} + b\_c),\\
o = \sigma(W\_{xo} x^{\mathrm{CNN}}\_t + W\_{ho} h\_{t-1} + w\_{co} \odot c\_t + b\_o),\\
h\_t = o \odot \mathrm{tanh}(c\_t).
$$<p>我们简单地写成$x^{\mathrm{CNN}}_t = W^{\mathrm{CNN}} \ast_\mathcal{G} x_t$，其中$W^{\mathrm{CNN}} \in \mathbb{R}^{K \times d_x \times d_x}$是切比雪夫系数。Peepholes由$w_{c\cdot} \in \mathbb{R}^{n \times d_h}$控制。这样的架构可能足以捕获数据的分布，通过挖掘局部静止性以及性质的组合性，还有动态属性。</p><p><strong>Model 2.</strong></p>$$\tag{9}
i = \sigma(W\_{xi} \ast\_\mathcal{G} x\_t + W\_{hi} \ast\_\mathcal{G} h\_{t-1} + w\_{ci} \odot c\_{t-1} + b\_i),\\
f = \sigma(W\_{xf} \ast\_\mathcal{G} x\_t + W\_{hf} \ast\_\mathcal{G} h\_{t-1} + w\_{cf} \odot c\_{t-1} + b\_f),\\
c\_t = f\_t \odot c\_{t-1} + i\_t \odot \mathrm{tanh}(W\_{xc} \ast\_\mathcal{G} x\_t + W\_{hc} \ast\_\mathcal{G} h\_{t-1} + b\_c),\\
o = \sigma(W\_{xo} \ast\_\mathcal{G} x\_t + W\_{ho} \ast\_\mathcal{G} h\_{t-1} + w\_{co} \odot c\_t + b\_o),\\
h\_t = o \odot \mathrm{tanh}(c\_t),
$$<p>图卷积核是切比雪夫系数$W_{h\cdot} \in \mathbb{R}^{K \times d_h \times d_h}$，$W_{x\cdot} \in \mathbb{R}^{K \times d_h \times d_x}$决定了参数的数目，与顶点数$n$无关。
这种RNN和CNN的混合，不限于LSTM。普通的RNN $h_t = \mathrm{tanh}(W_x x_t + W_h h_{t-1})$可以写为：</p>$$\tag{10}
h\_t = \mathrm{tanh}(W\_x \ast\_\mathcal{G} x\_t + W\_h \ast\_\mathcal{G} h\_{t-1}),
$$<p>GRU的版本可以写为：</p>$$\tag{11}
z = \sigma(W\_{xz} \ast\_\mathcal{G} x\_t + W\_{hz} \ast\_\mathcal{G} h\_{t-1}),\\
r = \sigma(W\_{xr} \ast\_\mathcal{G} x\_t + W\_{hr} \ast\_\mathcal{G} h\_{t-1}),\\
\tilde{h} = \mathrm{tanh}(W\_{xh} \ast\_\mathcal{G} x\_t + W\_{hh} \ast\_\mathcal{G} (r \odot h\_{t-1})),\\
h\_t = z \odot h\_{t-1} + (1 - z) \odot \tilde{h}.
$$<h1 id=5-experiments>5 Experiments</h1><p><img src=/blog/images/structured-sequence-modeling-with-graph-convolutional-recurrent-networks/Table1.JPG loading=lazy alt=Table1></p><p>数据集是moving-MNIST(Shi et al., 2015)。</p><p><img src=/blog/images/structured-sequence-modeling-with-graph-convolutional-recurrent-networks/Fig3.JPG loading=lazy alt="“Figure3 & Figure4”"></p></section><footer class=article-footer><section class=article-tags><a href=/blog/tags/deep-learning/>Deep Learning</a>
<a href=/blog/tags/graph/>Graph</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under Apache License 2.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>最后更新于 Jul 23, 2018 10:59 UTC</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/blog/p/multi-range-attentive-bicomponent-graph-convolutional-network-for-traffic-forecasting/><div class=article-details><h2 class=article-title>Multi-Range Attentive Bicomponent Graph Convolutional Network for Traffic Forecasting</h2></div></a></article><article><a href=/blog/p/gman-a-graph-multi-attention-network-for-traffic-prediction/><div class=article-details><h2 class=article-title>GMAN: A Graph Multi-Attention Network for Traffic Prediction</h2></div></a></article><article><a href=/blog/p/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/><div class=article-details><h2 class=article-title>STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting</h2></div></a></article><article><a href=/blog/p/self-attention-graph-pooling/><div class=article-details><h2 class=article-title>Self-Attention Graph Pooling</h2></div></a></article><article><a href=/blog/p/session-based-social-recommendation-via-dynamic-graph-attention-networks/><div class=article-details><h2 class=article-title>Session-based Social Recommendation via Dynamic Graph Attention Networks</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Davidham的博客</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>