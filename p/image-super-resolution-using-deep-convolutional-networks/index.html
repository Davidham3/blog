<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="PAMI 2016ï¼Œå¤§ä½“æ€è·¯ï¼šæŠŠè®­ç»ƒé›†ä¸­çš„æ‰€æœ‰æ ·æœ¬æ¨¡ç³ŠåŒ–ï¼Œæ‰”åˆ°ä¸‰å±‚çš„å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼ŒæŠŠè¾“å‡ºå’ŒåŸå§‹å›¾ç‰‡åšä¸€ä¸ªlossï¼Œè®­ç»ƒæ¨¡å‹å³å¯ã€‚åŸæ–‡é“¾æ¥ï¼š[Image Super-Resolution Using Deep Convolutional Networks](https://arxiv.org/abs/1501.00092)"><title>Image Super-Resolution Using Deep Convolutional Networks</title><link rel=canonical href=https://davidham3.github.io/blog/p/image-super-resolution-using-deep-convolutional-networks/><link rel=stylesheet href=/blog/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css><meta property='og:title' content="Image Super-Resolution Using Deep Convolutional Networks"><meta property='og:description' content="PAMI 2016ï¼Œå¤§ä½“æ€è·¯ï¼šæŠŠè®­ç»ƒé›†ä¸­çš„æ‰€æœ‰æ ·æœ¬æ¨¡ç³ŠåŒ–ï¼Œæ‰”åˆ°ä¸‰å±‚çš„å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼ŒæŠŠè¾“å‡ºå’ŒåŸå§‹å›¾ç‰‡åšä¸€ä¸ªlossï¼Œè®­ç»ƒæ¨¡å‹å³å¯ã€‚åŸæ–‡é“¾æ¥ï¼š[Image Super-Resolution Using Deep Convolutional Networks](https://arxiv.org/abs/1501.00092)"><meta property='og:url' content='https://davidham3.github.io/blog/p/image-super-resolution-using-deep-convolutional-networks/'><meta property='og:site_name' content='Davidhamçš„åšå®¢'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='deep learning'><meta property='article:tag' content='machine learning'><meta property='article:tag' content='super resolution'><meta property='article:published_time' content='2018-03-02T11:19:50+00:00'><meta property='article:modified_time' content='2018-03-02T11:19:50+00:00'><meta name=twitter:title content="Image Super-Resolution Using Deep Convolutional Networks"><meta name=twitter:description content="PAMI 2016ï¼Œå¤§ä½“æ€è·¯ï¼šæŠŠè®­ç»ƒé›†ä¸­çš„æ‰€æœ‰æ ·æœ¬æ¨¡ç³ŠåŒ–ï¼Œæ‰”åˆ°ä¸‰å±‚çš„å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼ŒæŠŠè¾“å‡ºå’ŒåŸå§‹å›¾ç‰‡åšä¸€ä¸ªlossï¼Œè®­ç»ƒæ¨¡å‹å³å¯ã€‚åŸæ–‡é“¾æ¥ï¼š[Image Super-Resolution Using Deep Convolutional Networks](https://arxiv.org/abs/1501.00092)"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=åˆ‡æ¢èœå•>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/><img src=/blog/img/avatar_hu_a95981f1fc190aef.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ğŸš€</span></figure><div class=site-meta><h1 class=site-name><a href=/blog>Davidhamçš„åšå®¢</a></h1><h2 class=site-description>éšä¾¿å†™å†™</h2></div></header><ol class=menu-social><li><a href=https://github.com/Davidham3 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/blog/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>æš—è‰²æ¨¡å¼</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">ç›®å½•</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#formulation>Formulation</a><ol><li><a href=#patch-extraction-and-representation>Patch Extraction and Representation</a></li><li><a href=#non-linear-mapping>Non-Linear Mapping</a></li><li><a href=#reconstruction>Reconstruction</a></li></ol></li><li><a href=#training>Training</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/>è®ºæ–‡é˜…è¯»ç¬”è®°</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/image-super-resolution-using-deep-convolutional-networks/>Image Super-Resolution Using Deep Convolutional Networks</a></h2><h3 class=article-subtitle>PAMI 2016ï¼Œå¤§ä½“æ€è·¯ï¼šæŠŠè®­ç»ƒé›†ä¸­çš„æ‰€æœ‰æ ·æœ¬æ¨¡ç³ŠåŒ–ï¼Œæ‰”åˆ°ä¸‰å±‚çš„å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼ŒæŠŠè¾“å‡ºå’ŒåŸå§‹å›¾ç‰‡åšä¸€ä¸ªlossï¼Œè®­ç»ƒæ¨¡å‹å³å¯ã€‚åŸæ–‡é“¾æ¥ï¼š[Image Super-Resolution Using Deep Convolutional Networks](https://arxiv.org/abs/1501.00092)</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Mar 02, 2018</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>é˜…è¯»æ—¶é•¿: 3 åˆ†é’Ÿ</time></div></footer></div></header><section class=article-content><p>PAMI 2016ï¼Œå¤§ä½“æ€è·¯ï¼šæŠŠè®­ç»ƒé›†ä¸­çš„æ‰€æœ‰æ ·æœ¬æ¨¡ç³ŠåŒ–ï¼Œæ‰”åˆ°ä¸‰å±‚çš„å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼ŒæŠŠè¾“å‡ºå’ŒåŸå§‹å›¾ç‰‡åšä¸€ä¸ªlossï¼Œè®­ç»ƒæ¨¡å‹å³å¯ã€‚åŸæ–‡é“¾æ¥ï¼š<a class=link href=https://arxiv.org/abs/1501.00092 target=_blank rel=noopener>Image Super-Resolution Using Deep Convolutional Networks</a></p><p>é¦–å…ˆæ˜¯ill-posed problemï¼Œå›¾åƒçš„ä¸é€‚å®šé—®é¢˜
æ³•å›½æ•°å­¦å®¶é˜¿è¾¾é©¬æ—©åœ¨19ä¸–çºªå°±æå‡ºäº†ä¸é€‚å®šé—®é¢˜çš„æ¦‚å¿µ:ç§°ä¸€ä¸ªæ•°å­¦ç‰©ç†å®šè§£é—®é¢˜çš„è§£å­˜åœ¨ã€å”¯ä¸€å¹¶ä¸”ç¨³å®šçš„åˆ™ç§°è¯¥é—®é¢˜æ˜¯é€‚å®šçš„ï¼ˆWell Posedï¼‰.å¦‚æœä¸æ»¡è¶³é€‚å®šæ€§æ¦‚å¿µä¸­çš„ä¸Šè¿°åˆ¤æ®ä¸­çš„ä¸€æ¡æˆ–å‡ æ¡ï¼Œç§°è¯¥é—®é¢˜æ˜¯ä¸é€‚å®šçš„ã€‚</p><h1 id=convolutional-neural-networks-for-super-resolution>Convolutional Neural Networks For Super-Resolution</h1><h2 id=formulation>Formulation</h2><p>We first upscale a single low-resolution image to the desired size using bicubic interpolation. Let us denote the interpolated image as $Y$. Our goal is to recover from $Y$ an image $F(Y)$ that is as similar as possible to the ground truth high-resolution image $X$. For the ease of presentation, we still call $Y$ a &ldquo;low-resolution&rdquo; image, although it has the same size as $X$. We wish to learning a mapping $F$, which conceptually consists of three operations:</p><ol><li>Patch extraction and representation. this operation extracts (overlapping) patches from the low-resolution image $Y$ and represents each patch as a high-dimensional vector. These vectors comprise a set of feature maps, of which the number equals to the dimensionality of the vectors.</li><li>Non-linear mapping. this operation nonlinearly maps each high-dimensional vector onto another high-dimensional vector. Each mapped vector is conceptually the representation of a high-resolution patch. These vectors comprise another set of feature maps.</li><li>Reconstruction. this operation aggregates the above high-resolution patch-wise representations to generate the final high-resolution image. This image is expected to be similar to the ground truth $X$.</li></ol><h3 id=patch-extraction-and-representation>Patch Extraction and Representation</h3><p>A popular strategy in image restoration is to densely extract patches and then represent them by a set of pre-trained bases such as PCA, DCT, Haar, etc. This is equivalent to convolving the image by a set of filters, each of which is a basis. In our formulation, we involve the optimization of these bases into the optimization of the network. Formally, our first layer is expressed as an operation $F_1$:</p>$$F\_1(Y)=max(0, W\_1 * Y + B\_1)$$<p>where $W_1$ and $B_1$ represent the filters and biases respectively, and $*$ denotes the convolution operation. Here, $W_1$ corresponds to $n_1$ filters of support $c \times f_1 \times f_1$, where $c$ is the number of channels in the input image, $f_1$ is the spatial size of a filter. Intuitively, $W_1$ applies $n_1$ convolutions on the image, and each convolution has a kernel size $c \times f_1 \times f_1$. The output is composed of $n_1$ feature maps. $B_1$ is an $n_1$-dimensional vector, whose each element is associated with a filter. We apply the ReLU on the filter responses.</p><h3 id=non-linear-mapping>Non-Linear Mapping</h3><p>The first layer extracts an $n_1$-dimensional feature for each patch. In the second operation, we map each of these $n_1$-dimensional vectors into an $n_2$-dimensional one. This is equivalent to applying $n_2$ filters which have a trivial spatial support $1 \times 1$. This interpretation is only valid for $1 \times 1$ filters. But it is easy to generalize to larger filters like $3 \times 3$ or $5 \times 5$. In that case, the non-linear mapping is not on a patch of the input image; instead, it is on a $3 \times 3$ or $5 \times 5$ &ldquo;patch&rdquo; of the feature map. The operation of the second layer is:</p>$$F\_2(Y) = max(0, W\_2 * F\_1(Y) + B\_2)$$<p>Here $W_2$ contains $n_2$ filters of size $n_1 \times f_2 \times f_2$, and $B_2$ is $n_2$-dimensional. Each of the output $n_2$-dimensional vectors is conceptually a representation of a high-resolution patch that will be used for reconstruction.</p><h3 id=reconstruction>Reconstruction</h3><p>In the traditional methods, the predicted overlapping high-resolution patches are often averaged to produce the final full image. The averaging can be considered as a pre-defined filter on a set of feature maps (where each position is the &ldquo;flattened&rdquo; vector form of a high-resolution patch). Motivated by this, we define a convolutional layer to produce the final high-resolution image:</p>$$F(Y)=W\_3 * F\_2(Y) + B\_3$$<p>Here W_3 corresponds to $c$ filters of a size $n_2 \times f_3 \times f_3$, and $B_3$ is a $c$-dimensional vector.</p><h2 id=training>Training</h2><p>Loss function: given a set of high-resolution images ${X_i}$ and their corresponding low-resolution images ${Y_i}$, we use mean squared error (MSE) as the loss function:</p>$$L(\Theta ) = \frac{1}{n}\sum^n\_{i=1}\Vert F(Y\_i;\Theta)-X\_i\Vert ^2$$<p>where $n$ is the number of training samples. Using MSE as the loss function favors a high PSNR. The PSNR is widely-used metric for quantitatively evaluating image restoration quality, and is at least partially related to the perceptual quality. Despite that the proposed model is trained favoring a high PSNR, we still observe satisfactory performance when the model is evaluated using alternative evaluation metrics, e.g., SSIM, MSSIM.
PSNR: Peak Signal to Noise Ratio. æ˜¯ä¸€ç§è¯„ä»·å›¾åƒçš„å®¢è§‚æ ‡å‡†ã€‚</p>$$PSNR = 10 \times \log\_{10}(\frac{(2^n-1)^2}{MSE})$$<p>å…¶ä¸­ï¼ŒMSEæ˜¯åŸå›¾åƒå’Œå¤„ç†å›¾åƒä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼Œnæ˜¯æ¯ä¸ªé‡‡æ ·å€¼çš„æ¯”ç‰¹æ•°ï¼Œå•ä½æ˜¯dBã€‚
The loss is minimized using stochastic gradient descent with the standard backpropagation. In particular, the weight matrices are updated as</p>$$\Delta\_{i+1}=0.9 \cdot \Delta\_i + \eta \cdot \frac{\partial{L}}{\partial{W^\ell\_i}}, W^\ell\_{i+1}=W^\ell\_{i}+\Delta\_{i+1}$$<p>where $\ell \in {1,2,3}$ and $i$ are the indices of layers and iterations, $\eta$ is the learning rate, and $\frac{\partial{L}}{\partial{W^\ell_i}}$ is the derivative. The filter weights of each layer are initialized by drawing randomly from a Gaussian distribution with zero mean and standard deviation 0.0001 (and 0 for biases). The learning rate is $10^{-4}$ for the first two layers, and $10^{-5}$ for the last layer. We empirically find that a smaller learning rate in the last layer is important for the network to converge (similar to the denoising case).
In the training phase, the ground truth images ${X_i}$ are prepared as $f_{sub} \times f_{sub} \times c$-pixel sub-images randomly cropped from the training images. By &ldquo;sub-images&rdquo; we mean these samples are treated as small &ldquo;images&rdquo; rather than &ldquo;patches&rdquo;, in the sense that &ldquo;patches&rdquo; are overlapping and require some averaging as post-processing but &ldquo;sub-images&rdquo; need not. To synthesize the low-resolution samples ${Y_i}$, we blur a sub-image by a Gaussian kernel, sub-sample it by the upscaling factor, and upscale it by the same factor via bicubic interpolation.
To avoid border effects during training, all the convolutional layers have no padding, and the network produces a smaller output $((f_{sub}-f_1-f_2-f_3+3)^2 \times c)$. The MSE loss function is evaluated only by the difference between the contral pixels of $X_i$ and the network output. Although we use a fixed image size in training, the convolutional nerual network can be applied on images of arbitrary sizes during testing.</p></section><footer class=article-footer><section class=article-tags><a href=/blog/tags/deep-learning/>Deep Learning</a>
<a href=/blog/tags/machine-learning/>Machine Learning</a>
<a href=/blog/tags/super-resolution/>Super Resolution</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under Apache License 2.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>ç›¸å…³æ–‡ç« </h2><div class=related-content><div class="flex article-list--tile"><article><a href=/blog/p/perceptual-losses-for-real-time-style-transfer-and-super-resolution/><div class=article-details><h2 class=article-title>Perceptual Losses for Real-Time Style Transfer and Super-Resolution</h2></div></a></article><article><a href=/blog/p/layer-normalization/><div class=article-details><h2 class=article-title>Layer Normalization</h2></div></a></article><article><a href=/blog/p/deeper-insights-into-graph-convolutional-networks-for-semi-supervised-learning/><div class=article-details><h2 class=article-title>Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</h2></div></a></article><article><a href=/blog/p/semi-supervised-classification-with-graph-convolutional-networks/><div class=article-details><h2 class=article-title>Semi-Supervised Classification With Graph Convolutional Networks</h2></div></a></article><article><a href=/blog/p/lattice-lstm-%E4%B8%AD%E6%96%87ner/><div class=article-details><h2 class=article-title>Lattice LSTM ä¸­æ–‡NER</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2020 -
2026 Davidhamçš„åšå®¢</section><section class=powerby>ä½¿ç”¨ <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> æ„å»º<br>ä¸»é¢˜ <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> ç”± <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> è®¾è®¡</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>