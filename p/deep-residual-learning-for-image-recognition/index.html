<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="CVPR 2015ÔºåResNetÔºåÂéüÊñáÈìæÊé•Ôºö[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)"><title>Deep Residual Learning for Image Recognition</title><link rel=canonical href=https://davidham3.github.io/blog/p/deep-residual-learning-for-image-recognition/><link rel=stylesheet href=/blog/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css><meta property='og:title' content="Deep Residual Learning for Image Recognition"><meta property='og:description' content="CVPR 2015ÔºåResNetÔºåÂéüÊñáÈìæÊé•Ôºö[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)"><meta property='og:url' content='https://davidham3.github.io/blog/p/deep-residual-learning-for-image-recognition/'><meta property='og:site_name' content='DavidhamÁöÑÂçöÂÆ¢'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='deep learning'><meta property='article:tag' content='machine learning'><meta property='article:tag' content='ResNet'><meta property='article:published_time' content='2018-03-04T18:59:20+00:00'><meta property='article:modified_time' content='2018-03-04T18:59:20+00:00'><meta name=twitter:title content="Deep Residual Learning for Image Recognition"><meta name=twitter:description content="CVPR 2015ÔºåResNetÔºåÂéüÊñáÈìæÊé•Ôºö[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=ÂàáÊç¢ËèúÂçï>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/><img src=/blog/img/avatar_hu_a95981f1fc190aef.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>üöÄ</span></figure><div class=site-meta><h1 class=site-name><a href=/blog>DavidhamÁöÑÂçöÂÆ¢</a></h1><h2 class=site-description>Èöè‰æøÂÜôÂÜô</h2></div></header><ol class=menu-social><li><a href=https://github.com/Davidham3 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/blog/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>ÊöóËâ≤Ê®°Âºè</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">ÁõÆÂΩï</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#problems>problems</a></li><li><a href=#deep-residual-learning>Deep Residual Learning</a><ol><li><a href=#residual-learning>Residual Learning</a></li><li><a href=#identity-mapping-by-shortcuts>Identity Mapping by Shortcuts</a></li></ol></li><li><a href=#residual-network>Residual Network</a></li><li><a href=#implementation>Implementation</a></li><li><a href=#imagenet-classification>ImageNet classification</a><ol><li><a href=#deeper-bottleneck-architecture>Deeper Bottleneck Architecture</a></li></ol></li><li><a href=#cifar-10-and-analysis>CIFAR-10 and Analysis</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/>ËÆ∫ÊñáÈòÖËØªÁ¨îËÆ∞</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/deep-residual-learning-for-image-recognition/>Deep Residual Learning for Image Recognition</a></h2><h3 class=article-subtitle>CVPR 2015ÔºåResNetÔºåÂéüÊñáÈìæÊé•Ôºö[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Mar 04, 2018</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>ÈòÖËØªÊó∂Èïø: 3 ÂàÜÈíü</time></div></footer></div></header><section class=article-content><p>CVPR 2015ÔºåResNetÔºåÂéüÊñáÈìæÊé•Ôºö<a class=link href=https://arxiv.org/abs/1512.03385 target=_blank rel=noopener>Deep Residual Learning for Image Recognition</a></p><h1 id=deep-residual-learning-for-image-recongnition>Deep Residual Learning for Image Recongnition</h1><h2 id=problems>problems</h2><p>When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error, as reported in [11, 42] and thoroughly verified by our experiments. Fig. 1 shows a typical example.
<img src=/blog/images/deep-residual-learning-for-image-recognition/Fig1.PNG loading=lazy alt=Fig1></p><p>Figure 1. Training error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer ‚Äúplain‚Äù networks. The deeper network has higher training error, and thus test error. Similar phenomena on ImageNet is presented in Fig. 4.</p><p>The degradation (of training accuracy) indicates that not all systems are similarly easy to optimize. Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution <em>by construction</em> to the deeper model: the added layers are <em>identity</em> mapping and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time).</p><h2 id=deep-residual-learning>Deep Residual Learning</h2><h3 id=residual-learning>Residual Learning</h3><p>Let us consider $\mathcal{H}(x)$ as an underlying mapping to be fit by a few stacked layers (not necessarily the entire net), with $x$ denoting the inputs to the first of these layers. If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, $i.e.$, $\mathcal{H}(x)-x$ (assuming that the input and output are of the same dimensions). So rather than expect stacked layers to approximate $\mathcal{H}(x)$, we explicitly let these layers approximate a residual function $\mathcal{F}(x):=\mathcal{H}(x)-x$. The original function thus becomes $\mathcal{F}(x)+x$. Although both forms should be able to asymptotically approximate the desired functions (as hypothesized), the ease of learning might be different.</p><h3 id=identity-mapping-by-shortcuts>Identity Mapping by Shortcuts</h3>$$y = \mathcal{F}(x, {W\_i})+x$$<p>Here $x$ and $y$ are the input and output vectors of the layers considered. The function $\mathcal{F}(x, W_i)$ represents the residual mapping to be learned. For the example in Fig. 2 that has two layers, $\mathcal{F} = W_2\sigma (W_1x)$ in which $\sigma $ denotes ReLU and the bias are omitting for simplifying notations. The operation $\mathcal{F}+x$ is performed by a shortcut connection and element-wise addition. We adopt the second nonlinearity after the addtion (<em>i.e.</em>, $\sigma(y)$, see Fig.2).
<img src=/blog/images/deep-residual-learning-for-image-recognition/Fig2.PNG loading=lazy alt=Fig2></p><p>Figure2. Residual learning: a building block.</p><p>The dimensions of $x$ and $\mathcal{F}$ must be equal in Eqn.(1). If this is not the case (<em>e.g.</em>, when changing the input/output channels), we can perform a linear projection $W_s$ by the shortcut connections to match the dimensions:</p>$$y = \mathcal{F}(x, {W\_i}) + W\_sx$$<p>We can also use a square matrix $W_s$ in Eqn.(1). But we will show by experiments that the identity mapping is sufficient for addressing the degradation problem and is economical, and thus $W_s$ is only used when matching dimensions.
We also note that although the above notations are about fully-connected layers for simplicity, they are applicable to convolutional layers. The function $\mathcal{F}(x, {W_i})$ can represent multiple convolutional layers. The element-wise addition is performed on two feature maps, channel by channel.</p><h2 id=residual-network>Residual Network</h2><p>The identity shortcuts can be directly used when the input and output are of the same dimensions (solid line shortcuts in Fig.3). When the dimensions increase (dotted line shortcuts in Fig.3), we consider two options: (A) The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. This option introduces no extra parameter; (B) The projection shortcut in Eqn.(2) is used to match dimensions (done by $1 \times 1$ convolutions). For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2.
<img src=/blog/images/deep-residual-learning-for-image-recognition/Fig3.PNG loading=lazy alt=Fig3></p><p>Figure3. Example network architectures for ImageNet. <b>Left</b>: the VGG-19 model. <b>Middle</b>: a plain network with 34-parameter layers. <strong>Right</strong>: a residual network with 34 parameter layers. The dotted shortcuts increase dimensions. <b>Table 1</b> shows more details and other variants.</p><h2 id=implementation>Implementation</h2><p>Our implementation for ImageNet follows the practice in <em>[Imagenet classification
with deep convolutional neural networks]</em> and <em>[Very deep convolutional networks for large-scale image recognition]</em>.</p><ol><li>The Image is resized with its shorter side randomly sampled in $[256, 480]$ for scale agumentation.</li><li>A $224 \times 224$ crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted.</li><li>The standard color augmentation in <em>Imagenet classification
with deep convolutional neural networks</em> is used.</li><li>We adopt batch normalization (BN) right after each convolution and before activation.</li><li>We initialize the weights as in <em>Delving deep into rectifiers:
Surpassing human-level performance on imagenet classification</em> and train all plain/residual nets from scratch.</li><li>We use SGD with a mini-batch size of 256.</li><li>The learning rate starts from 0.1 and is divided by 10 when the error plateaus, and the models are trained from up to $60 \times 10^4$ iterations.</li><li>We use a weight decay of 0.0001 and a momentum of 0.9.</li><li>We do not use dropout, following the practice in <em>Batch normalization: Accelerating deep
network training by reducing internal covariate shift</em>.</li><li>In testing, for comparison studies we adopt the standard 10-crop testing.[<em>Imagenet classification
with deep convolutional neural networks</em>]</li><li>For best results, we adopt the fully-convolutional form as in <em>Very deep convolutional networks for large-scale image recognition</em> and <em>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</em>, and average the scores at multiple scales (images are resized such that the shorter side is in $\lbrace 224, 256, 384, 480, 640\rbrace $.</li></ol><h2 id=imagenet-classification>ImageNet classification</h2><h3 id=deeper-bottleneck-architecture>Deeper Bottleneck Architecture</h3><p>Next we describe our deeper nets for ImageNet. Because of concerns on the training time that we can afford, we modify the building block as a <em>bottleneck</em> design. For each residual function $\mathcal{F}$, we use a stack of 3 layers instead of 2 (Fig. 5). The three layers are $1 \times 1$, $3 \times 3$, and $1 \times 1$ convolutions, where the $1 \times 1$ layers are responsible for reducing and then increasing (restoring) dimensions, leaving the $3 \times 3$ layer a bottleneck with smaller input/output dimensions. Fig. 5 shows an example, where both designs have similar time complexity.
The parameter-free indentity shortcuts are particularly important for the bottleneck architectures. If the identity</p><h2 id=cifar-10-and-analysis>CIFAR-10 and Analysis</h2><p>The plain/residual architectures follow the form in Fig.3(middle/right). The network inputs are $32 \times 32$ images, with the per-pixel mean subtracted. The first layer is $3 \times 3$ convolutions. Then we use a stack of $6n$ layers with $3 \times 3$ convolutions on the feature maps of sizes $\lbrace 32, 16, 8\rbrace $ respectively, with $2n$ layers for each feature map size. The numbers of filters are $\lbrace 16, 32, 64\rbrace $ respectively, with $2n$ layers for each feature map size. The subsampling is performed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax.
When shortcut connections are used, they are connected to the pairs of $3 \times 3$ layers(totally $3n$ shortcuts). On this dataset we use identity shortcuts in all cases (<em>i.e.</em>, option A), so our residual models have exactly the same depth, width and number of parameters as the plain counterparts.
We use a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization in <em>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</em> and BN in <em>Accelerating deep network training by reducing internal covariate shift</em> but with no dropout. These models are trained with a mini-batch size of 128 on two GPUs. We start with a learning rate of 0.1, divide it by 10 at 32k and 48k iterations, and terminate training at 64k iterations, which is determined on a 45k/5k train/val split. We follow the simple data augmentation in <em>Deeply-supervised nets</em> for training: 4 pixels are padded on each side, and a $32 \times 32$ crop is randomly sampled from the padded image or its horizontal flip. For testing, we only evaluate the single view of the original $32 \times 32$ image.
We compare $n=\lbrace 3, 5, 7, 9\rbrace $, leading to 20, 32, 44, and 56-layer networks.</p></section><footer class=article-footer><section class=article-tags><a href=/blog/tags/deep-learning/>Deep Learning</a>
<a href=/blog/tags/machine-learning/>Machine Learning</a>
<a href=/blog/tags/resnet/>ResNet</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under Apache License 2.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Áõ∏ÂÖ≥ÊñáÁ´†</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/blog/p/identity-mappings-in-deep-residual-networks/><div class=article-details><h2 class=article-title>Identity Mappings in Deep Residual Networks</h2></div></a></article><article><a href=/blog/p/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/><div class=article-details><h2 class=article-title>Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning</h2></div></a></article><article><a href=/blog/p/layer-normalization/><div class=article-details><h2 class=article-title>Layer Normalization</h2></div></a></article><article><a href=/blog/p/deeper-insights-into-graph-convolutional-networks-for-semi-supervised-learning/><div class=article-details><h2 class=article-title>Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</h2></div></a></article><article><a href=/blog/p/semi-supervised-classification-with-graph-convolutional-networks/><div class=article-details><h2 class=article-title>Semi-Supervised Classification With Graph Convolutional Networks</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2020 -
2026 DavidhamÁöÑÂçöÂÆ¢</section><section class=powerby>‰ΩøÁî® <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> ÊûÑÂª∫<br>‰∏ªÈ¢ò <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> Áî± <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> ËÆæËÆ°</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>