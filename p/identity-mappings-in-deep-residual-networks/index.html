<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="ECCV 2016, ResNet v2, 原文链接：[Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)"><title>Identity Mappings in Deep Residual Networks</title>
<link rel=canonical href=https://davidham3.github.io/blog/p/identity-mappings-in-deep-residual-networks/><link rel=stylesheet href=/blog/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Identity Mappings in Deep Residual Networks"><meta property='og:description' content="ECCV 2016, ResNet v2, 原文链接：[Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)"><meta property='og:url' content='https://davidham3.github.io/blog/p/identity-mappings-in-deep-residual-networks/'><meta property='og:site_name' content='Davidham的博客'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='deep learning'><meta property='article:tag' content='machine learning'><meta property='article:tag' content='ResNet'><meta property='article:tag' content='已复现'><meta property='article:published_time' content='2018-03-08T18:45:45+00:00'><meta property='article:modified_time' content='2018-03-08T18:45:45+00:00'><meta name=twitter:title content="Identity Mappings in Deep Residual Networks"><meta name=twitter:description content="ECCV 2016, ResNet v2, 原文链接：[Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/><img src=/blog/img/avatar_hu_a92d3b55c5d43e55.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🚀</span></figure><div class=site-meta><h1 class=site-name><a href=/blog>Davidham的博客</a></h1><h2 class=site-description>随便写写</h2></div></header><ol class=menu-social><li><a href=https://github.com/Davidham3 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/blog/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#analysis-of-deep-residual-networks>Analysis of Deep Residual Networks</a></li><li><a href=#on-the-importance-of-identity-skip-connections>On the Importance of Identity Skip Connections</a><ol><li><a href=#experiments-on-skip-connections>Experiments on skip Connections</a></li></ol></li><li><a href=#on-the-usage-of-activation-functions>On the Usage of Activation Functions</a></li><li><a href=#experiments-on-activation>Experiments on Activation</a></li><li><a href=#implementation>Implementation</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/>论文阅读笔记</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/identity-mappings-in-deep-residual-networks/>Identity Mappings in Deep Residual Networks</a></h2><h3 class=article-subtitle>ECCV 2016, ResNet v2, 原文链接：[Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Mar 08, 2018</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 6 分钟</time></div></footer></div></header><section class=article-content><p>ECCV 2016, ResNet v2, 原文链接：<a class=link href=https://arxiv.org/abs/1603.05027 target=_blank rel=noopener>Identity Mappings in Deep Residual Networks</a></p><h1 id=identity-mappings-in-deep-residual-networks>Identity Mappings in Deep Residual Networks</h1><h2 id=introduction>Introduction</h2><p>Deep residual network (ResNets) consist of many stacked &ldquo;Residual Units&rdquo;. Each unit (Fig. 1(a)) can be expressed in a general form:</p>$$y\_l = h(x\_l) + \mathcal{F}(x\_l, \mathcal{W\_l})$$<p></p>$$x\_{l+1}=f(y\_l)$$<p>where $x_l$ and $x_{l+1}$ are input and output of the $l$-th unit, and $\mathcal{F}$ is a residual function.$h(x_l)=x_l$ is an identity mapping and $f$ is a ReLU function.
The central idea of ResNets is to learn the additive residual function $\mathcal{F}$ with respect to $h(x_l)$, with a key choice of using an identity mapping $h(x_l)=x_l$. This is realized by attaching an identity skip connection (&ldquo;shortcut&rdquo;).
In this paper, we analyze deep residual networks by focusing on creating a &ldquo;direct&rdquo; path for propagating information &ndash; not only within a residual unit, but through the entire network. Our derivations reveal that <em>if both h(x_l) and f(y_l) are identity mappings, the signal could be directly</em> propagated from one unit to any other units, in both forward and backward passes.
To understand the role of skip connections, we analyse and compare various types of $h(x_l)$. We find that the identity mapping $h(x_l) = x_l$ chosen in achieves the fastest error reduction and lowest training loss among all variants we investigated, whereas skip connections of scaling, gating, and $1 \times 1$ convolutions all lead to higher training loss and error. These experiments suggest that keeping a &ldquo;clean&rdquo; information path (indicated by the grey arrows in Fig. 1,2, and 4) is helpful for easing optimization.
<img src=/blog/images/identity-mappings-in-deep-residual-networks/Fig1.PNG loading=lazy alt=Fig1>
Figure 1. Left: (a) original Residual Unit in [1]; (b) proposed Residual Unit. The grey arrows indicate the easiest paths for the information to propagate, corresponding to the additive term &ldquo;x_l&rdquo; in Eqn.(4) (forward propagation) and the additive term &ldquo;1&rdquo; in Eqn.(5) (backward propagation). Right: training curves on CIFAR-10 of 1001-layer ResNets. Solid lines denote test error (y-axis on the right), and dashed lines denote training loss (y-axis on the left). The proposed unit makes ResNet-1001 easier to train.</p><p>To construct an identity mapping $f(y_l)=y_l$, we view the activation functions (ReLU and BN) as &ldquo;pre-activation&rdquo; of the weight layers, in constrast to conventional wisdom of &ldquo;post-activation&rdquo;. This point of view leads to a new residual unit design, shown in (Fig. 1(b)). Based on this unit, we present competitive results on CIFAR-10/100 with a 1001-layer ResNet, which is much easier to train and generalizes better than the original ResNet in [1]. We further report improved results on ImageNet using a 200-layer ResNet, for which the counterpart of [1] starts to overfit. These results suggest that there is much room to exploit the dimension of <em>network depth</em>, a key to the success of modern deep learning.</p><h2 id=analysis-of-deep-residual-networks>Analysis of Deep Residual Networks</h2><p>The ResNets developed in [1] are <em>modularized</em> architectures that stack building blocks of the same connecting shape. In this paper we call these blocks &ldquo;Residual Units&rdquo;. The original Residual Unit in [1] performs the following computation:</p>$$y\_l = h(x\_l) + \mathcal{F}(x\_l, \mathcal{W-l})$$<p></p>$$x\_{l+1}=f(y\_l)$$<p>Here $x_l$ is the input feature to the $l$-th Residual Unit. $\mathcal{W_l}=\lbrace W_{l,k} \mid 1 \le k \le K\rbrace$ is a set of weights (and biases) associated with the $l$-th Residual Unit, and $K$ is the number of layers in a Residual Unit ($K$ is 2 or 3 in [1]). $\mathcal{F}$ denotes the residual function, <em>e.g.</em>, a stack of two $3 \times 3$ convolutional layers in [1]. The function $f$ is the operation after element-wise addition, and in [1] $f$ is ReLU. The function $h$ is set as an identity mapping: $h(x_l)=x_l$.
If $f$ is also an identity mapping: $x_{l+1} \equiv y_l$, we can put Eqn.(2) into Eqn.(1) and obtain:</p>$$x\_{l+1}=x\_l+\mathcal{F}(x\_l, \mathcal{W\_l})$$<p>Recursively $(x_{l+2}=x_{l+1} + \mathcal{F}(x_{l+1}, \mathcal{W_{l+1}}) = x_l + \mathcal{F}(x_l, \mathcal{W_l}) + \mathcal{F}(x_{l+1},\mathcal{W_{l+1}}), etc.)$ we will have:</p>$$x\_L = x\_l + \sum\_{i=1}^{L-1}\mathcal{F}(x\_i, \mathcal{W\_i})$$<p>for <em>any deeper unit</em> $L$ and <em>any shallower unit</em> $l$. Eqn.(4) exhibits some nice properties.</p><ol><li>The feature $x_L$ of any deeper unit $L$ can be represented as the feature $x_l$ of any shallower unit $l$ plus a residual function in a form of $\sum_{i=1}^{L-1}\mathcal{F}$, indicating that the model is in a <em>residual</em> fashion between any units $L$ and $l$.</li><li>The feature $x_L = x_0 + \sum_{i=0}^{L-1}\mathcal{F}(x_i, \mathcal{W_i})$, of any deep unit $L$, is the <em>summation</em> of the outputs of all preceding residual functions (plus $x_0$). This is in contrast to a &ldquo;plain network&rdquo; where a feature $x_L$ is a series of matrix-vector <em>products</em>, say, $\prod_{i=0}^{L-1}W_ix_0$ (ignoring BN and ReLU).
Eqn.(4) also leads to nice backward propagation properties. Denoting the loss function as $\varepsilon$, from the chain rule of backpropagation [9] we have:
$$\frac{\partial{\varepsilon}}{\partial{x\_l}}=\frac{\partial{\varepsilon}}{\partial{x\_L}}\frac{\partial{x\_L}}{\partial{x\_l}}=\frac{\partial{\varepsilon}}{\partial{x\_L}}(1+\frac{\partial}{\partial{x\_l}}\sum\_{i=l}^{L-1}\mathcal{F}(x\_i, \mathcal{W\_i}))$$
Eqn.(5) indicates that the gradient $\frac{\partial{\varepsilon}}{\partial{x_i}}$ can be decomposed into two additive terms: a term of $\frac{\partial{\varepsilon}}{\partial{x_L}}$ that propagates information directly without concerning any weight layers, and another term of $\frac{\partial{\varepsilon}}{\partial{x_L}}(\frac{\partial}{\partial{x_l}}\sum_{i=l}^{L-1}\mathcal{F})$ that propagates through the weight layers. The additive term of $\frac{\partial{\varepsilon}}{\partial{x_L}}$ ensures that information is directly propagated back to <em>any shallower unit</em> $l$. Eqn.(5) also suggests that it is unlikely for the gradient $\frac{\partial{\varepsilon}}{\partial{x_l}}$ to be canceled out for a mini-batch, because in general the term $\frac{\partial}{\partial{x_l}}\sum_{i=l}^{L-1}\mathcal{F}$ cannot be always -1 for all samples in a mini-batch. This implies that the gradient of a layer does not vanish even when the weights are arbitrarily small.</li></ol><h2 id=on-the-importance-of-identity-skip-connections>On the Importance of Identity Skip Connections</h2><p>Let&rsquo;s consider a simple modification, $h(x_l)=\lambda_lx_l$, to break the identity shortcut:</p>$$x\_{l+1}=\lambda\_lx\_l+\mathcal{F}(x\_l, \mathcal{W\_l})$$<p>where $\lambda_l$ is a modulating scalar (for simplicity we still assume $f$ is identity).
Recursively applying this forumulation we obtain an equation similar to Eqn. (4): $x_L=(\prod_{i=l}^{L-1}\lambda_i)x_l+\sum_{i=1}^{L-1}(\prod_{j=i+1}^{L-1}\lambda_j)\mathcal{F}(x_i, \mathcal{W_i})$, or simply:</p>$$x\_L = (\prod\_{i=l}^{L-1}\lambda\_i)x\_l+\sum\_{i=l}^{L-1}\hat{\mathcal{F}}(x\_i, \mathcal{W\_i})$$<p>where the notation $\hat{\mathcal{F}}$ absorbs the scalars into the residual functions. Similar to Eqn.(5), we have backpropagation of the following form:</p>$$\frac{\partial{\varepsilon}}{\partial{x\_l}}=\frac{\partial{\varepsilon}}{\partial{x\_L}}((\prod\_{i=l}^{L-1}\lambda\_i)+\frac{\partial}{\partial{x\_l}}\sum\_{i=l}^{L-1}\hat{\mathcal{F}}(x\_i, \mathcal{W\_i}))$$<p>For an extremely deep network ($L$ is large), if $\lambda_i > 1$ for all $i$, this factor can be exponentially large; if $\lambda_i &lt; 1$ for all $i$, this factor can be expoentially small and vanish, which blocks the backpropagated signal from the shortcur and forces it to flow through the weighted layers. This results in optimization difficuties as we show by experiments.
If the skip connection $h(x_l)$ represents more complicated transforms (such as gating and $1 \times 1$ convolutions), in Eqn.(8) the first term becomes $\prod_{i=l}^{L-1}h_i&rsquo;$ where $h&rsquo;$ is the derivative of $h$. This product may also impede information propagation and hamper the training procedure as witnessed in the following experiments.</p><h3 id=experiments-on-skip-connections>Experiments on skip Connections</h3><p>We experiments with the 110-layer ResNet as presented in [1] on CIFAR-10. Though our above analysis is driven by identity $f$, the experiments in this section are all based on $f = ReLU$ as in [1]; we address identity $f$ in the next section. Our baseline ResNet-110 has 6.61% error on the test set. The comparisons of other variants (Fig.2 and Table 1) are summarized as follows:
<strong>Table 1.</strong> Classification error on the CIFAR-10 test set using ResNet-110 [1], with different types of shortcut connections applied to all Residual Units. We report &ldquo;fail&rdquo; when the test error is higher than 20%.
<img src=/blog/images/identity-mappings-in-deep-residual-networks/Table1.PNG loading=lazy alt=Table1></p><p><strong>Constant scaling</strong>. We set $\lambda = 0.5$ for all shortcuts (Fig. 2(b)). We further study two cases of scaling $\mathcal{F}$:</p><ol><li>$\mathcal{F}$ is not scaled;</li><li>$\mathcal{F}$ is scaled by a constant scalar of $1-\lambda = 0.5$, which is similar to the highway gating [6,7] but with frozen gates. The former case does not converge well; the latter is able to converge, but the test error (Table 1, 12.35%) is substantially higher than the original ResNet-110. Fig 3(a) shows that the training error is higher than that of the original ResNet-110, suggesting that the optimization has difficulties when the shortcut signal is scaled down.</li></ol><p><strong>Exclusive gating</strong>. Following the Highway Networks [6,7] that adopt a gating mechanism [5], we consider a gating function $g(x)=\sigma(W_gx+b_g)$ where a transform is represented by weights $W_g$ and biases $b_g$ followed by the sigmoid function $\sigma(x)=\frac{1}{1+e^{-x}}$. In a convolutional network $g(x)$ is realized by a $1 \times 1$ convolutional layer. The gating function modulates the signal by element-wise multiplication.
We investigate the &ldquo;exclusive&rdquo; gates as used in [6,7] &ndash; the $\mathcal{F}$ path is scaled by $g(x)$ and the shortcut path is scaled by $1-g(x)$. See Fig 2(c). We find that the initialization of the biases $b_g$ is critical for training gated models, and following the guidelines in [6,7], we conduct hyper-parameter search on the initial value of $b_g$ in the range of 0 to -10 with a decrement step of -1 on the training set by cross-validation. The best value (-6 here) is then used for training on the training set, leading to a test result of 8.70% (Table 1), which still lags far behind the ResNet-110 baseline. Fig 3(b) shows the training curves. Table 1 also reports the results of using other initialized values, noting that the exclusive gating network does not converge to a good solution when $b_g$ is not appropriately initialized.</p><p><strong>Shortcut-only gating</strong>. In this case the function $\mathcal{F}$ is not scaled; only the shortcut path is gated by $1-g(x)$. See Fig 2(d). The initialized value of $b_g$ is still essential in this case. When the initialized $b_g$ is 0 (so initially the expectation of $1-g(x)$ is 0.5), the network converges to a poor result of 12.86% (Table 1). This is also caused by higher training error (Fig 3(c)).
When the initialized $b_g$ is very negatively biased (e.g., -6), the value of $1-g(x)$ is closer to 1 and the shortcut connection is nearly an identity mapping. Therefore, the result (6.91%, Table 1) is much closer to the ResNet-110 baseline.</p><p><strong>$1 \times 1$ convolutional shortcut</strong>. Next we experiment with $1 \times 1$ convolutional shortcut connections that replace the identity. This option has been investigated in [1] (known as option C) on a 34-layer ResNet (16 Residual Units) and shows good results, suggesting that $1 \times 1$ shortcut connections could be useful. But we find that this is not the case when there are many Residual Units. The 110-layer ResNet has a poorer result (12.22%, Table 1) when using $1 \times 1$ convolutional shortcuts. Again, the training error becomes higher (Fig 3(d)). When stacking so many Residual Units (54 for ResNet-110), even the shortest path may still impede signal propagation. We witnessed similar phenomena on ImageNet with ResNet-101 when using $1 \times 1$ convolutional shortcuts.</p><p><strong>Dropout shortcut</strong>. Last we experiment with dropout [11] (at a ratio of 0.5) which we adopt on the output of the identity shortcut (Fig. 2(f)). The network fails to converge to a good solution. Dropout statistically imposes a scale of $\lambda $ with an expectation of 0.5 on the shortcut, and similar to constant scaling by 0.5, it impedes signal propagation.</p><h2 id=on-the-usage-of-activation-functions>On the Usage of Activation Functions</h2><p>We want to make $f$ an identity mapping, which is done by re-arranging the activation function (ReLU and/or BN). The original Residual Unit in [1] has a shape in Fig.4(a) &ndash; BN is used after each weight layer, and ReLU is adopted after BN expect that the last ReLU in a Residual Unit is after element-wise addition ($f=ReLU$). Fig.4(b-e) show the laternatives we investigated, explained as following.</p><h2 id=experiments-on-activation>Experiments on Activation</h2><p>In this section we experiment with ResNet-110 and a 164-layer Bottlenect [1] architecture (denoted as ResNet-164). A bottleneck Residual Unit consist of a $1 \times 1$ layer for reducing dimension, a $3 \times 3$ layer, and a $1 \times 1$ layer for restoring dimension. As designed in [1], its computational complexity is similar to the two-$3 \times 3$ Residual Unit. More details are in the appendix. The baseline ResNet-164 has a competitive result of 5.93% on CIFAR-10 (Table 2).</p><p><strong>BN after addition</strong>. Before turning $f$ into an identity mapping, we go the opposite way by adopting BN after addition (Fig. 4(b)). In this case $f$ involves BN and ReLU. The results become considerably worse than the baseline (Table 2). Unlike the original design, now the BN layer alters the signal that passes through the shortcut and impedes information propagation, as reflected by the difficulties on reducing training loss at the begining of training (Fib. 6 left).</p><p><strong>ReLU before addition</strong>. A naive choice of making $f$ into an identity mapping is to move the ReLU</p><h2 id=implementation>Implementation</h2><p>使用mxnet实现了一版</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>mxnet</span> <span class=kn>import</span> <span class=n>nd</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>mxnet</span> <span class=k>as</span> <span class=nn>mx</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pickle</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>mxnet</span> <span class=kn>import</span> <span class=n>image</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>unpickle</span><span class=p>(</span><span class=n>file</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>file</span><span class=p>,</span> <span class=s1>&#39;rb&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>fo</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>dicts</span> <span class=o>=</span> <span class=n>pickle</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>fo</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;bytes&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>dicts</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>residual_unit</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>channels</span><span class=p>,</span> <span class=n>name</span><span class=p>,</span> <span class=n>same_shape</span> <span class=o>=</span> <span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>stride</span> <span class=o>=</span> <span class=mi>1</span> <span class=k>if</span> <span class=n>same_shape</span> <span class=k>else</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>BatchNorm</span><span class=p>(</span><span class=n>data</span> <span class=o>=</span> <span class=n>x</span><span class=p>,</span> <span class=n>fix_gamma</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span> <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;</span><span class=si>%s</span><span class=s1>_bn1&#39;</span><span class=o>%</span><span class=p>(</span><span class=n>name</span><span class=p>),</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>Activation</span><span class=p>(</span><span class=n>data</span> <span class=o>=</span> <span class=n>net</span><span class=p>,</span> <span class=n>act_type</span> <span class=o>=</span> <span class=s1>&#39;relu&#39;</span><span class=p>,</span> <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;</span><span class=si>%s</span><span class=s1>_relu1&#39;</span><span class=o>%</span><span class=p>(</span><span class=n>name</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>Convolution</span><span class=p>(</span><span class=n>data</span> <span class=o>=</span> <span class=n>net</span><span class=p>,</span> <span class=n>num_filter</span> <span class=o>=</span> <span class=n>channels</span><span class=p>,</span> <span class=n>kernel</span> <span class=o>=</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span>\
</span></span><span class=line><span class=cl>                             <span class=n>pad</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>stride</span> <span class=o>=</span> <span class=p>(</span><span class=n>stride</span><span class=p>,</span> <span class=n>stride</span><span class=p>),</span> <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;</span><span class=si>%s</span><span class=s1>_conv1&#39;</span><span class=o>%</span><span class=p>(</span><span class=n>name</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>BatchNorm</span><span class=p>(</span><span class=n>data</span> <span class=o>=</span> <span class=n>net</span><span class=p>,</span> <span class=n>fix_gamma</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span> <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;</span><span class=si>%s</span><span class=s1>_bn2&#39;</span><span class=o>%</span><span class=p>(</span><span class=n>name</span><span class=p>),</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>Activation</span><span class=p>(</span><span class=n>data</span> <span class=o>=</span> <span class=n>net</span><span class=p>,</span> <span class=n>act_type</span> <span class=o>=</span> <span class=s1>&#39;relu&#39;</span><span class=p>,</span> <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;</span><span class=si>%s</span><span class=s1>_relu2&#39;</span><span class=o>%</span><span class=p>(</span><span class=n>name</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>Convolution</span><span class=p>(</span><span class=n>data</span> <span class=o>=</span> <span class=n>net</span><span class=p>,</span> <span class=n>num_filter</span> <span class=o>=</span> <span class=n>channels</span><span class=p>,</span> <span class=n>kernel</span> <span class=o>=</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span>\
</span></span><span class=line><span class=cl>                             <span class=n>pad</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;</span><span class=si>%s</span><span class=s1>_conv2&#39;</span><span class=o>%</span><span class=p>(</span><span class=n>name</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=n>same_shape</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>Convolution</span><span class=p>(</span><span class=n>data</span> <span class=o>=</span> <span class=n>x</span><span class=p>,</span> <span class=n>num_filter</span> <span class=o>=</span> <span class=n>channels</span><span class=p>,</span> <span class=n>pad</span> <span class=o>=</span> <span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>),</span>\
</span></span><span class=line><span class=cl>                               <span class=n>stride</span> <span class=o>=</span> <span class=p>(</span><span class=n>stride</span><span class=p>,</span> <span class=n>stride</span><span class=p>),</span> <span class=n>kernel</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>name</span> <span class=o>=</span> <span class=s2>&#34;</span><span class=si>%s</span><span class=s2>_conv3&#34;</span><span class=o>%</span><span class=p>(</span><span class=n>name</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>net</span> <span class=o>+</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>ResNet</span><span class=p>(</span><span class=n>units</span><span class=p>,</span> <span class=n>nums</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=s1>&#39;data&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>Convolution</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>num_filter</span> <span class=o>=</span> <span class=mi>16</span><span class=p>,</span> <span class=n>kernel</span> <span class=o>=</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span> <span class=n>pad</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>num</span> <span class=ow>in</span> <span class=n>nums</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>net</span> <span class=o>=</span> <span class=n>residual_unit</span><span class=p>(</span><span class=n>net</span><span class=p>,</span> <span class=n>num</span><span class=p>,</span> <span class=s1>&#39;r</span><span class=si>%s%s</span><span class=s1>&#39;</span><span class=o>%</span><span class=p>(</span><span class=n>num</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>units</span><span class=o>+</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>net</span> <span class=o>=</span> <span class=n>residual_unit</span><span class=p>(</span><span class=n>net</span><span class=p>,</span> <span class=n>num</span><span class=p>,</span> <span class=s1>&#39;r</span><span class=si>%s%s</span><span class=s1>&#39;</span><span class=o>%</span><span class=p>(</span><span class=n>num</span><span class=p>,</span> <span class=n>i</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>BatchNorm</span><span class=p>(</span><span class=n>net</span><span class=p>,</span> <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;batch1&#39;</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>Activation</span><span class=p>(</span><span class=n>net</span><span class=p>,</span> <span class=n>act_type</span> <span class=o>=</span> <span class=s1>&#39;relu&#39;</span><span class=p>,</span> <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;relu1&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>Pooling</span><span class=p>(</span><span class=n>net</span><span class=p>,</span> <span class=n>pool_type</span> <span class=o>=</span> <span class=s1>&#39;avg&#39;</span><span class=p>,</span> <span class=n>kernel</span> <span class=o>=</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span> <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;pool1&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>Flatten</span><span class=p>(</span><span class=n>net</span><span class=p>,</span> <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;flat1&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>FullyConnected</span><span class=p>(</span><span class=n>net</span><span class=p>,</span> <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;fc1&#39;</span><span class=p>,</span> <span class=n>num_hidden</span> <span class=o>=</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>sym</span><span class=o>.</span><span class=n>SoftmaxOutput</span><span class=p>(</span><span class=n>net</span><span class=p>,</span> <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;softmax&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>net</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>all_data</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=n>unpickle</span><span class=p>(</span><span class=s1>&#39;../data/cifar-10-batches-py/data_batch_</span><span class=si>%s</span><span class=s1>&#39;</span><span class=o>%</span><span class=p>(</span><span class=n>i</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>all_data</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>nd</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=sa>b</span><span class=s1>&#39;data&#39;</span><span class=p>]),</span> <span class=n>nd</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=sa>b</span><span class=s1>&#39;labels&#39;</span><span class=p>])))</span>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=nb>zip</span><span class=p>(</span><span class=o>*</span><span class=n>all_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainX</span><span class=p>,</span> <span class=n>trainY</span> <span class=o>=</span> <span class=n>nd</span><span class=o>.</span><span class=n>concat</span><span class=p>(</span><span class=o>*</span><span class=n>X</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>shape</span> <span class=o>=</span> <span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>32</span><span class=p>))</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s1>&#39;float32&#39;</span><span class=p>),</span>\
</span></span><span class=line><span class=cl>                    <span class=n>nd</span><span class=o>.</span><span class=n>concat</span><span class=p>(</span><span class=o>*</span><span class=n>y</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>unpickle</span><span class=p>(</span><span class=s1>&#39;../data/cifar-10-batches-py/test_batch&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>testX</span> <span class=o>=</span> <span class=n>nd</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=sa>b</span><span class=s1>&#39;data&#39;</span><span class=p>])</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>shape</span> <span class=o>=</span> <span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>32</span><span class=p>))</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s1>&#39;float32&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>testY</span> <span class=o>=</span> <span class=n>nd</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=sa>b</span><span class=s1>&#39;labels&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># batch_size = 128</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=n>train_iter</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>io</span><span class=o>.</span><span class=n>NDArrayIter</span><span class=p>(</span><span class=n>trainX</span><span class=p>,</span> <span class=n>trainY</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>shuffle</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>test_iter</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>io</span><span class=o>.</span><span class=n>NDArrayIter</span><span class=p>(</span><span class=n>testX</span><span class=p>,</span> <span class=n>testY</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>shuffle</span> <span class=o>=</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>net</span> <span class=o>=</span> <span class=n>ResNet</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=p>[</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>256</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>mod</span> <span class=o>=</span> <span class=n>mx</span><span class=o>.</span><span class=n>mod</span><span class=o>.</span><span class=n>Module</span><span class=p>(</span><span class=n>symbol</span><span class=o>=</span><span class=n>net</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>context</span><span class=o>=</span><span class=p>[</span><span class=n>mx</span><span class=o>.</span><span class=n>gpu</span><span class=p>(</span><span class=n>i</span><span class=p>)</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>)],</span>
</span></span><span class=line><span class=cl><span class=c1>#                     context = mx.gpu(0),</span>
</span></span><span class=line><span class=cl>                    <span class=n>data_names</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;data&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                    <span class=n>label_names</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;softmax_label&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>mod</span><span class=o>.</span><span class=n>bind</span><span class=p>(</span><span class=n>data_shapes</span> <span class=o>=</span> <span class=n>train_iter</span><span class=o>.</span><span class=n>provide_data</span><span class=p>,</span> <span class=n>label_shapes</span> <span class=o>=</span> <span class=n>train_iter</span><span class=o>.</span><span class=n>provide_label</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mod</span><span class=o>.</span><span class=n>init_params</span><span class=p>(</span><span class=n>initializer</span><span class=o>=</span><span class=n>mx</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>Xavier</span><span class=p>(</span><span class=n>rnd_type</span><span class=o>=</span><span class=s1>&#39;gaussian&#39;</span><span class=p>,</span> <span class=n>factor_type</span><span class=o>=</span><span class=s2>&#34;in&#34;</span><span class=p>,</span> <span class=n>magnitude</span><span class=o>=</span><span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>mod</span><span class=o>.</span><span class=n>init_optimizer</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;nag&#39;</span><span class=p>,</span> <span class=n>optimizer_params</span><span class=o>=</span><span class=p>((</span><span class=s1>&#39;learning_rate&#39;</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                                                       <span class=p>(</span><span class=s1>&#39;wd&#39;</span><span class=p>,</span> <span class=mf>0.0001</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                                                       <span class=p>(</span><span class=s1>&#39;momentum&#39;</span><span class=p>,</span> <span class=mf>0.9</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=c1># mod.init_optimizer(optimizer=&#39;adam&#39;, optimizer_params=((&#39;learning_rate&#39;, 5e-4),</span>
</span></span><span class=line><span class=cl><span class=c1>#                                                        (&#39;beta1&#39;, 0.9),</span>
</span></span><span class=line><span class=cl><span class=c1>#                                                        (&#39;beta2&#39;, 0.99)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>accuracy</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>metrics</span> <span class=o>=</span> <span class=p>[</span><span class=n>mx</span><span class=o>.</span><span class=n>metric</span><span class=o>.</span><span class=n>create</span><span class=p>(</span><span class=s1>&#39;acc&#39;</span><span class=p>),</span> <span class=n>mx</span><span class=o>.</span><span class=n>metric</span><span class=o>.</span><span class=n>CrossEntropy</span><span class=p>()]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>train_iter</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=n>i</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>metrics</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>train_iter</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>mod</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>is_train</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mod</span><span class=o>.</span><span class=n>update_metric</span><span class=p>(</span><span class=n>metrics</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>batch</span><span class=o>.</span><span class=n>label</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mod</span><span class=o>.</span><span class=n>update_metric</span><span class=p>(</span><span class=n>metrics</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>batch</span><span class=o>.</span><span class=n>label</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mod</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>mod</span><span class=o>.</span><span class=n>update</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>epoch</span> <span class=o>%</span> <span class=mi>1</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>score</span> <span class=o>=</span> <span class=n>mod</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>test_iter</span><span class=p>,</span> <span class=p>[</span><span class=s1>&#39;acc&#39;</span><span class=p>,</span> <span class=n>mx</span><span class=o>.</span><span class=n>metric</span><span class=o>.</span><span class=n>CrossEntropy</span><span class=p>()])</span>
</span></span><span class=line><span class=cl>        <span class=n>losses</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>metrics</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>get</span><span class=p>()[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>score</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>        <span class=n>accuracy</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>metrics</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>get</span><span class=p>()[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>score</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch </span><span class=si>%d</span><span class=s1>, Training acc </span><span class=si>%s</span><span class=s1>, loss </span><span class=si>%s</span><span class=s1>&#39;</span><span class=o>%</span><span class=p>(</span><span class=n>epoch</span><span class=p>,</span> <span class=n>accuracy</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=mi>0</span><span class=p>],</span> <span class=n>losses</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=mi>0</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch </span><span class=si>%d</span><span class=s1>, Validation acc </span><span class=si>%s</span><span class=s1>, loss </span><span class=si>%s</span><span class=s1>&#39;</span><span class=o>%</span><span class=p>(</span><span class=n>epoch</span><span class=p>,</span> <span class=n>accuracy</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=mi>1</span><span class=p>],</span> <span class=n>losses</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=mi>1</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>train_loss</span><span class=p>,</span> <span class=n>test_loss</span> <span class=o>=</span> <span class=nb>zip</span><span class=p>(</span><span class=o>*</span><span class=n>losses</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>train_loss</span><span class=p>,</span> <span class=s1>&#39;-&#39;</span><span class=p>,</span> <span class=n>color</span> <span class=o>=</span> <span class=s1>&#39;blue&#39;</span><span class=p>,</span> <span class=n>label</span> <span class=o>=</span> <span class=s1>&#39;training loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>test_loss</span><span class=p>,</span> <span class=s1>&#39;-&#39;</span><span class=p>,</span> <span class=n>color</span> <span class=o>=</span> <span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>label</span> <span class=o>=</span> <span class=s1>&#39;testing loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>loc</span> <span class=o>=</span> <span class=s1>&#39;upper right&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;iteration&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>train_acc</span><span class=p>,</span> <span class=n>test_acc</span> <span class=o>=</span> <span class=nb>zip</span><span class=p>(</span><span class=o>*</span><span class=n>accuracy</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>train_acc</span><span class=p>,</span> <span class=s1>&#39;-&#39;</span><span class=p>,</span> <span class=n>color</span> <span class=o>=</span> <span class=s1>&#39;blue&#39;</span><span class=p>,</span> <span class=n>label</span> <span class=o>=</span> <span class=s1>&#39;training acc&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>test_acc</span><span class=p>,</span> <span class=s1>&#39;-&#39;</span><span class=p>,</span> <span class=n>color</span> <span class=o>=</span> <span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>label</span> <span class=o>=</span> <span class=s1>&#39;testing acc&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>loc</span> <span class=o>=</span> <span class=s1>&#39;upper right&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;iteration&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;acc&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div></section><footer class=article-footer><section class=article-tags><a href=/blog/tags/deep-learning/>Deep Learning</a>
<a href=/blog/tags/machine-learning/>Machine Learning</a>
<a href=/blog/tags/resnet/>ResNet</a>
<a href=/blog/tags/%E5%B7%B2%E5%A4%8D%E7%8E%B0/>已复现</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under Apache License 2.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>最后更新于 Mar 08, 2018 18:45 UTC</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/blog/p/spatio-temporal-graph-convolutional-networks-a-deep-learning-framework-for-traffic/><div class=article-details><h2 class=article-title>Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic</h2></div></a></article><article><a href=/blog/p/deep-residual-learning-for-image-recognition/><div class=article-details><h2 class=article-title>Deep Residual Learning for Image Recognition</h2></div></a></article><article><a href=/blog/p/perceptual-losses-for-real-time-style-transfer-and-super-resolution/><div class=article-details><h2 class=article-title>Perceptual Losses for Real-Time Style Transfer and Super-Resolution</h2></div></a></article><article><a href=/blog/p/image-style-transfer-using-convolutional-neural-networks/><div class=article-details><h2 class=article-title>Image Style Transfer Using Convolutional Neural Networks</h2></div></a></article><article><a href=/blog/p/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/><div class=article-details><h2 class=article-title>Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Davidham的博客</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>